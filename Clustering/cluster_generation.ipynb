{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import pandas as pd\n",
    "import category_encoders\n",
    "import numpy as np\n",
    "import math\n",
    "PREDICCION_REAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APERTURA DE ARCHIVO DE ARCHIVOS\n",
    "entrenamiento = pd.read_pickle(\"../Archivos/Arboles_entrenamiento_exp.pkl\").drop(columns = [\"Opportunity_ID\"])\n",
    "test = pd.read_pickle(\"../Archivos/Arboles_validacion_exp.pkl\").drop(columns = [\"Opportunity_ID\"])\n",
    "test_real = pd.read_pickle(\"../Archivos/Arboles_validacion_exp.pkl\")\n",
    "if PREDICCION_REAL:\n",
    "    test = pd.read_pickle(\"../Archivos/Arboles_test_exp.pkl\").drop(columns = [\"Opportunity_ID\"])\n",
    "    test_real = pd.read_pickle(\"../Archivos/Arboles_test_exp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FECHAS A DIAS\n",
    "\n",
    "columnas_fecha = ['Last_Modified_Date','Account_Created_Date','Opportunity_Created_Date','Quote_Expiry_Date','Planned_Delivery_Start_Date','Planned_Delivery_End_Date',\"Year-Month\"]\n",
    "def fecha_a_dias(x):\n",
    "    for columna in columnas_fecha:\n",
    "        x[columna] = x[columna].apply(lambda x : (x - pd.to_datetime('01/01/2000', format='%m/%d/%Y')).days)\n",
    "\n",
    "fecha_a_dias(entrenamiento)\n",
    "fecha_a_dias(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "objetivo = (entrenamiento['Stage'] == 1).astype(int)\n",
    "entrenamiento = entrenamiento.drop(columns=['Stage'])\n",
    "columnas_category = list(entrenamiento.select_dtypes(include=['category']).columns)\n",
    "if 'Stage' in columnas_category : columnas_category.remove('Stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PREDICCION_REAL:\n",
    "    test_label = (test['Stage'] == 1).astype(int)\n",
    "    test = test.drop(columns=['Stage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gianb\\anaconda3\\envs\\hdbscan\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "C:\\Users\\gianb\\anaconda3\\envs\\hdbscan\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "def categoricas_a_numericas(train,test,label,usar_label):\n",
    "    if (usar_label):\n",
    "        columnas_object = list(train.select_dtypes(include=['category']).columns)\n",
    "    else:\n",
    "        columnas_object = list(test.select_dtypes(include=['category']).columns)\n",
    "    if 'Stage' in columnas_object : columnas_object.remove('Stage')\n",
    "    ohe = category_encoders.cat_boost.CatBoostEncoder(cols = columnas_object,return_df = True)\n",
    "    ohe.fit(train,label)\n",
    "    if (usar_label):\n",
    "        columnas = ohe.transform(train,label)\n",
    "        for columna in columnas_object:\n",
    "            train[columna] = columnas[columna].copy()\n",
    "    else:\n",
    "        columnas = ohe.transform(test)\n",
    "        for columna in columnas_object:\n",
    "            test[columna] = columnas[columna].copy()\n",
    "categoricas_a_numericas(entrenamiento,test,objetivo,False)\n",
    "categoricas_a_numericas(entrenamiento,test,objetivo,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_nan(col,entrenamiento,test):\n",
    "    mean = entrenamiento[col].mean()\n",
    "    entrenamiento[col] = entrenamiento[col].replace(np.NaN,mean)\n",
    "    mean = test[col].mean()\n",
    "    test[col] = test[col].replace(np.NaN,mean)\n",
    "    \n",
    "def limpiar_inf(col,entrenamiento,test):\n",
    "    entrenamiento[col] = entrenamiento[col].replace(math.inf,np.NaN)\n",
    "    test[col] = test[col].replace(math.inf,np.NaN)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar(entrenamiento,test,limite):\n",
    "    for col in entrenamiento.columns:\n",
    "        ent_null = entrenamiento.loc[entrenamiento[col] == np.inf]\n",
    "        if (ent_null[col].count() > limite):\n",
    "            entrenamiento.drop(columns = [col],inplace=True)\n",
    "            test.drop(columns = [col],inplace=True)\n",
    "        if (0<ent_null[col].count() <= limite):\n",
    "            limpiar_inf(col,entrenamiento,test)        \n",
    "    for col in test.columns:\n",
    "        ent_null = test.loc[test[col] == np.inf]\n",
    "        if (ent_null[col].count() > limite):\n",
    "            entrenamiento.drop(columns = [col],inplace=True)\n",
    "            test.drop(columns = [col],inplace=True)\n",
    "        if (0<ent_null[col].count() <= limite):\n",
    "            limpiar_inf(col,entrenamiento,test)        \n",
    "\n",
    "    for col in entrenamiento.columns:\n",
    "        ent_null = entrenamiento.loc[entrenamiento[col].isnull()]\n",
    "        ent_null = ent_null.replace(np.NaN,0)\n",
    "        if (ent_null[col].count() > limite):\n",
    "            entrenamiento.drop(columns = [col],inplace=True)\n",
    "            test.drop(columns = [col],inplace=True)\n",
    "        if (0<ent_null[col].count() <= limite):\n",
    "            limpiar_nan(col,entrenamiento,test)        \n",
    "    for col in test.columns:\n",
    "        ent_null = test.loc[test[col].isnull()]\n",
    "        ent_null = ent_null.replace(np.NaN,0)\n",
    "        if (ent_null[col].count() > limite):\n",
    "            entrenamiento.drop(columns = [col],inplace=True)\n",
    "            test.drop(columns = [col],inplace=True)\n",
    "        if (0<ent_null[col].count() <= limite):\n",
    "            limpiar_nan(col,entrenamiento,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_usar = ['Bureaucratic_Code',\n",
    " 'Account_Name',\n",
    " 'Opportunity_Name',\n",
    " 'Opportunity_Owner',\n",
    " 'Last_Modified_By',\n",
    " 'Product_Family',\n",
    " 'Product_Name',\n",
    " 'Opportunity_Name_Planned_Time_Until_Deliver_mean',\n",
    " 'Opportunity_Name_Planned_Opportunity_Duration_mean',\n",
    " 'Product_Name_Planned_Time_Until_Deliver_mean',\n",
    " 'Product_Name_Planned_Opportunity_Duration_mean',\n",
    " 'Territory',\n",
    " 'Account_Owner',\n",
    " 'Account_Type',\n",
    " 'Opportunity_Type',\n",
    " 'Planned_Opportunity_Duration',\n",
    " 'Bureaucratic_Code_ASP_(converted)_std',\n",
    " 'Bureaucratic_Code_Total_Amount_mean',\n",
    " 'Bureaucratic_Code_Total_Product_Family_Region_Last_Month_mean',\n",
    " 'Bureaucratic_Code_Total_Product_Family_Region_Last_Month_std',\n",
    " 'Bureaucratic_Code_Total_Products_Region_Last_Week_std',\n",
    " 'Bureaucratic_Code_Total_Products_Region_Last_Month_mean',\n",
    " 'Bureaucratic_Code_Planned_Deliver_Duration_mean',\n",
    " 'Bureaucratic_Code_Planned_Deliver_Duration_std',\n",
    " 'Bureaucratic_Code_Actual_Opportunity_Duration_std',\n",
    " 'Bureaucratic_Code_Planned_Time_Until_Deliver_mean',\n",
    " 'Bureaucratic_Code_Planned_Time_Until_Deliver_std',\n",
    " 'Bureaucratic_Code_Planned_Opportunity_Duration_mean',\n",
    " 'Bureaucratic_Code_Product_Amount_Deviation_of_Product_Family_rate_std',\n",
    " 'Bureaucratic_Code_Opportunity_Duration_Ratio_std',\n",
    " 'Bureaucratic_Code_Opportunity_Total_Amount_Region_avg_std',\n",
    " 'Bureaucratic_Code_Opportunity_TRF_Region_std_Ratio_std',\n",
    " 'Bureaucratic_Code_Opportunity_Duration_by_Account_Type_std',\n",
    " 'Bureaucratic_Code_ASP_by_Billing_Country_mean_std',\n",
    " 'Bureaucratic_Code_Opportunity_Duration_by_Product_Family_mean_mean',\n",
    " 'Bureaucratic_Code_Buro_Approved_by_Product_Family_std',\n",
    " 'Account_Name_Planned_Opportunity_Duration_mean',\n",
    " 'Opportunity_Type_Planned_Time_Until_Deliver_mean',\n",
    " 'Opportunity_Type_Opportunity_Duration_by_Account_Type_mean',\n",
    " 'Last_Modified_By_Planned_Time_Until_Deliver_mean',\n",
    " 'Last_Modified_By_Opportunity_Total_Amount_Region_avg_std',\n",
    " 'Last_Modified_By_Opportunity_Total_Amount_Region_std_std',\n",
    " 'Last_Modified_By_Opportunity_TRF_Region_avg_std',\n",
    " 'Last_Modified_By_ASP_by_Region_mean_std',\n",
    " 'Product_Family_Planned_Opportunity_Duration_mean',\n",
    " 'Product_Name_Planned_Time_Until_Deliver_std',\n",
    " 'Product_Name_Opportunity_Duration_by_Account_Type_mean',\n",
    " 'Bureaucratic_Code_Billing_Country_unique',\n",
    " 'Bureaucratic_Code_Opportunity_Name_unique',\n",
    " 'Bureaucratic_Code_Opportunity_Type_unique',\n",
    " 'Bureaucratic_Code_Total_Amount_Currency_unique',\n",
    " 'Last_Modified_By_Territory_unique',\n",
    " 'Last_Modified_By_Billing_Country_unique',\n",
    " 'Last_Modified_By_Account_Name_unique',\n",
    " 'Last_Modified_By_Account_Owner_unique',\n",
    " 'Last_Modified_By_Account_Type_unique',\n",
    " 'Last_Modified_By_Product_Name_unique',\n",
    " 'Last_Modified_By_Total_Amount_Currency_unique',\n",
    " 'Region',\n",
    " 'Billing_Country']\n",
    "\n",
    "drop = list(entrenamiento.columns)\n",
    "for col in a_usar:\n",
    "    drop.remove(col)\n",
    "    \n",
    "entrenamiento = entrenamiento.drop(columns = drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(columns = drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "limpiar(entrenamiento,test,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=100, prediction_data=True).fit(entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: 77, 0: 128, 1: 152, 2: 1501, 3: 1479, 4: 2288, 5: 6515}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "clusters = dict()\n",
    "labels = set(clusterer.labels_)\n",
    "for l in labels:\n",
    "    cluster = [1 if i == l else 0 for i in clusterer.labels_]\n",
    "    suma = 0\n",
    "    for i in cluster:\n",
    "        suma += i\n",
    "    clusters[l] = suma\n",
    "pprint.PrettyPrinter().pprint(clusters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrenamiento_con_cluster = entrenamiento.copy()\n",
    "entrenamiento_con_cluster[\"Cluster\"] = clusterer.labels_\n",
    "entrenamientos_separados = []\n",
    "for label in labels:\n",
    "    entrenamiento_cluster = entrenamiento_con_cluster.loc[entrenamiento_con_cluster[\"Cluster\"] == label]\n",
    "    entrenamientos_separados.append(entrenamiento_cluster)\n",
    "entrenamientos_separados = [i.drop(columns = [\"Cluster\"]) for i in entrenamientos_separados] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 4, 4, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels, strengths = hdbscan.approximate_predict(clusterer, test)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: 33, 0: 42, 1: 109, 2: 0, 3: 368, 4: 1995, 5: 4}\n"
     ]
    }
   ],
   "source": [
    "clusters = dict()\n",
    "labels = set(clusterer.labels_)\n",
    "for l in labels:\n",
    "    cluster = [1 if i == l else 0 for i in test_labels]\n",
    "    suma = 0\n",
    "    for i in cluster:\n",
    "        suma += i\n",
    "    clusters[l] = suma\n",
    "pprint.PrettyPrinter().pprint(clusters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
