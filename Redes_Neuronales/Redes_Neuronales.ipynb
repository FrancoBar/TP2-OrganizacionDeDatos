{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQfQ2ppfQasK"
   },
   "source": [
    "# Redes Neuronales\n",
    "## Preparacion de datos para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "8OQLMHvqkvdV",
    "outputId": "4d6f4c0b-232b-468e-8734-083a58109f70"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Region</th>\n",
       "      <th>Territory</th>\n",
       "      <th>Pricing, Delivery_Terms_Quote_Appr</th>\n",
       "      <th>Pricing, Delivery_Terms_Approved</th>\n",
       "      <th>Bureaucratic_Code_0_Approval</th>\n",
       "      <th>Bureaucratic_Code_0_Approved</th>\n",
       "      <th>Submitted_for_Approval</th>\n",
       "      <th>Bureaucratic_Code</th>\n",
       "      <th>Account_Created_Date</th>\n",
       "      <th>...</th>\n",
       "      <th>Buro_Approved_by_Billing_Country_mean</th>\n",
       "      <th>Buro_Approved_by_Billing_Country_std</th>\n",
       "      <th>Opportunity_Duration_by_Product_Family_mean</th>\n",
       "      <th>Opportunity_Duration_by_Product_Family_std</th>\n",
       "      <th>Total_Amount_by_Product_Family_mean</th>\n",
       "      <th>Total_Amount_by_Product_Family_std</th>\n",
       "      <th>Buro_Approved_by_Product_Family</th>\n",
       "      <th>Family_Duration</th>\n",
       "      <th>Region_Duration</th>\n",
       "      <th>Territory_Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12714</td>\n",
       "      <td>EMEA</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_4</td>\n",
       "      <td>2013-07-27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246835</td>\n",
       "      <td>0.431854</td>\n",
       "      <td>231.896694</td>\n",
       "      <td>113.273701</td>\n",
       "      <td>2.662164e+05</td>\n",
       "      <td>5.108615e+05</td>\n",
       "      <td>0.446370</td>\n",
       "      <td>-713.0</td>\n",
       "      <td>-1106</td>\n",
       "      <td>-1085.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18684</td>\n",
       "      <td>APAC</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_3</td>\n",
       "      <td>2014-01-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307380</td>\n",
       "      <td>0.461596</td>\n",
       "      <td>227.637202</td>\n",
       "      <td>144.868574</td>\n",
       "      <td>3.011984e+06</td>\n",
       "      <td>1.178595e+07</td>\n",
       "      <td>0.442007</td>\n",
       "      <td>-1282.0</td>\n",
       "      <td>-2072</td>\n",
       "      <td>-1261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20675</td>\n",
       "      <td>Americas</td>\n",
       "      <td>NW America</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_5</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305915</td>\n",
       "      <td>0.460884</td>\n",
       "      <td>220.969543</td>\n",
       "      <td>181.414190</td>\n",
       "      <td>6.810823e+06</td>\n",
       "      <td>2.429178e+07</td>\n",
       "      <td>0.495553</td>\n",
       "      <td>-1053.0</td>\n",
       "      <td>-1795</td>\n",
       "      <td>-1795.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID    Region    Territory  Pricing, Delivery_Terms_Quote_Appr  \\\n",
       "0  12714      EMEA       France                                   0   \n",
       "1  18684      APAC  Philippines                                   1   \n",
       "2  20675  Americas   NW America                                   1   \n",
       "\n",
       "   Pricing, Delivery_Terms_Approved  Bureaucratic_Code_0_Approval  \\\n",
       "0                                 0                             0   \n",
       "1                                 1                             1   \n",
       "2                                 0                             1   \n",
       "\n",
       "   Bureaucratic_Code_0_Approved  Submitted_for_Approval    Bureaucratic_Code  \\\n",
       "0                             0                       0  Bureaucratic_Code_4   \n",
       "1                             1                       0  Bureaucratic_Code_3   \n",
       "2                             0                       0  Bureaucratic_Code_5   \n",
       "\n",
       "  Account_Created_Date  ... Buro_Approved_by_Billing_Country_mean  \\\n",
       "0           2013-07-27  ...                              0.246835   \n",
       "1           2014-01-22  ...                              0.307380   \n",
       "2           2013-08-22  ...                              0.305915   \n",
       "\n",
       "  Buro_Approved_by_Billing_Country_std  \\\n",
       "0                             0.431854   \n",
       "1                             0.461596   \n",
       "2                             0.460884   \n",
       "\n",
       "  Opportunity_Duration_by_Product_Family_mean  \\\n",
       "0                                  231.896694   \n",
       "1                                  227.637202   \n",
       "2                                  220.969543   \n",
       "\n",
       "  Opportunity_Duration_by_Product_Family_std  \\\n",
       "0                                 113.273701   \n",
       "1                                 144.868574   \n",
       "2                                 181.414190   \n",
       "\n",
       "  Total_Amount_by_Product_Family_mean Total_Amount_by_Product_Family_std  \\\n",
       "0                        2.662164e+05                       5.108615e+05   \n",
       "1                        3.011984e+06                       1.178595e+07   \n",
       "2                        6.810823e+06                       2.429178e+07   \n",
       "\n",
       "  Buro_Approved_by_Product_Family Family_Duration Region_Duration  \\\n",
       "0                        0.446370          -713.0           -1106   \n",
       "1                        0.442007         -1282.0           -2072   \n",
       "2                        0.495553         -1053.0           -1795   \n",
       "\n",
       "  Territory_Duration  \n",
       "0            -1085.0  \n",
       "1            -1261.0  \n",
       "2            -1795.0  \n",
       "\n",
       "[3 rows x 85 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import Utilidades as ut\n",
    "import Modelos as md\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_train = pd.read_pickle(\"../Archivos/Neuronales_entrenamiento.pkl\")\n",
    "df_test = pd.read_pickle(\"../Archivos/Neuronales_validacion.pkl\")\n",
    "\n",
    "x_train, y_train = ut.split_labels(df_train)\n",
    "x_test, y_test = ut.split_labels(df_test)\n",
    "\n",
    "#Convertimos las fechas a numeros (cantidad de dias transcurridos) y luego las normalizamos\n",
    "#x_train, x_test = ut.conversion_fechas(x_train, x_test)\n",
    "#x_train, x_test = ut.codificar_categoricas(x_train, y_train, x_test)\n",
    "#x_train, x_test = ut.normalizacion_numericas(x_train, x_test, modo='normalizacion')\n",
    "\n",
    "x_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Region</th>\n",
       "      <th>Territory</th>\n",
       "      <th>Pricing, Delivery_Terms_Quote_Appr</th>\n",
       "      <th>Pricing, Delivery_Terms_Approved</th>\n",
       "      <th>Bureaucratic_Code_0_Approval</th>\n",
       "      <th>Bureaucratic_Code_0_Approved</th>\n",
       "      <th>Submitted_for_Approval</th>\n",
       "      <th>Bureaucratic_Code</th>\n",
       "      <th>Account_Created_Date</th>\n",
       "      <th>...</th>\n",
       "      <th>Buro_Approved_by_Billing_Country_mean</th>\n",
       "      <th>Buro_Approved_by_Billing_Country_std</th>\n",
       "      <th>Opportunity_Duration_by_Product_Family_mean</th>\n",
       "      <th>Opportunity_Duration_by_Product_Family_std</th>\n",
       "      <th>Total_Amount_by_Product_Family_mean</th>\n",
       "      <th>Total_Amount_by_Product_Family_std</th>\n",
       "      <th>Buro_Approved_by_Product_Family</th>\n",
       "      <th>Family_Duration</th>\n",
       "      <th>Region_Duration</th>\n",
       "      <th>Territory_Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12714</td>\n",
       "      <td>EMEA</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_4</td>\n",
       "      <td>2013-07-27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246835</td>\n",
       "      <td>0.431854</td>\n",
       "      <td>231.896694</td>\n",
       "      <td>113.273701</td>\n",
       "      <td>2.662164e+05</td>\n",
       "      <td>5.108615e+05</td>\n",
       "      <td>0.446370</td>\n",
       "      <td>-713.0</td>\n",
       "      <td>-1106</td>\n",
       "      <td>-1085.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18684</td>\n",
       "      <td>APAC</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_3</td>\n",
       "      <td>2014-01-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307380</td>\n",
       "      <td>0.461596</td>\n",
       "      <td>227.637202</td>\n",
       "      <td>144.868574</td>\n",
       "      <td>3.011984e+06</td>\n",
       "      <td>1.178595e+07</td>\n",
       "      <td>0.442007</td>\n",
       "      <td>-1282.0</td>\n",
       "      <td>-2072</td>\n",
       "      <td>-1261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20675</td>\n",
       "      <td>Americas</td>\n",
       "      <td>NW America</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_5</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305915</td>\n",
       "      <td>0.460884</td>\n",
       "      <td>220.969543</td>\n",
       "      <td>181.414190</td>\n",
       "      <td>6.810823e+06</td>\n",
       "      <td>2.429178e+07</td>\n",
       "      <td>0.495553</td>\n",
       "      <td>-1053.0</td>\n",
       "      <td>-1795</td>\n",
       "      <td>-1795.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20682</td>\n",
       "      <td>Americas</td>\n",
       "      <td>NW America</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_5</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305915</td>\n",
       "      <td>0.460884</td>\n",
       "      <td>220.969543</td>\n",
       "      <td>181.414190</td>\n",
       "      <td>6.810823e+06</td>\n",
       "      <td>2.429178e+07</td>\n",
       "      <td>0.495553</td>\n",
       "      <td>-1084.0</td>\n",
       "      <td>-1826</td>\n",
       "      <td>-1826.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20513</td>\n",
       "      <td>Americas</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_5</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305915</td>\n",
       "      <td>0.460884</td>\n",
       "      <td>268.402985</td>\n",
       "      <td>163.865255</td>\n",
       "      <td>3.454407e+06</td>\n",
       "      <td>9.303901e+06</td>\n",
       "      <td>0.499851</td>\n",
       "      <td>-1364.0</td>\n",
       "      <td>-2008</td>\n",
       "      <td>-2038.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12076</th>\n",
       "      <td>20616</td>\n",
       "      <td>Americas</td>\n",
       "      <td>Central US</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_5</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305915</td>\n",
       "      <td>0.460884</td>\n",
       "      <td>152.858333</td>\n",
       "      <td>82.009970</td>\n",
       "      <td>2.149203e+04</td>\n",
       "      <td>5.028947e+04</td>\n",
       "      <td>0.460179</td>\n",
       "      <td>-832.0</td>\n",
       "      <td>-1242</td>\n",
       "      <td>-574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12077</th>\n",
       "      <td>20642</td>\n",
       "      <td>Americas</td>\n",
       "      <td>Central US</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_4</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305915</td>\n",
       "      <td>0.460884</td>\n",
       "      <td>165.431818</td>\n",
       "      <td>75.534498</td>\n",
       "      <td>7.303231e+05</td>\n",
       "      <td>1.443275e+06</td>\n",
       "      <td>0.254972</td>\n",
       "      <td>-408.0</td>\n",
       "      <td>-1205</td>\n",
       "      <td>-537.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12078</th>\n",
       "      <td>20619</td>\n",
       "      <td>Americas</td>\n",
       "      <td>Central US</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_5</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305915</td>\n",
       "      <td>0.460884</td>\n",
       "      <td>231.896694</td>\n",
       "      <td>113.273701</td>\n",
       "      <td>2.662164e+05</td>\n",
       "      <td>5.108615e+05</td>\n",
       "      <td>0.446370</td>\n",
       "      <td>-972.0</td>\n",
       "      <td>-1242</td>\n",
       "      <td>-574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12079</th>\n",
       "      <td>8546</td>\n",
       "      <td>Japan</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_4</td>\n",
       "      <td>2017-09-14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156338</td>\n",
       "      <td>0.363223</td>\n",
       "      <td>237.292453</td>\n",
       "      <td>132.292686</td>\n",
       "      <td>5.628536e+06</td>\n",
       "      <td>4.608606e+07</td>\n",
       "      <td>0.441924</td>\n",
       "      <td>-1158.0</td>\n",
       "      <td>-1289</td>\n",
       "      <td>-1257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12080</th>\n",
       "      <td>12168</td>\n",
       "      <td>Americas</td>\n",
       "      <td>NW America</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bureaucratic_Code_4</td>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305915</td>\n",
       "      <td>0.460884</td>\n",
       "      <td>160.653846</td>\n",
       "      <td>89.868090</td>\n",
       "      <td>1.591498e+06</td>\n",
       "      <td>3.643825e+06</td>\n",
       "      <td>0.490040</td>\n",
       "      <td>-483.0</td>\n",
       "      <td>-1246</td>\n",
       "      <td>-1246.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12081 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    Region    Territory  Pricing, Delivery_Terms_Quote_Appr  \\\n",
       "0      12714      EMEA       France                                   0   \n",
       "1      18684      APAC  Philippines                                   1   \n",
       "2      20675  Americas   NW America                                   1   \n",
       "3      20682  Americas   NW America                                   1   \n",
       "4      20513  Americas         None                                   1   \n",
       "...      ...       ...          ...                                 ...   \n",
       "12076  20616  Americas   Central US                                   1   \n",
       "12077  20642  Americas   Central US                                   1   \n",
       "12078  20619  Americas   Central US                                   1   \n",
       "12079   8546     Japan         None                                   1   \n",
       "12080  12168  Americas   NW America                                   1   \n",
       "\n",
       "       Pricing, Delivery_Terms_Approved  Bureaucratic_Code_0_Approval  \\\n",
       "0                                     0                             0   \n",
       "1                                     1                             1   \n",
       "2                                     0                             1   \n",
       "3                                     0                             1   \n",
       "4                                     0                             1   \n",
       "...                                 ...                           ...   \n",
       "12076                                 0                             0   \n",
       "12077                                 1                             0   \n",
       "12078                                 0                             0   \n",
       "12079                                 1                             0   \n",
       "12080                                 1                             0   \n",
       "\n",
       "       Bureaucratic_Code_0_Approved  Submitted_for_Approval  \\\n",
       "0                                 0                       0   \n",
       "1                                 1                       0   \n",
       "2                                 0                       0   \n",
       "3                                 0                       0   \n",
       "4                                 0                       0   \n",
       "...                             ...                     ...   \n",
       "12076                             0                       0   \n",
       "12077                             0                       0   \n",
       "12078                             0                       0   \n",
       "12079                             0                       0   \n",
       "12080                             0                       0   \n",
       "\n",
       "         Bureaucratic_Code Account_Created_Date  ...  \\\n",
       "0      Bureaucratic_Code_4           2013-07-27  ...   \n",
       "1      Bureaucratic_Code_3           2014-01-22  ...   \n",
       "2      Bureaucratic_Code_5           2013-08-22  ...   \n",
       "3      Bureaucratic_Code_5           2013-08-22  ...   \n",
       "4      Bureaucratic_Code_5           2013-08-22  ...   \n",
       "...                    ...                  ...  ...   \n",
       "12076  Bureaucratic_Code_5           2013-08-22  ...   \n",
       "12077  Bureaucratic_Code_4           2013-08-22  ...   \n",
       "12078  Bureaucratic_Code_5           2013-08-22  ...   \n",
       "12079  Bureaucratic_Code_4           2017-09-14  ...   \n",
       "12080  Bureaucratic_Code_4           2017-12-30  ...   \n",
       "\n",
       "      Buro_Approved_by_Billing_Country_mean  \\\n",
       "0                                  0.246835   \n",
       "1                                  0.307380   \n",
       "2                                  0.305915   \n",
       "3                                  0.305915   \n",
       "4                                  0.305915   \n",
       "...                                     ...   \n",
       "12076                              0.305915   \n",
       "12077                              0.305915   \n",
       "12078                              0.305915   \n",
       "12079                              0.156338   \n",
       "12080                              0.305915   \n",
       "\n",
       "      Buro_Approved_by_Billing_Country_std  \\\n",
       "0                                 0.431854   \n",
       "1                                 0.461596   \n",
       "2                                 0.460884   \n",
       "3                                 0.460884   \n",
       "4                                 0.460884   \n",
       "...                                    ...   \n",
       "12076                             0.460884   \n",
       "12077                             0.460884   \n",
       "12078                             0.460884   \n",
       "12079                             0.363223   \n",
       "12080                             0.460884   \n",
       "\n",
       "      Opportunity_Duration_by_Product_Family_mean  \\\n",
       "0                                      231.896694   \n",
       "1                                      227.637202   \n",
       "2                                      220.969543   \n",
       "3                                      220.969543   \n",
       "4                                      268.402985   \n",
       "...                                           ...   \n",
       "12076                                  152.858333   \n",
       "12077                                  165.431818   \n",
       "12078                                  231.896694   \n",
       "12079                                  237.292453   \n",
       "12080                                  160.653846   \n",
       "\n",
       "      Opportunity_Duration_by_Product_Family_std  \\\n",
       "0                                     113.273701   \n",
       "1                                     144.868574   \n",
       "2                                     181.414190   \n",
       "3                                     181.414190   \n",
       "4                                     163.865255   \n",
       "...                                          ...   \n",
       "12076                                  82.009970   \n",
       "12077                                  75.534498   \n",
       "12078                                 113.273701   \n",
       "12079                                 132.292686   \n",
       "12080                                  89.868090   \n",
       "\n",
       "      Total_Amount_by_Product_Family_mean Total_Amount_by_Product_Family_std  \\\n",
       "0                            2.662164e+05                       5.108615e+05   \n",
       "1                            3.011984e+06                       1.178595e+07   \n",
       "2                            6.810823e+06                       2.429178e+07   \n",
       "3                            6.810823e+06                       2.429178e+07   \n",
       "4                            3.454407e+06                       9.303901e+06   \n",
       "...                                   ...                                ...   \n",
       "12076                        2.149203e+04                       5.028947e+04   \n",
       "12077                        7.303231e+05                       1.443275e+06   \n",
       "12078                        2.662164e+05                       5.108615e+05   \n",
       "12079                        5.628536e+06                       4.608606e+07   \n",
       "12080                        1.591498e+06                       3.643825e+06   \n",
       "\n",
       "      Buro_Approved_by_Product_Family Family_Duration Region_Duration  \\\n",
       "0                            0.446370          -713.0           -1106   \n",
       "1                            0.442007         -1282.0           -2072   \n",
       "2                            0.495553         -1053.0           -1795   \n",
       "3                            0.495553         -1084.0           -1826   \n",
       "4                            0.499851         -1364.0           -2008   \n",
       "...                               ...             ...             ...   \n",
       "12076                        0.460179          -832.0           -1242   \n",
       "12077                        0.254972          -408.0           -1205   \n",
       "12078                        0.446370          -972.0           -1242   \n",
       "12079                        0.441924         -1158.0           -1289   \n",
       "12080                        0.490040          -483.0           -1246   \n",
       "\n",
       "      Territory_Duration  \n",
       "0                -1085.0  \n",
       "1                -1261.0  \n",
       "2                -1795.0  \n",
       "3                -1826.0  \n",
       "4                -2038.0  \n",
       "...                  ...  \n",
       "12076             -574.0  \n",
       "12077             -537.0  \n",
       "12078             -574.0  \n",
       "12079            -1257.0  \n",
       "12080            -1246.0  \n",
       "\n",
       "[12081 rows x 85 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "\n",
    "\n",
    "columnas = df_train.select_dtypes(include='category').columns\n",
    "encoder = TargetEncoder(return_df=True, cols=columnas)\n",
    "sarasa = encoder.fit(x_train, y_train)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Region</th>\n",
       "      <th>Territory</th>\n",
       "      <th>Pricing, Delivery_Terms_Quote_Appr</th>\n",
       "      <th>Pricing, Delivery_Terms_Approved</th>\n",
       "      <th>Bureaucratic_Code_0_Approval</th>\n",
       "      <th>Bureaucratic_Code_0_Approved</th>\n",
       "      <th>Submitted_for_Approval</th>\n",
       "      <th>Bureaucratic_Code</th>\n",
       "      <th>Account_Created_Date</th>\n",
       "      <th>...</th>\n",
       "      <th>Buro_Approved_by_Billing_Country_mean</th>\n",
       "      <th>Buro_Approved_by_Billing_Country_std</th>\n",
       "      <th>Opportunity_Duration_by_Product_Family_mean</th>\n",
       "      <th>Opportunity_Duration_by_Product_Family_std</th>\n",
       "      <th>Total_Amount_by_Product_Family_mean</th>\n",
       "      <th>Total_Amount_by_Product_Family_std</th>\n",
       "      <th>Buro_Approved_by_Product_Family</th>\n",
       "      <th>Family_Duration</th>\n",
       "      <th>Region_Duration</th>\n",
       "      <th>Territory_Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.85</td>\n",
       "      <td>-3.71</td>\n",
       "      <td>-2.20</td>\n",
       "      <td>-2.05</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>-3.71</td>\n",
       "      <td>-2.20</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.30</td>\n",
       "      <td>-3.71</td>\n",
       "      <td>-2.20</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.31</td>\n",
       "      <td>-3.71</td>\n",
       "      <td>-2.20</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.28</td>\n",
       "      <td>-3.71</td>\n",
       "      <td>-2.20</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-1.41</td>\n",
       "      <td>-1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12076</th>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12077</th>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12078</th>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12079</th>\n",
       "      <td>-1.45</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.92</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12080</th>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12081 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Region  Territory  Pricing, Delivery_Terms_Quote_Appr  \\\n",
       "0     -0.85   -3.71      -2.20                               -2.05   \n",
       "1      0.02   -3.71      -2.20                                0.49   \n",
       "2      0.30   -3.71      -2.20                                0.49   \n",
       "3      0.31   -3.71      -2.20                                0.49   \n",
       "4      0.28   -3.71      -2.20                                0.49   \n",
       "...     ...     ...        ...                                 ...   \n",
       "12076  0.30   -0.27       0.02                                0.49   \n",
       "12077  0.30   -0.27       0.04                                0.49   \n",
       "12078  0.30   -0.27       0.06                                0.49   \n",
       "12079 -1.45    1.60       0.45                                0.49   \n",
       "12080 -0.93   -0.27       0.51                                0.49   \n",
       "\n",
       "       Pricing, Delivery_Terms_Approved  Bureaucratic_Code_0_Approval  \\\n",
       "0                                 -1.13                         -0.90   \n",
       "1                                  0.88                          1.11   \n",
       "2                                 -1.13                          1.11   \n",
       "3                                 -1.13                          1.11   \n",
       "4                                 -1.13                          1.11   \n",
       "...                                 ...                           ...   \n",
       "12076                             -1.13                         -0.90   \n",
       "12077                              0.88                         -0.90   \n",
       "12078                             -1.13                         -0.90   \n",
       "12079                              0.88                         -0.90   \n",
       "12080                              0.88                         -0.90   \n",
       "\n",
       "       Bureaucratic_Code_0_Approved  Submitted_for_Approval  \\\n",
       "0                             -0.64                     0.0   \n",
       "1                              1.56                     0.0   \n",
       "2                             -0.64                     0.0   \n",
       "3                             -0.64                     0.0   \n",
       "4                             -0.64                     0.0   \n",
       "...                             ...                     ...   \n",
       "12076                         -0.64                     0.0   \n",
       "12077                         -0.64                     0.0   \n",
       "12078                         -0.64                     0.0   \n",
       "12079                         -0.64                     0.0   \n",
       "12080                         -0.64                     0.0   \n",
       "\n",
       "       Bureaucratic_Code  Account_Created_Date  ...  \\\n",
       "0                  -2.06                 -1.35  ...   \n",
       "1                  -2.06                 -0.96  ...   \n",
       "2                  -2.06                 -1.29  ...   \n",
       "3                  -2.06                 -1.29  ...   \n",
       "4                  -2.06                 -1.29  ...   \n",
       "...                  ...                   ...  ...   \n",
       "12076              -1.26                 -1.29  ...   \n",
       "12077               0.62                 -1.29  ...   \n",
       "12078              -1.26                 -1.29  ...   \n",
       "12079               0.62                  1.92  ...   \n",
       "12080               0.62                  2.15  ...   \n",
       "\n",
       "       Buro_Approved_by_Billing_Country_mean  \\\n",
       "0                                      -0.31   \n",
       "1                                       0.11   \n",
       "2                                       0.10   \n",
       "3                                       0.10   \n",
       "4                                       0.10   \n",
       "...                                      ...   \n",
       "12076                                   0.10   \n",
       "12077                                   0.10   \n",
       "12078                                   0.10   \n",
       "12079                                  -0.93   \n",
       "12080                                   0.10   \n",
       "\n",
       "       Buro_Approved_by_Billing_Country_std  \\\n",
       "0                                      0.08   \n",
       "1                                      0.54   \n",
       "2                                      0.53   \n",
       "3                                      0.53   \n",
       "4                                      0.53   \n",
       "...                                     ...   \n",
       "12076                                  0.53   \n",
       "12077                                  0.53   \n",
       "12078                                  0.53   \n",
       "12079                                 -1.00   \n",
       "12080                                  0.53   \n",
       "\n",
       "       Opportunity_Duration_by_Product_Family_mean  \\\n",
       "0                                            -0.01   \n",
       "1                                            -0.12   \n",
       "2                                            -0.28   \n",
       "3                                            -0.28   \n",
       "4                                             0.86   \n",
       "...                                            ...   \n",
       "12076                                        -1.91   \n",
       "12077                                        -1.61   \n",
       "12078                                        -0.01   \n",
       "12079                                         0.12   \n",
       "12080                                        -1.72   \n",
       "\n",
       "       Opportunity_Duration_by_Product_Family_std  \\\n",
       "0                                           -0.43   \n",
       "1                                            0.46   \n",
       "2                                            1.48   \n",
       "3                                            1.48   \n",
       "4                                            0.99   \n",
       "...                                           ...   \n",
       "12076                                       -1.30   \n",
       "12077                                       -1.48   \n",
       "12078                                       -0.43   \n",
       "12079                                        0.10   \n",
       "12080                                       -1.08   \n",
       "\n",
       "       Total_Amount_by_Product_Family_mean  \\\n",
       "0                                    -0.75   \n",
       "1                                     0.40   \n",
       "2                                     1.99   \n",
       "3                                     1.99   \n",
       "4                                     0.59   \n",
       "...                                    ...   \n",
       "12076                                -0.85   \n",
       "12077                                -0.55   \n",
       "12078                                -0.75   \n",
       "12079                                 1.50   \n",
       "12080                                -0.19   \n",
       "\n",
       "       Total_Amount_by_Product_Family_std  Buro_Approved_by_Product_Family  \\\n",
       "0                                   -0.64                             0.26   \n",
       "1                                    0.31                             0.22   \n",
       "2                                    1.37                             0.69   \n",
       "3                                    1.37                             0.69   \n",
       "4                                    0.10                             0.73   \n",
       "...                                   ...                              ...   \n",
       "12076                               -0.68                             0.38   \n",
       "12077                               -0.56                            -1.45   \n",
       "12078                               -0.64                             0.26   \n",
       "12079                                3.20                             0.22   \n",
       "12080                               -0.37                             0.64   \n",
       "\n",
       "       Family_Duration  Region_Duration  Territory_Duration  \n",
       "0                 0.61             1.97                0.84  \n",
       "1                -0.82            -1.65                0.41  \n",
       "2                -0.25            -0.61               -0.90  \n",
       "3                -0.32            -0.73               -0.98  \n",
       "4                -1.03            -1.41               -1.50  \n",
       "...                ...              ...                 ...  \n",
       "12076             0.31             1.46                2.10  \n",
       "12077             1.38             1.60                2.19  \n",
       "12078            -0.04             1.46                2.10  \n",
       "12079            -0.51             1.29                0.42  \n",
       "12080             1.19             1.45                0.45  \n",
       "\n",
       "[12081 rows x 85 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oSHQttP_cuK"
   },
   "source": [
    "## Creacion del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "-rN3ODhM_Zxm",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "81bc755e-a46a-4f34-eced-99d6297b67e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando hiperparametros desde el archivo: '../Archivos/Neuronales_best_hyperparam.json'\n",
      "Epoch 1/250\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 2655.6018 - val_loss: 775.9537 - lr: 0.0022\n",
      "Epoch 2/250\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 228.2518 - val_loss: 33.2166 - lr: 0.0022\n",
      "Epoch 3/250\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 21.4239 - val_loss: 17.4182 - lr: 0.0022\n",
      "Epoch 4/250\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 17.0368 - val_loss: 17.0190 - lr: 0.0022\n",
      "Epoch 5/250\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 16.9447 - val_loss: 17.0383 - lr: 0.0022\n",
      "Epoch 6/250\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 16.9473 - val_loss: 17.0842 - lr: 0.0022\n",
      "Epoch 7/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 16.9292 - val_loss: 16.9428 - lr: 0.0022\n",
      "Epoch 8/250\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 16.9123 - val_loss: 17.0528 - lr: 0.0022\n",
      "Epoch 9/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 16.9247 - val_loss: 16.9955 - lr: 0.0022\n",
      "Epoch 10/250\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 16.9188 - val_loss: 16.9495 - lr: 0.0022\n",
      "Epoch 11/250\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 16.9043 - val_loss: 17.0285 - lr: 0.0022\n",
      "Epoch 12/250\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 16.9106 - val_loss: 16.9863 - lr: 0.0022\n",
      "Epoch 13/250\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 9.9869 - val_loss: 8.3066 - lr: 0.0011\n",
      "Epoch 14/250\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 8.4741 - val_loss: 8.5141 - lr: 0.0011\n",
      "Epoch 15/250\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 8.4190 - val_loss: 8.5649 - lr: 0.0011\n",
      "Epoch 16/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 8.4402 - val_loss: 8.4119 - lr: 0.0011\n",
      "Epoch 17/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 8.4292 - val_loss: 8.3804 - lr: 0.0011\n",
      "Epoch 18/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 8.4258 - val_loss: 8.2688 - lr: 0.0011\n",
      "Epoch 19/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 8.4255 - val_loss: 8.3537 - lr: 0.0011\n",
      "Epoch 20/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 8.4037 - val_loss: 8.5751 - lr: 0.0011\n",
      "Epoch 21/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 8.4446 - val_loss: 8.2357 - lr: 0.0011\n",
      "Epoch 22/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 8.4089 - val_loss: 8.3567 - lr: 0.0011\n",
      "Epoch 23/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 8.4078 - val_loss: 8.3198 - lr: 0.0011\n",
      "Epoch 24/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 8.4289 - val_loss: 8.3644 - lr: 0.0011\n",
      "Epoch 25/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 8.4025 - val_loss: 8.6448 - lr: 0.0011\n",
      "Epoch 26/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 8.4158 - val_loss: 8.1181 - lr: 0.0011\n",
      "Epoch 27/250\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 8.4198 - val_loss: 8.2486 - lr: 0.0011\n",
      "Epoch 28/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 8.4159 - val_loss: 8.3239 - lr: 0.0011\n",
      "Epoch 29/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 8.4005 - val_loss: 8.4226 - lr: 0.0011\n",
      "Epoch 30/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 8.4011 - val_loss: 8.5470 - lr: 0.0011\n",
      "Epoch 31/250\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 8.4218 - val_loss: 8.3240 - lr: 0.0011\n",
      "Epoch 32/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 5.2201 - val_loss: 4.5901 - lr: 5.5946e-04\n",
      "Epoch 33/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 4.5777 - val_loss: 4.5718 - lr: 5.5946e-04\n",
      "Epoch 34/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 4.5612 - val_loss: 4.6387 - lr: 5.5946e-04\n",
      "Epoch 35/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 4.5689 - val_loss: 4.5690 - lr: 5.5946e-04\n",
      "Epoch 36/250\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 4.5663 - val_loss: 4.5590 - lr: 5.5946e-04\n",
      "Epoch 37/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 4.5615 - val_loss: 4.5381 - lr: 5.5946e-04\n",
      "Epoch 38/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 4.5647 - val_loss: 4.5229 - lr: 5.5946e-04\n",
      "Epoch 39/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 4.5537 - val_loss: 4.6616 - lr: 5.5946e-04\n",
      "Epoch 40/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 4.5662 - val_loss: 4.4769 - lr: 5.5946e-04\n",
      "Epoch 41/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 4.5605 - val_loss: 4.5024 - lr: 5.5946e-04\n",
      "Epoch 42/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 4.5595 - val_loss: 4.5268 - lr: 5.5946e-04\n",
      "Epoch 43/250\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 4.5615 - val_loss: 4.5953 - lr: 5.5946e-04\n",
      "Epoch 44/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 4.5531 - val_loss: 4.7200 - lr: 5.5946e-04\n",
      "Epoch 45/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 4.5582 - val_loss: 4.4080 - lr: 5.5946e-04\n",
      "Epoch 46/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 4.5636 - val_loss: 4.5063 - lr: 5.5946e-04\n",
      "Epoch 47/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 4.5554 - val_loss: 4.5152 - lr: 5.5946e-04\n",
      "Epoch 48/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 4.5478 - val_loss: 4.5285 - lr: 5.5946e-04\n",
      "Epoch 49/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 4.5497 - val_loss: 4.6719 - lr: 5.5946e-04\n",
      "Epoch 50/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 4.5539 - val_loss: 4.5532 - lr: 5.5946e-04\n",
      "Epoch 51/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 2.9715 - val_loss: 2.6319 - lr: 2.7973e-04\n",
      "Epoch 52/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 2.6279 - val_loss: 2.6384 - lr: 2.7973e-04\n",
      "Epoch 53/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 2.6212 - val_loss: 2.6630 - lr: 2.7973e-04\n",
      "Epoch 54/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 2.6255 - val_loss: 2.6294 - lr: 2.7973e-04\n",
      "Epoch 55/250\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 2.6201 - val_loss: 2.6098 - lr: 2.7973e-04\n",
      "Epoch 56/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 2.6196 - val_loss: 2.5976 - lr: 2.7973e-04\n",
      "Epoch 57/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 2.6229 - val_loss: 2.6056 - lr: 2.7973e-04\n",
      "Epoch 58/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 2.6146 - val_loss: 2.6822 - lr: 2.7973e-04\n",
      "Epoch 59/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.6242 - val_loss: 2.5778 - lr: 2.7973e-04\n",
      "Epoch 60/250\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 2.6170 - val_loss: 2.5939 - lr: 2.7973e-04\n",
      "Epoch 61/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 2.6167 - val_loss: 2.5970 - lr: 2.7973e-04\n",
      "Epoch 62/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 2.6231 - val_loss: 2.6118 - lr: 2.7973e-04\n",
      "Epoch 63/250\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 2.6128 - val_loss: 2.7040 - lr: 2.7973e-04\n",
      "Epoch 64/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 2.6202 - val_loss: 2.5741 - lr: 2.7973e-04\n",
      "Epoch 65/250\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 2.6202 - val_loss: 2.5749 - lr: 2.7973e-04\n",
      "Epoch 66/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 2.6185 - val_loss: 2.5999 - lr: 2.7973e-04\n",
      "Epoch 67/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 2.6119 - val_loss: 2.6100 - lr: 2.7973e-04\n",
      "Epoch 68/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 2.6163 - val_loss: 2.6685 - lr: 2.7973e-04\n",
      "Epoch 69/250\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 2.6175 - val_loss: 2.6117 - lr: 2.7973e-04\n",
      "Epoch 70/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 1.8206 - val_loss: 1.6417 - lr: 1.3986e-04\n",
      "Epoch 71/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.6481 - val_loss: 1.6361 - lr: 1.3986e-04\n",
      "Epoch 72/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.6442 - val_loss: 1.6537 - lr: 1.3986e-04\n",
      "Epoch 73/250\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 1.6457 - val_loss: 1.6354 - lr: 1.3986e-04\n",
      "Epoch 74/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.6434 - val_loss: 1.6324 - lr: 1.3986e-04\n",
      "Epoch 75/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.6440 - val_loss: 1.6199 - lr: 1.3986e-04\n",
      "Epoch 76/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.6437 - val_loss: 1.6185 - lr: 1.3986e-04\n",
      "Epoch 77/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 1.6397 - val_loss: 1.6724 - lr: 1.3986e-04\n",
      "Epoch 78/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.6456 - val_loss: 1.6038 - lr: 1.3986e-04\n",
      "Epoch 79/250\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 1.6425 - val_loss: 1.6198 - lr: 1.3986e-04\n",
      "Epoch 80/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.6429 - val_loss: 1.6282 - lr: 1.3986e-04\n",
      "Epoch 81/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 1.6449 - val_loss: 1.6237 - lr: 1.3986e-04\n",
      "Epoch 82/250\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 1.6405 - val_loss: 1.6782 - lr: 1.3986e-04\n",
      "Epoch 83/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.6450 - val_loss: 1.6016 - lr: 1.3986e-04\n",
      "Epoch 84/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.6434 - val_loss: 1.6110 - lr: 1.3986e-04\n",
      "Epoch 85/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.6423 - val_loss: 1.6234 - lr: 1.3986e-04\n",
      "Epoch 86/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.6404 - val_loss: 1.6255 - lr: 1.3986e-04\n",
      "Epoch 87/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.6407 - val_loss: 1.6633 - lr: 1.3986e-04\n",
      "Epoch 88/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.6440 - val_loss: 1.6233 - lr: 1.3986e-04\n",
      "Epoch 89/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 1.2497 - val_loss: 1.1604 - lr: 6.9932e-05\n",
      "Epoch 90/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.1666 - val_loss: 1.1579 - lr: 6.9932e-05\n",
      "Epoch 91/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.1637 - val_loss: 1.1682 - lr: 6.9932e-05\n",
      "Epoch 92/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 1.1647 - val_loss: 1.1603 - lr: 6.9932e-05\n",
      "Epoch 93/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 1.1642 - val_loss: 1.1559 - lr: 6.9932e-05\n",
      "Epoch 94/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 1.1649 - val_loss: 1.1464 - lr: 6.9932e-05\n",
      "Epoch 95/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 1.1649 - val_loss: 1.1491 - lr: 6.9932e-05\n",
      "Epoch 96/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.1635 - val_loss: 1.1747 - lr: 6.9932e-05\n",
      "Epoch 97/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 1.1652 - val_loss: 1.1440 - lr: 6.9932e-05\n",
      "Epoch 98/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.1644 - val_loss: 1.1472 - lr: 6.9932e-05\n",
      "Epoch 99/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 1.1646 - val_loss: 1.1547 - lr: 6.9932e-05\n",
      "Epoch 100/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.1641 - val_loss: 1.1535 - lr: 6.9932e-05\n",
      "Epoch 101/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 1.1633 - val_loss: 1.1757 - lr: 6.9932e-05\n",
      "Epoch 102/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.1643 - val_loss: 1.1399 - lr: 6.9932e-05\n",
      "Epoch 103/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 1.1642 - val_loss: 1.1462 - lr: 6.9932e-05\n",
      "Epoch 104/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.1645 - val_loss: 1.1496 - lr: 6.9932e-05\n",
      "Epoch 105/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 1.1638 - val_loss: 1.1509 - lr: 6.9932e-05\n",
      "Epoch 106/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.1623 - val_loss: 1.1741 - lr: 6.9932e-05\n",
      "Epoch 107/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.1632 - val_loss: 1.1511 - lr: 6.9932e-05\n",
      "Epoch 108/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.9711 - val_loss: 0.9206 - lr: 3.4966e-05\n",
      "Epoch 109/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.9311 - val_loss: 0.9187 - lr: 3.4966e-05\n",
      "Epoch 110/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.9291 - val_loss: 0.9259 - lr: 3.4966e-05\n",
      "Epoch 111/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.9301 - val_loss: 0.9195 - lr: 3.4966e-05\n",
      "Epoch 112/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.9299 - val_loss: 0.9187 - lr: 3.4966e-05\n",
      "Epoch 113/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.9291 - val_loss: 0.9144 - lr: 3.4966e-05\n",
      "Epoch 114/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.9292 - val_loss: 0.9158 - lr: 3.4966e-05\n",
      "Epoch 115/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.9284 - val_loss: 0.9276 - lr: 3.4966e-05\n",
      "Epoch 116/250\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.9297 - val_loss: 0.9107 - lr: 3.4966e-05\n",
      "Epoch 117/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.9298 - val_loss: 0.9156 - lr: 3.4966e-05\n",
      "Epoch 118/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.9283 - val_loss: 0.9189 - lr: 3.4966e-05\n",
      "Epoch 119/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.9304 - val_loss: 0.9150 - lr: 3.4966e-05\n",
      "Epoch 120/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.9287 - val_loss: 0.9309 - lr: 3.4966e-05\n",
      "Epoch 121/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.9298 - val_loss: 0.9100 - lr: 3.4966e-05\n",
      "Epoch 122/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.9282 - val_loss: 0.9134 - lr: 3.4966e-05\n",
      "Epoch 123/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.9295 - val_loss: 0.9171 - lr: 3.4966e-05\n",
      "Epoch 124/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.9284 - val_loss: 0.9166 - lr: 3.4966e-05\n",
      "Epoch 125/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.9282 - val_loss: 0.9278 - lr: 3.4966e-05\n",
      "Epoch 126/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.9288 - val_loss: 0.9146 - lr: 3.4966e-05\n",
      "Epoch 127/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.8313 - val_loss: 0.8006 - lr: 1.7483e-05\n",
      "Epoch 128/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.8110 - val_loss: 0.7984 - lr: 1.7483e-05\n",
      "Epoch 129/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.8105 - val_loss: 0.8019 - lr: 1.7483e-05\n",
      "Epoch 130/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.8114 - val_loss: 0.8008 - lr: 1.7483e-05\n",
      "Epoch 131/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.8100 - val_loss: 0.7982 - lr: 1.7483e-05\n",
      "Epoch 132/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.8099 - val_loss: 0.7973 - lr: 1.7483e-05\n",
      "Epoch 133/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.8105 - val_loss: 0.7973 - lr: 1.7483e-05\n",
      "Epoch 134/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.8105 - val_loss: 0.8036 - lr: 1.7483e-05\n",
      "Epoch 135/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.8123 - val_loss: 0.7953 - lr: 1.7483e-05\n",
      "Epoch 136/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.8100 - val_loss: 0.7963 - lr: 1.7483e-05\n",
      "Epoch 137/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.8094 - val_loss: 0.7994 - lr: 1.7483e-05\n",
      "Epoch 138/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.8114 - val_loss: 0.7977 - lr: 1.7483e-05\n",
      "Epoch 139/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.8105 - val_loss: 0.8048 - lr: 1.7483e-05\n",
      "Epoch 140/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.8098 - val_loss: 0.7954 - lr: 1.7483e-05\n",
      "Epoch 141/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7608 - val_loss: 0.7385 - lr: 8.7415e-06\n",
      "Epoch 142/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.7522 - val_loss: 0.7390 - lr: 8.7415e-06\n",
      "Epoch 143/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7503 - val_loss: 0.7398 - lr: 8.7415e-06\n",
      "Epoch 144/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.7501 - val_loss: 0.7389 - lr: 8.7415e-06\n",
      "Epoch 145/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.7508 - val_loss: 0.7379 - lr: 8.7415e-06\n",
      "Epoch 146/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7509 - val_loss: 0.7369 - lr: 8.7415e-06\n",
      "Epoch 147/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7509 - val_loss: 0.7377 - lr: 8.7415e-06\n",
      "Epoch 148/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.7490 - val_loss: 0.7410 - lr: 8.7415e-06\n",
      "Epoch 149/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.7513 - val_loss: 0.7367 - lr: 8.7415e-06\n",
      "Epoch 150/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.7505 - val_loss: 0.7371 - lr: 8.7415e-06\n",
      "Epoch 151/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7507 - val_loss: 0.7388 - lr: 8.7415e-06\n",
      "Epoch 152/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7503 - val_loss: 0.7377 - lr: 8.7415e-06\n",
      "Epoch 153/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7511 - val_loss: 0.7410 - lr: 8.7415e-06\n",
      "Epoch 154/250\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.7515 - val_loss: 0.7368 - lr: 8.7415e-06\n",
      "Epoch 155/250\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.7265 - val_loss: 0.7089 - lr: 4.3707e-06\n",
      "Epoch 156/250\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.7212 - val_loss: 0.7087 - lr: 4.3707e-06\n",
      "Epoch 157/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.7218 - val_loss: 0.7095 - lr: 4.3707e-06\n",
      "Epoch 158/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7220 - val_loss: 0.7091 - lr: 4.3707e-06\n",
      "Epoch 159/250\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.7215 - val_loss: 0.7083 - lr: 4.3707e-06\n",
      "Epoch 160/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.7220 - val_loss: 0.7080 - lr: 4.3707e-06\n",
      "Epoch 161/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7204 - val_loss: 0.7082 - lr: 4.3707e-06\n",
      "Epoch 162/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7209 - val_loss: 0.7100 - lr: 4.3707e-06\n",
      "Epoch 163/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7214 - val_loss: 0.7078 - lr: 4.3707e-06\n",
      "Epoch 164/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.7216 - val_loss: 0.7081 - lr: 4.3707e-06\n",
      "Epoch 165/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7217 - val_loss: 0.7090 - lr: 4.3707e-06\n",
      "Epoch 166/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.7213 - val_loss: 0.7084 - lr: 4.3707e-06\n",
      "Epoch 167/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.7213 - val_loss: 0.7102 - lr: 4.3707e-06\n",
      "Epoch 168/250\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.7219 - val_loss: 0.7078 - lr: 4.3707e-06\n",
      "Epoch 169/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.7087 - val_loss: 0.6939 - lr: 2.1854e-06\n",
      "Epoch 170/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.7062 - val_loss: 0.6939 - lr: 2.1854e-06\n",
      "Epoch 171/250\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.7068 - val_loss: 0.6944 - lr: 2.1854e-06\n",
      "Epoch 172/250\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.7057 - val_loss: 0.6941 - lr: 2.1854e-06\n",
      "Epoch 173/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.7061 - val_loss: 0.6935 - lr: 2.1854e-06\n",
      "Epoch 174/250\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.7065 - val_loss: 0.6936 - lr: 2.1854e-06\n",
      "Epoch 175/250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7061 - val_loss: 0.6936 - lr: 2.1854e-06\n",
      "Epoch 176/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7073 - val_loss: 0.6946 - lr: 2.1854e-06\n",
      "Epoch 177/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.7074 - val_loss: 0.6936 - lr: 2.1854e-06\n",
      "Epoch 178/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.7066 - val_loss: 0.6936 - lr: 2.1854e-06\n",
      "Epoch 179/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7000 - val_loss: 0.6865 - lr: 1.0927e-06\n",
      "Epoch 180/250\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6982 - val_loss: 0.6866 - lr: 1.0927e-06\n",
      "Epoch 181/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6993 - val_loss: 0.6867 - lr: 1.0927e-06\n",
      "Epoch 182/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6993 - val_loss: 0.6866 - lr: 1.0927e-06\n",
      "Epoch 183/250\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6986 - val_loss: 0.6865 - lr: 1.0927e-06\n",
      "Epoch 184/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6998 - val_loss: 0.6864 - lr: 1.0927e-06\n",
      "Epoch 185/250\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6995 - val_loss: 0.6866 - lr: 1.0927e-06\n",
      "Epoch 186/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6989 - val_loss: 0.6868 - lr: 1.0927e-06\n",
      "Epoch 187/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6982 - val_loss: 0.6864 - lr: 1.0927e-06\n",
      "Epoch 188/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6977 - val_loss: 0.6864 - lr: 1.0927e-06\n",
      "Epoch 189/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6986 - val_loss: 0.6864 - lr: 1.0927e-06\n",
      "Epoch 190/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6964 - val_loss: 0.6828 - lr: 5.4634e-07\n",
      "Epoch 191/250\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6937 - val_loss: 0.6829 - lr: 5.4634e-07\n",
      "Epoch 192/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6946 - val_loss: 0.6829 - lr: 5.4634e-07\n",
      "Epoch 193/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6954 - val_loss: 0.6828 - lr: 5.4634e-07\n",
      "Epoch 194/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6963 - val_loss: 0.6828 - lr: 5.4634e-07\n",
      "Epoch 195/250\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6953 - val_loss: 0.6827 - lr: 5.4634e-07\n",
      "Epoch 196/250\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6939 - val_loss: 0.6810 - lr: 2.7317e-07\n",
      "Epoch 197/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6931 - val_loss: 0.6810 - lr: 2.7317e-07\n",
      "Epoch 198/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6930 - val_loss: 0.6810 - lr: 2.7317e-07\n",
      "Epoch 199/250\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6942 - val_loss: 0.6810 - lr: 2.7317e-07\n",
      "Epoch 200/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6940 - val_loss: 0.6810 - lr: 2.7317e-07\n",
      "Epoch 201/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6939 - val_loss: 0.6809 - lr: 2.7317e-07\n",
      "Epoch 202/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6923 - val_loss: 0.6801 - lr: 1.3659e-07\n",
      "Epoch 203/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6931 - val_loss: 0.6801 - lr: 1.3659e-07\n",
      "Epoch 204/250\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6924 - val_loss: 0.6801 - lr: 1.3659e-07\n",
      "Epoch 205/250\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6926 - val_loss: 0.6801 - lr: 1.3659e-07\n",
      "Epoch 206/250\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6925 - val_loss: 0.6800 - lr: 1.3659e-07\n",
      "Epoch 207/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6917 - val_loss: 0.6800 - lr: 1.3659e-07\n",
      "Epoch 208/250\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6929 - val_loss: 0.6796 - lr: 6.8293e-08\n",
      "Epoch 209/250\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6909 - val_loss: 0.6796 - lr: 6.8293e-08\n",
      "Epoch 210/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6934 - val_loss: 0.6796 - lr: 6.8293e-08\n",
      "Epoch 211/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6925 - val_loss: 0.6796 - lr: 6.8293e-08\n",
      "Epoch 212/250\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6923 - val_loss: 0.6796 - lr: 6.8293e-08\n",
      "Epoch 213/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6912 - val_loss: 0.6796 - lr: 6.8293e-08\n",
      "Epoch 214/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6913 - val_loss: 0.6794 - lr: 3.4146e-08\n",
      "Epoch 215/250\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6911 - val_loss: 0.6794 - lr: 3.4146e-08\n",
      "Epoch 216/250\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6926 - val_loss: 0.6794 - lr: 3.4146e-08\n",
      "Epoch 217/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6913 - val_loss: 0.6794 - lr: 3.4146e-08\n",
      "Epoch 218/250\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6913 - val_loss: 0.6794 - lr: 3.4146e-08\n",
      "Epoch 219/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6912 - val_loss: 0.6794 - lr: 3.4146e-08\n",
      "Epoch 220/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6910 - val_loss: 0.6792 - lr: 1.7073e-08\n",
      "Epoch 221/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6918 - val_loss: 0.6792 - lr: 1.7073e-08\n",
      "Epoch 222/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6915 - val_loss: 0.6792 - lr: 1.7073e-08\n",
      "Epoch 223/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6911 - val_loss: 0.6792 - lr: 1.7073e-08\n",
      "Epoch 224/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6922 - val_loss: 0.6792 - lr: 1.7073e-08\n",
      "Epoch 225/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6924 - val_loss: 0.6792 - lr: 1.7073e-08\n",
      "Epoch 226/250\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.6930 - val_loss: 0.6792 - lr: 8.5366e-09\n",
      "Epoch 227/250\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6920 - val_loss: 0.6792 - lr: 8.5366e-09\n",
      "Epoch 228/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6913 - val_loss: 0.6792 - lr: 8.5366e-09\n",
      "Epoch 229/250\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6930 - val_loss: 0.6792 - lr: 8.5366e-09\n",
      "Epoch 230/250\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6923 - val_loss: 0.6792 - lr: 8.5366e-09\n",
      "Epoch 231/250\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6920 - val_loss: 0.6791 - lr: 4.2683e-09\n",
      "Epoch 232/250\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6915 - val_loss: 0.6791 - lr: 4.2683e-09\n",
      "Epoch 233/250\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6908 - val_loss: 0.6791 - lr: 4.2683e-09\n",
      "Epoch 234/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6914 - val_loss: 0.6791 - lr: 4.2683e-09\n",
      "Epoch 235/250\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6918 - val_loss: 0.6791 - lr: 4.2683e-09\n",
      "Epoch 236/250\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6929 - val_loss: 0.6791 - lr: 2.1342e-09\n",
      "Epoch 237/250\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6923 - val_loss: 0.6791 - lr: 2.1342e-09\n",
      "Epoch 238/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6920 - val_loss: 0.6791 - lr: 2.1342e-09\n",
      "Epoch 239/250\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6917 - val_loss: 0.6791 - lr: 2.1342e-09\n",
      "Epoch 240/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6921 - val_loss: 0.6791 - lr: 2.1342e-09\n",
      "Epoch 241/250\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6915 - val_loss: 0.6791 - lr: 2.1342e-09\n",
      "Epoch 242/250\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6912 - val_loss: 0.6791 - lr: 1.0671e-09\n",
      "Epoch 243/250\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.6921 - val_loss: 0.6791 - lr: 1.0671e-09\n",
      "Epoch 244/250\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6909 - val_loss: 0.6791 - lr: 1.0671e-09\n",
      "Epoch 245/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6926 - val_loss: 0.6791 - lr: 1.0671e-09\n",
      "Epoch 246/250\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.6918 - val_loss: 0.6791 - lr: 1.0671e-09\n",
      "Epoch 247/250\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6920 - val_loss: 0.6791 - lr: 5.3354e-10\n",
      "Epoch 248/250\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6923 - val_loss: 0.6791 - lr: 5.3354e-10\n",
      "Epoch 249/250\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6908 - val_loss: 0.6791 - lr: 5.3354e-10\n",
      "Epoch 250/250\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6917 - val_loss: 0.6791 - lr: 5.3354e-10\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "#from keras import backend\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import Dropout\n",
    "#from keras.regularizers import l1\n",
    "#from keras.regularizers import l2\n",
    "#from keras.regularizers import l1_l2\n",
    "\n",
    "\n",
    "x_train_vector = ut.df_a_vector(x_train)\n",
    "y_train_vector = ut.df_a_vector(y_train)\n",
    "x_test_vector = ut.df_a_vector(x_test)\n",
    "y_test_vector = ut.df_a_vector(y_test)\n",
    "\n",
    "#input_dim = x_train.shape[1]\n",
    "#\n",
    "#alfa = 1e-3\n",
    "#\n",
    "#model = Sequential()\n",
    "#model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(Dense(128, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "##model.add(Dropout(0.25))\n",
    "##model.add(Dense(256, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='tanh'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(128, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(64, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='tanh'))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(Dense(16, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "#model.add(Dense(8, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "#model.add(Dense(1, activation='tanh'))\n",
    "#\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#\n",
    "#backend.set_value(model.optimizer.learning_rate, 5e-4)\n",
    "\n",
    "best_hparams = ut.hyperparams_from_json('../Archivos/Neuronales')\n",
    "model = md.get_neural_network_model(best_hparams, x_train.shape[1])\n",
    "\n",
    "epochs = 250\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"Neuronales_Mejor_Modelo.hdf5\", \n",
    "                                       monitor='val_loss', \n",
    "                                       verbose=0,\n",
    "                                       save_best_only=True, \n",
    "                                       mode='min'),\n",
    "    \n",
    "    #tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "    #                                 min_delta=0.01,\n",
    "    #                                 mode='min',\n",
    "    #                                 patience=10),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                      mode='min',\n",
    "                                      factor=0.5,\n",
    "                                      patience=5,\n",
    "                                      cooldown=0, \n",
    "                                      min_lr=1e-24)\n",
    "]\n",
    "\n",
    "fit_dict = {\n",
    "    #'x' : x_train_vector,\n",
    "    #'y' : y_train_vector,\n",
    "    #'validation_data' : (x_test_vector, y_test_vector),\n",
    "    'epochs' : epochs,\n",
    "    'batch_size' : batch_size,\n",
    "    'verbose' : 1,\n",
    "    'callbacks' : my_callbacks\n",
    "}\n",
    "\n",
    "\n",
    "history = model.fit(x_train_vector,\n",
    "                    y_train_vector,\n",
    "                    validation_data=(x_test_vector, y_test_vector),\n",
    "                    **fit_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('Neuronales_7740.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "BoO8syeWoQhd",
    "outputId": "e4737981-19af-4c5f-c69e-f2208387dcff"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Accuracy vs Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model = keras.models.load_model('Neuronales_Mejor_Modelo.hdf5')\n",
    "#new_predictions = new_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6053156146179401"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = [a > 0.5 for a in model.predict(x_test_vector)]\n",
    "\n",
    "\n",
    "score = accuracy_score(y_pred, y_test_vector)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 0s 3ms/step - loss: 0.6791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6791225075721741"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_vector, y_test_vector)#, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Pruebas TP2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
