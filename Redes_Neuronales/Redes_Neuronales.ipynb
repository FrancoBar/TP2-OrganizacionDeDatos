{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQfQ2ppfQasK"
   },
   "source": [
    "# Redes Neuronales\n",
    "## Preparacion de datos para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "8OQLMHvqkvdV",
    "outputId": "4d6f4c0b-232b-468e-8734-083a58109f70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12140, 5)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import Utilidades as ut\n",
    "import Modelos as md\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_train = pd.read_pickle(\"../Archivos/Neuronales_entrenamiento.pkl\")\n",
    "df_test = pd.read_pickle(\"../Archivos/Neuronales_validacion.pkl\")\n",
    "\n",
    "if ('Opportunity_ID' in df_test):\n",
    "    df_test = df_test.drop(columns=['Opportunity_ID'])\n",
    "\n",
    "####MOMENTANEO\n",
    "\n",
    "df_train = df_train.drop(columns=['Opportunity_Created_Date'])\n",
    "df_test = df_test.drop(columns=['Opportunity_Created_Date'])\n",
    "\n",
    "##############3\n",
    "    \n",
    "x_train, y_train = ut.split_labels(df_train)\n",
    "x_test, y_test = ut.split_labels(df_test)\n",
    "\n",
    "#Convertimos las fechas a numeros (cantidad de dias transcurridos) y luego las normalizamos\n",
    "x_train, x_test = ut.conversion_fechas(x_train, x_test)\n",
    "x_train, x_test = ut.codificar_categoricas(x_train, y_train, x_test, modo='catboost')\n",
    "x_train, x_test = ut.normalizacion_numericas(x_train, x_test, modo='normalizacion')\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oSHQttP_cuK"
   },
   "source": [
    "## Creacion del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rN3ODhM_Zxm",
    "outputId": "81bc755e-a46a-4f34-eced-99d6297b67e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando hiperparametros desde el archivo: '../Archivos/Neuronales_best_hyperparam.json'\n",
      "Iteracion numero 1 de 25\n",
      "Iteracion numero 2 de 25\n",
      "Iteracion numero 3 de 25\n",
      "Iteracion numero 4 de 25\n",
      "Iteracion numero 5 de 25\n",
      "Iteracion numero 6 de 25\n",
      "Iteracion numero 7 de 25\n",
      "Iteracion numero 8 de 25\n",
      "Iteracion numero 9 de 25\n",
      "Iteracion numero 10 de 25\n",
      "Iteracion numero 11 de 25\n",
      "Iteracion numero 12 de 25\n",
      "Iteracion numero 13 de 25\n",
      "Iteracion numero 14 de 25\n",
      "Iteracion numero 15 de 25\n",
      "Iteracion numero 16 de 25\n",
      "Iteracion numero 17 de 25\n",
      "Iteracion numero 18 de 25\n",
      "Iteracion numero 19 de 25\n",
      "Iteracion numero 20 de 25\n",
      "Iteracion numero 21 de 25\n",
      "Iteracion numero 22 de 25\n",
      "Iteracion numero 23 de 25\n",
      "Iteracion numero 24 de 25\n",
      "Iteracion numero 25 de 25\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "from keras.regularizers import l1_l2\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "\n",
    "x_train_vector = ut.df_a_vector(x_train)\n",
    "y_train_vector = ut.df_a_vector(y_train)\n",
    "x_test_vector = ut.df_a_vector(x_test)\n",
    "y_test_vector = ut.df_a_vector(y_test)\n",
    "\n",
    "#input_dim = x_train.shape[1]\n",
    "#\n",
    "#alfa = 0.01\n",
    "#\n",
    "#model = Sequential()\n",
    "#model.add(Dense(96, input_dim=input_dim, kernel_regularizer=l1(alfa), activation='tanh'))\n",
    "#model.add(Dropout(0.39))\n",
    "#model.add(Dense(54, kernel_regularizer=l1(alfa), activation='tanh'))\n",
    "#model.add(Dropout(0.28))\n",
    "##model.add(Dense(256, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='tanh'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(128, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(64, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='tanh'))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(Dense(16, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "#model.add(Dense(8, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "#backend.set_value(model.optimizer.learning_rate, 0.0016)\n",
    "\n",
    "best_hparams = ut.hyperparams_from_json('../Archivos/Neuronales')\n",
    "model = md.get_neural_network_model(best_hparams, x_train.shape[1])\n",
    "\n",
    "epochs = 250\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"Neuronales_Mejor_Modelo.hdf5\", \n",
    "                                       monitor='val_loss', \n",
    "                                       verbose=0,\n",
    "                                       save_best_only=True, \n",
    "                                       mode='min'),\n",
    "    \n",
    "    tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                     min_delta=0.0001,\n",
    "                                     mode='min',\n",
    "                                     patience=10),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                      mode='min',\n",
    "                                      factor=0.5,\n",
    "                                      min_delta=0.0001,\n",
    "                                      patience=2,\n",
    "                                      cooldown=0, \n",
    "                                      min_lr=1e-24)\n",
    "]\n",
    "\n",
    "#def train(last_best):\n",
    "#    \n",
    "#    for a in range(0, 50):\n",
    "#        model = md.get_neural_network_model(best_hparams, x_train.shape[1])\n",
    "#        history = model.fit(x_train_vector,\n",
    "#                            y_train_vector,\n",
    "#                            validation_data=(x_test_vector, y_test_vector),\n",
    "#                            **fit_dict)\n",
    "#        if history.history['val_loss'][-1] < last_best:\n",
    "#            return\n",
    "#        \n",
    "#    \n",
    "#\n",
    "#train(0.281)\n",
    "#Mejor modelo hasta el momento val_loss = 0.311\n",
    "#Ultimo mejor 0.2809\n",
    "last_best = 0.200\n",
    "\n",
    "tot = 25\n",
    "\n",
    "for a in range(1, tot+1):\n",
    "    print(f\"Iteracion numero {a} de {tot}\")\n",
    "    model = md.get_neural_network_model(best_hparams, x_train.shape[1])\n",
    "    #backend.set_value(model.optimizer.learning_rate, 1e-4)\n",
    "    history = model.fit(x_train_vector,\n",
    "                        y_train_vector,\n",
    "                        validation_data=(x_test_vector, y_test_vector),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0,\n",
    "                        callbacks=my_callbacks)\n",
    "    new_model = keras.models.load_model('Neuronales_Mejor_Modelo.hdf5')\n",
    "    y_pred_proba = new_model.predict(x_test_vector).flatten()\n",
    "    score = log_loss(y_test_vector.flatten(), y_pred_proba, eps=1e-7)\n",
    "    if score < last_best:\n",
    "        print(f\"Se ha conseguido un score mejor al anterior. log_loss = {score}\")\n",
    "        score_redondeado = round(score, 4)\n",
    "        print(\"Guardando modelo en: 'Mejores_Modelos/Neuronales_logloss_\" + str(score_redondeado) + \"_10_features.hdf5\")\n",
    "        new_model.save('Mejores_Modelos/Neuronales_logloss_' + str(score_redondeado) + '_10_features.hdf5')\n",
    "        print(f\"Best log_loss = {score}\")\n",
    "        last_best = score\n",
    "\n",
    "#backend.set_value(model.optimizer.learning_rate, 1e-4)\n",
    "\n",
    "# history = model.fit(x_train_vector,\n",
    "#                        y_train_vector,\n",
    "#                        validation_data=(x_test_vector, y_test_vector),\n",
    "#                        epochs=epochs,\n",
    "#                        batch_size=batch_size,\n",
    "#                        verbose=1,\n",
    "#                        callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model('Neuronales_Mejor_Modelo.hdf5')\n",
    "#new_model = keras.models.load_model('Neuronales_logloss_1937_5_features.hdf5')\n",
    "#new_model.save('Neuronales_logloss_1937_5_features.hdf5')\n",
    "#new_model = keras.models.load_model('Neuronales_Mejor_Modelo.hdf5')\n",
    "#new_predictions = new_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9483942414174973\n",
      "log_loss: 0.20239236776614902\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "y_pred_proba = new_model.predict(x_test_vector).flatten()\n",
    "y_pred = [a > 0.5 for a in y_pred_proba]\n",
    "\n",
    "\n",
    "#Mejor acc obtenido 0.94839\n",
    "#Mejor log_loss obtenido 0.18612\n",
    "#Mejor log_loss obtenido que resulto en un buen score en Kaggle -> 0.19372\n",
    "\n",
    "acc = accuracy_score(y_pred, y_test_vector)\n",
    "loss = log_loss(y_test_vector.flatten(), y_pred_proba, eps=1e-7)\n",
    "print(f\"acc: {acc}\")\n",
    "print(f\"log_loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probamos normalizar al intervalo (0, 1)\n",
    "vector = y_pred_proba.flatten()\n",
    "\n",
    "min_ = vector.min()\n",
    "max_ = vector.max()\n",
    "div = max_ - min_\n",
    "\n",
    "for i, a in enumerate(vector):\n",
    "    vector[i] = (a - min_) / div\n",
    "\n",
    "    \n",
    "score = log_loss(y_test_vector.flatten(), vector, eps=1e-7)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.evaluate(x_test_vector, y_test_vector)#, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Podemos analizar las filas que originaron los errores de prediccion\n",
    "\n",
    "analysis_df = y_test.copy()\n",
    "analysis_df['Predicted'] = pd.Series(y_pred).astype(int)\n",
    "differences = (analysis_df['Predicted'] != analysis_df['Stage'])\n",
    "df_test[differences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "BoO8syeWoQhd",
    "outputId": "e4737981-19af-4c5f-c69e-f2208387dcff"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Accuracy vs Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Pruebas TP2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
