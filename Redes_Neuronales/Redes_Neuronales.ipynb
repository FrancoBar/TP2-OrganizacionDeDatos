{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQfQ2ppfQasK"
   },
   "source": [
    "# Redes Neuronales\n",
    "## Preparacion de datos para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "8OQLMHvqkvdV",
    "outputId": "4d6f4c0b-232b-468e-8734-083a58109f70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Territory</th>\n",
       "      <th>Pricing, Delivery_Terms_Quote_Appr</th>\n",
       "      <th>Pricing, Delivery_Terms_Approved</th>\n",
       "      <th>Bureaucratic_Code_0_Approval</th>\n",
       "      <th>Bureaucratic_Code_0_Approved</th>\n",
       "      <th>Submitted_for_Approval</th>\n",
       "      <th>Bureaucratic_Code</th>\n",
       "      <th>Account_Created_Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>...</th>\n",
       "      <th>Buro_Approved_by_Billing_Country_mean</th>\n",
       "      <th>Buro_Approved_by_Billing_Country_std</th>\n",
       "      <th>Opportunity_Duration_by_Product_Family_mean</th>\n",
       "      <th>Opportunity_Duration_by_Product_Family_std</th>\n",
       "      <th>Total_Amount_by_Product_Family_mean</th>\n",
       "      <th>Total_Amount_by_Product_Family_std</th>\n",
       "      <th>Buro_Approved_by_Product_Family</th>\n",
       "      <th>Family_Duration</th>\n",
       "      <th>Region_Duration</th>\n",
       "      <th>Territory_Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-2.05</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>0.69</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>-1.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Region  Territory  Pricing, Delivery_Terms_Quote_Appr  \\\n",
       "0    0.86       0.36                               -2.05   \n",
       "1    0.86       0.36                                0.49   \n",
       "2    0.86       0.36                                0.49   \n",
       "\n",
       "   Pricing, Delivery_Terms_Approved  Bureaucratic_Code_0_Approval  \\\n",
       "0                             -1.13                         -0.90   \n",
       "1                              0.88                          1.11   \n",
       "2                             -1.13                          1.11   \n",
       "\n",
       "   Bureaucratic_Code_0_Approved  Submitted_for_Approval  Bureaucratic_Code  \\\n",
       "0                         -0.64                     0.0               0.14   \n",
       "1                          1.56                     0.0               0.14   \n",
       "2                         -0.64                     0.0               0.14   \n",
       "\n",
       "   Account_Created_Date  Source  ...  Buro_Approved_by_Billing_Country_mean  \\\n",
       "0                 -1.35    0.69  ...                                  -0.31   \n",
       "1                 -0.96    0.69  ...                                   0.11   \n",
       "2                 -1.29   -1.34  ...                                   0.10   \n",
       "\n",
       "   Buro_Approved_by_Billing_Country_std  \\\n",
       "0                                  0.08   \n",
       "1                                  0.54   \n",
       "2                                  0.53   \n",
       "\n",
       "   Opportunity_Duration_by_Product_Family_mean  \\\n",
       "0                                        -0.01   \n",
       "1                                        -0.12   \n",
       "2                                        -0.28   \n",
       "\n",
       "   Opportunity_Duration_by_Product_Family_std  \\\n",
       "0                                       -0.43   \n",
       "1                                        0.46   \n",
       "2                                        1.48   \n",
       "\n",
       "   Total_Amount_by_Product_Family_mean  Total_Amount_by_Product_Family_std  \\\n",
       "0                                -0.75                               -0.64   \n",
       "1                                 0.40                                0.31   \n",
       "2                                 1.99                                1.37   \n",
       "\n",
       "   Buro_Approved_by_Product_Family  Family_Duration  Region_Duration  \\\n",
       "0                             0.26             0.61             1.97   \n",
       "1                             0.22            -0.82            -1.65   \n",
       "2                             0.69            -0.25            -0.61   \n",
       "\n",
       "   Territory_Duration  \n",
       "0                0.84  \n",
       "1                0.41  \n",
       "2               -0.90  \n",
       "\n",
       "[3 rows x 84 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import Utilidades as ut\n",
    "import Modelos as md\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_train = pd.read_pickle(\"../Archivos/Neuronales_entrenamiento.pkl\")\n",
    "df_test = pd.read_pickle(\"../Archivos/Neuronales_validacion.pkl\")\n",
    "\n",
    "x_train, y_train = ut.split_labels(df_train)\n",
    "x_test, y_test = ut.split_labels(df_test)\n",
    "\n",
    "#Convertimos las fechas a numeros (cantidad de dias transcurridos) y luego las normalizamos\n",
    "x_train, x_test = ut.conversion_fechas(x_train, x_test)\n",
    "x_train, x_test = ut.codificar_categoricas(x_train, y_train, x_test, modo='catboost')\n",
    "x_train, x_test = ut.normalizacion_numericas(x_train, x_test, modo='normalizacion')\n",
    "x_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oSHQttP_cuK"
   },
   "source": [
    "## Creacion del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rN3ODhM_Zxm",
    "outputId": "81bc755e-a46a-4f34-eced-99d6297b67e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando hiperparametros desde el archivo: '../Archivos/Neuronales_best_hyperparam.json'\n",
      "Epoch 1/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 2625.2500 - val_loss: 771.8998 - lr: 0.0022\n",
      "Epoch 2/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 227.7168 - val_loss: 33.1897 - lr: 0.0022\n",
      "Epoch 3/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 21.3687 - val_loss: 17.3249 - lr: 0.0022\n",
      "Epoch 4/300\n",
      "48/48 [==============================] - 1s 10ms/step - loss: 16.9898 - val_loss: 17.0409 - lr: 0.0022\n",
      "Epoch 5/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 16.9402 - val_loss: 16.9532 - lr: 0.0022\n",
      "Epoch 6/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 16.9400 - val_loss: 16.9363 - lr: 0.0022\n",
      "Epoch 7/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 16.9217 - val_loss: 17.0230 - lr: 0.0022\n",
      "Epoch 8/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 16.9231 - val_loss: 16.9681 - lr: 0.0022\n",
      "Epoch 9/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 16.9113 - val_loss: 16.9294 - lr: 0.0022\n",
      "Epoch 10/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 16.9068 - val_loss: 17.0147 - lr: 0.0022\n",
      "Epoch 11/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 16.8916 - val_loss: 16.9927 - lr: 0.0022\n",
      "Epoch 12/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 16.8898 - val_loss: 16.8430 - lr: 0.0022\n",
      "Epoch 13/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 16.9076 - val_loss: 16.9336 - lr: 0.0022\n",
      "Epoch 14/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 16.8712 - val_loss: 17.0496 - lr: 0.0022\n",
      "Epoch 15/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 16.8962 - val_loss: 16.8438 - lr: 0.0022\n",
      "Epoch 16/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 16.8803 - val_loss: 16.8791 - lr: 0.0022\n",
      "Epoch 17/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 16.8690 - val_loss: 16.8545 - lr: 0.0022\n",
      "Epoch 18/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 9.9782 - val_loss: 8.3577 - lr: 0.0011\n",
      "Epoch 19/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 8.4877 - val_loss: 8.5848 - lr: 0.0011\n",
      "Epoch 20/300\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 8.4397 - val_loss: 8.5420 - lr: 0.0011\n",
      "Epoch 21/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 8.4651 - val_loss: 8.4690 - lr: 0.0011\n",
      "Epoch 22/300\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 8.4502 - val_loss: 8.4004 - lr: 0.0011\n",
      "Epoch 23/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 8.4472 - val_loss: 8.2894 - lr: 0.0011\n",
      "Epoch 24/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 8.4551 - val_loss: 8.4061 - lr: 0.0011\n",
      "Epoch 25/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 8.4322 - val_loss: 8.5779 - lr: 0.0011\n",
      "Epoch 26/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 8.4598 - val_loss: 8.3040 - lr: 0.0011\n",
      "Epoch 27/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 8.4376 - val_loss: 8.3363 - lr: 0.0011\n",
      "Epoch 28/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 8.4424 - val_loss: 8.2949 - lr: 0.0011\n",
      "Epoch 29/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 5.2330 - val_loss: 4.5034 - lr: 5.5946e-04\n",
      "Epoch 30/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 4.5664 - val_loss: 4.6031 - lr: 5.5946e-04\n",
      "Epoch 31/300\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 4.5556 - val_loss: 4.5787 - lr: 5.5946e-04\n",
      "Epoch 32/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 4.5596 - val_loss: 4.4957 - lr: 5.5946e-04\n",
      "Epoch 33/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 4.5526 - val_loss: 4.5485 - lr: 5.5946e-04\n",
      "Epoch 34/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 4.5573 - val_loss: 4.4452 - lr: 5.5946e-04\n",
      "Epoch 35/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 4.5586 - val_loss: 4.4550 - lr: 5.5946e-04\n",
      "Epoch 36/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 4.5448 - val_loss: 4.6925 - lr: 5.5946e-04\n",
      "Epoch 37/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 4.5568 - val_loss: 4.4439 - lr: 5.5946e-04\n",
      "Epoch 38/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 4.5536 - val_loss: 4.4616 - lr: 5.5946e-04\n",
      "Epoch 39/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 4.5512 - val_loss: 4.5109 - lr: 5.5946e-04\n",
      "Epoch 40/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 4.5497 - val_loss: 4.5220 - lr: 5.5946e-04\n",
      "Epoch 41/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 4.5496 - val_loss: 4.6331 - lr: 5.5946e-04\n",
      "Epoch 42/300\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 4.5526 - val_loss: 4.3943 - lr: 5.5946e-04\n",
      "Epoch 43/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 4.5495 - val_loss: 4.4838 - lr: 5.5946e-04\n",
      "Epoch 44/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 4.5541 - val_loss: 4.4610 - lr: 5.5946e-04\n",
      "Epoch 45/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 4.5363 - val_loss: 4.5384 - lr: 5.5946e-04\n",
      "Epoch 46/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 4.5445 - val_loss: 4.6410 - lr: 5.5946e-04\n",
      "Epoch 47/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 4.5486 - val_loss: 4.4638 - lr: 5.5946e-04\n",
      "Epoch 48/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 2.9445 - val_loss: 2.6134 - lr: 2.7973e-04\n",
      "Epoch 49/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 2.6225 - val_loss: 2.6045 - lr: 2.7973e-04\n",
      "Epoch 50/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 2.6126 - val_loss: 2.6430 - lr: 2.7973e-04\n",
      "Epoch 51/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 2.6165 - val_loss: 2.6263 - lr: 2.7973e-04\n",
      "Epoch 52/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 2.6126 - val_loss: 2.5803 - lr: 2.7973e-04\n",
      "Epoch 53/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 2.6114 - val_loss: 2.5801 - lr: 2.7973e-04\n",
      "Epoch 54/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 2.6157 - val_loss: 2.5778 - lr: 2.7973e-04\n",
      "Epoch 55/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 2.6084 - val_loss: 2.6510 - lr: 2.7973e-04\n",
      "Epoch 56/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 2.6168 - val_loss: 2.5608 - lr: 2.7973e-04\n",
      "Epoch 57/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 2.6088 - val_loss: 2.5653 - lr: 2.7973e-04\n",
      "Epoch 58/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 2.6106 - val_loss: 2.5785 - lr: 2.7973e-04\n",
      "Epoch 59/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 2.6127 - val_loss: 2.5844 - lr: 2.7973e-04\n",
      "Epoch 60/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 2.6063 - val_loss: 2.6819 - lr: 2.7973e-04\n",
      "Epoch 61/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 2.6125 - val_loss: 2.5512 - lr: 2.7973e-04\n",
      "Epoch 62/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 2.6129 - val_loss: 2.5562 - lr: 2.7973e-04\n",
      "Epoch 63/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 2.6100 - val_loss: 2.5847 - lr: 2.7973e-04\n",
      "Epoch 64/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 2.6073 - val_loss: 2.5832 - lr: 2.7973e-04\n",
      "Epoch 65/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 2.6093 - val_loss: 2.6507 - lr: 2.7973e-04\n",
      "Epoch 66/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 2.6114 - val_loss: 2.5929 - lr: 2.7973e-04\n",
      "Epoch 67/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 1.8154 - val_loss: 1.6479 - lr: 1.3986e-04\n",
      "Epoch 68/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 1.6470 - val_loss: 1.6369 - lr: 1.3986e-04\n",
      "Epoch 69/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 1.6430 - val_loss: 1.6569 - lr: 1.3986e-04\n",
      "Epoch 70/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 1.6452 - val_loss: 1.6461 - lr: 1.3986e-04\n",
      "Epoch 71/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 1.6423 - val_loss: 1.6332 - lr: 1.3986e-04\n",
      "Epoch 72/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 1.6425 - val_loss: 1.6168 - lr: 1.3986e-04\n",
      "Epoch 73/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 1.6438 - val_loss: 1.6227 - lr: 1.3986e-04\n",
      "Epoch 74/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.6391 - val_loss: 1.6749 - lr: 1.3986e-04\n",
      "Epoch 75/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 1.6443 - val_loss: 1.6099 - lr: 1.3986e-04\n",
      "Epoch 76/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 1.6414 - val_loss: 1.6275 - lr: 1.3986e-04\n",
      "Epoch 77/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 1.6398 - val_loss: 1.6271 - lr: 1.3986e-04\n",
      "Epoch 78/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 1.6425 - val_loss: 1.6245 - lr: 1.3986e-04\n",
      "Epoch 79/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 1.6395 - val_loss: 1.6773 - lr: 1.3986e-04\n",
      "Epoch 80/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 1.6411 - val_loss: 1.6042 - lr: 1.3986e-04\n",
      "Epoch 81/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 1.6418 - val_loss: 1.6178 - lr: 1.3986e-04\n",
      "Epoch 82/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 1.6405 - val_loss: 1.6232 - lr: 1.3986e-04\n",
      "Epoch 83/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 1.6388 - val_loss: 1.6276 - lr: 1.3986e-04\n",
      "Epoch 84/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 1.6405 - val_loss: 1.6608 - lr: 1.3986e-04\n",
      "Epoch 85/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 1.6398 - val_loss: 1.6347 - lr: 1.3986e-04\n",
      "Epoch 86/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.2532 - val_loss: 1.1607 - lr: 6.9932e-05\n",
      "Epoch 87/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 1.1708 - val_loss: 1.1623 - lr: 6.9932e-05\n",
      "Epoch 88/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 1.1689 - val_loss: 1.1679 - lr: 6.9932e-05\n",
      "Epoch 89/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 1.1702 - val_loss: 1.1610 - lr: 6.9932e-05\n",
      "Epoch 90/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 1.1690 - val_loss: 1.1617 - lr: 6.9932e-05\n",
      "Epoch 91/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.1693 - val_loss: 1.1529 - lr: 6.9932e-05\n",
      "Epoch 92/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 1.1702 - val_loss: 1.1522 - lr: 6.9932e-05\n",
      "Epoch 93/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 1.1672 - val_loss: 1.1785 - lr: 6.9932e-05\n",
      "Epoch 94/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 1.1707 - val_loss: 1.1430 - lr: 6.9932e-05\n",
      "Epoch 95/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 1.1674 - val_loss: 1.1544 - lr: 6.9932e-05\n",
      "Epoch 96/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 1.1678 - val_loss: 1.1574 - lr: 6.9932e-05\n",
      "Epoch 97/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 1.1705 - val_loss: 1.1584 - lr: 6.9932e-05\n",
      "Epoch 98/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 1.1680 - val_loss: 1.1818 - lr: 6.9932e-05\n",
      "Epoch 99/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 1.1694 - val_loss: 1.1418 - lr: 6.9932e-05\n",
      "Epoch 100/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 1.1694 - val_loss: 1.1526 - lr: 6.9932e-05\n",
      "Epoch 101/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 1.1703 - val_loss: 1.1530 - lr: 6.9932e-05\n",
      "Epoch 102/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 1.1696 - val_loss: 1.1564 - lr: 6.9932e-05\n",
      "Epoch 103/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 1.1686 - val_loss: 1.1743 - lr: 6.9932e-05\n",
      "Epoch 104/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 1.1695 - val_loss: 1.1527 - lr: 6.9932e-05\n",
      "Epoch 105/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.9710 - val_loss: 0.9203 - lr: 3.4966e-05\n",
      "Epoch 106/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.9303 - val_loss: 0.9186 - lr: 3.4966e-05\n",
      "Epoch 107/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.9298 - val_loss: 0.9247 - lr: 3.4966e-05\n",
      "Epoch 108/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.9303 - val_loss: 0.9195 - lr: 3.4966e-05\n",
      "Epoch 109/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.9286 - val_loss: 0.9193 - lr: 3.4966e-05\n",
      "Epoch 110/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.9292 - val_loss: 0.9127 - lr: 3.4966e-05\n",
      "Epoch 111/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.9296 - val_loss: 0.9153 - lr: 3.4966e-05\n",
      "Epoch 112/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.9285 - val_loss: 0.9274 - lr: 3.4966e-05\n",
      "Epoch 113/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.9299 - val_loss: 0.9108 - lr: 3.4966e-05\n",
      "Epoch 114/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.9287 - val_loss: 0.9153 - lr: 3.4966e-05\n",
      "Epoch 115/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.9277 - val_loss: 0.9166 - lr: 3.4966e-05\n",
      "Epoch 116/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.9295 - val_loss: 0.9173 - lr: 3.4966e-05\n",
      "Epoch 117/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.9286 - val_loss: 0.9294 - lr: 3.4966e-05\n",
      "Epoch 118/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.9294 - val_loss: 0.9096 - lr: 3.4966e-05\n",
      "Epoch 119/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.9289 - val_loss: 0.9142 - lr: 3.4966e-05\n",
      "Epoch 120/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.9294 - val_loss: 0.9138 - lr: 3.4966e-05\n",
      "Epoch 121/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.9289 - val_loss: 0.9158 - lr: 3.4966e-05\n",
      "Epoch 122/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.9279 - val_loss: 0.9274 - lr: 3.4966e-05\n",
      "Epoch 123/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.9295 - val_loss: 0.9144 - lr: 3.4966e-05\n",
      "Epoch 124/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.8304 - val_loss: 0.7985 - lr: 1.7483e-05\n",
      "Epoch 125/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.8102 - val_loss: 0.7981 - lr: 1.7483e-05\n",
      "Epoch 126/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.8098 - val_loss: 0.8008 - lr: 1.7483e-05\n",
      "Epoch 127/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.8096 - val_loss: 0.7981 - lr: 1.7483e-05\n",
      "Epoch 128/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.8086 - val_loss: 0.7969 - lr: 1.7483e-05\n",
      "Epoch 129/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.8099 - val_loss: 0.7960 - lr: 1.7483e-05\n",
      "Epoch 130/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.8097 - val_loss: 0.7959 - lr: 1.7483e-05\n",
      "Epoch 131/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.8096 - val_loss: 0.8020 - lr: 1.7483e-05\n",
      "Epoch 132/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.8104 - val_loss: 0.7943 - lr: 1.7483e-05\n",
      "Epoch 133/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.8085 - val_loss: 0.7962 - lr: 1.7483e-05\n",
      "Epoch 134/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.8097 - val_loss: 0.7972 - lr: 1.7483e-05\n",
      "Epoch 135/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.8095 - val_loss: 0.7970 - lr: 1.7483e-05\n",
      "Epoch 136/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.8095 - val_loss: 0.8033 - lr: 1.7483e-05\n",
      "Epoch 137/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.8106 - val_loss: 0.7934 - lr: 1.7483e-05\n",
      "Epoch 138/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.8092 - val_loss: 0.7948 - lr: 1.7483e-05\n",
      "Epoch 139/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.8108 - val_loss: 0.7964 - lr: 1.7483e-05\n",
      "Epoch 140/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.8102 - val_loss: 0.7970 - lr: 1.7483e-05\n",
      "Epoch 141/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.8096 - val_loss: 0.8021 - lr: 1.7483e-05\n",
      "Epoch 142/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.8098 - val_loss: 0.7964 - lr: 1.7483e-05\n",
      "Epoch 143/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7613 - val_loss: 0.7393 - lr: 8.7415e-06\n",
      "Epoch 144/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.7506 - val_loss: 0.7391 - lr: 8.7415e-06\n",
      "Epoch 145/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.7519 - val_loss: 0.7402 - lr: 8.7415e-06\n",
      "Epoch 146/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.7507 - val_loss: 0.7389 - lr: 8.7415e-06\n",
      "Epoch 147/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7499 - val_loss: 0.7388 - lr: 8.7415e-06\n",
      "Epoch 148/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7512 - val_loss: 0.7376 - lr: 8.7415e-06\n",
      "Epoch 149/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7511 - val_loss: 0.7378 - lr: 8.7415e-06\n",
      "Epoch 150/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.7506 - val_loss: 0.7411 - lr: 8.7415e-06\n",
      "Epoch 151/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7513 - val_loss: 0.7371 - lr: 8.7415e-06\n",
      "Epoch 152/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7506 - val_loss: 0.7372 - lr: 8.7415e-06\n",
      "Epoch 153/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7492 - val_loss: 0.7385 - lr: 8.7415e-06\n",
      "Epoch 154/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7504 - val_loss: 0.7383 - lr: 8.7415e-06\n",
      "Epoch 155/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7496 - val_loss: 0.7414 - lr: 8.7415e-06\n",
      "Epoch 156/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.7502 - val_loss: 0.7363 - lr: 8.7415e-06\n",
      "Epoch 157/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.7502 - val_loss: 0.7375 - lr: 8.7415e-06\n",
      "Epoch 158/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.7510 - val_loss: 0.7380 - lr: 8.7415e-06\n",
      "Epoch 159/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.7509 - val_loss: 0.7380 - lr: 8.7415e-06\n",
      "Epoch 160/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.7509 - val_loss: 0.7409 - lr: 8.7415e-06\n",
      "Epoch 161/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.7500 - val_loss: 0.7381 - lr: 8.7415e-06\n",
      "Epoch 162/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7269 - val_loss: 0.7092 - lr: 4.3707e-06\n",
      "Epoch 163/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7209 - val_loss: 0.7090 - lr: 4.3707e-06\n",
      "Epoch 164/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.7222 - val_loss: 0.7098 - lr: 4.3707e-06\n",
      "Epoch 165/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7203 - val_loss: 0.7092 - lr: 4.3707e-06\n",
      "Epoch 166/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.7217 - val_loss: 0.7087 - lr: 4.3707e-06\n",
      "Epoch 167/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7209 - val_loss: 0.7084 - lr: 4.3707e-06\n",
      "Epoch 168/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.7205 - val_loss: 0.7084 - lr: 4.3707e-06\n",
      "Epoch 169/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7202 - val_loss: 0.7103 - lr: 4.3707e-06\n",
      "Epoch 170/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.7213 - val_loss: 0.7081 - lr: 4.3707e-06\n",
      "Epoch 171/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.7199 - val_loss: 0.7084 - lr: 4.3707e-06\n",
      "Epoch 172/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.7218 - val_loss: 0.7090 - lr: 4.3707e-06\n",
      "Epoch 173/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7210 - val_loss: 0.7086 - lr: 4.3707e-06\n",
      "Epoch 174/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.7206 - val_loss: 0.7104 - lr: 4.3707e-06\n",
      "Epoch 175/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.7209 - val_loss: 0.7079 - lr: 4.3707e-06\n",
      "Epoch 176/300\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.7216 - val_loss: 0.7083 - lr: 4.3707e-06\n",
      "Epoch 177/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7197 - val_loss: 0.7085 - lr: 4.3707e-06\n",
      "Epoch 178/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7208 - val_loss: 0.7085 - lr: 4.3707e-06\n",
      "Epoch 179/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7203 - val_loss: 0.7102 - lr: 4.3707e-06\n",
      "Epoch 180/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.7220 - val_loss: 0.7085 - lr: 4.3707e-06\n",
      "Epoch 181/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.7087 - val_loss: 0.6941 - lr: 2.1854e-06\n",
      "Epoch 182/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7051 - val_loss: 0.6939 - lr: 2.1854e-06\n",
      "Epoch 183/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7057 - val_loss: 0.6942 - lr: 2.1854e-06\n",
      "Epoch 184/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.7060 - val_loss: 0.6939 - lr: 2.1854e-06\n",
      "Epoch 185/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7069 - val_loss: 0.6938 - lr: 2.1854e-06\n",
      "Epoch 186/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.7047 - val_loss: 0.6936 - lr: 2.1854e-06\n",
      "Epoch 187/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.7072 - val_loss: 0.6936 - lr: 2.1854e-06\n",
      "Epoch 188/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7065 - val_loss: 0.6945 - lr: 2.1854e-06\n",
      "Epoch 189/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.7065 - val_loss: 0.6934 - lr: 2.1854e-06\n",
      "Epoch 190/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.7060 - val_loss: 0.6935 - lr: 2.1854e-06\n",
      "Epoch 191/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7055 - val_loss: 0.6939 - lr: 2.1854e-06\n",
      "Epoch 192/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.7060 - val_loss: 0.6937 - lr: 2.1854e-06\n",
      "Epoch 193/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.7062 - val_loss: 0.6945 - lr: 2.1854e-06\n",
      "Epoch 194/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.7066 - val_loss: 0.6934 - lr: 2.1854e-06\n",
      "Epoch 195/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.7005 - val_loss: 0.6864 - lr: 1.0927e-06\n",
      "Epoch 196/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6992 - val_loss: 0.6864 - lr: 1.0927e-06\n",
      "Epoch 197/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6989 - val_loss: 0.6866 - lr: 1.0927e-06\n",
      "Epoch 198/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6980 - val_loss: 0.6865 - lr: 1.0927e-06\n",
      "Epoch 199/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6993 - val_loss: 0.6864 - lr: 1.0927e-06\n",
      "Epoch 200/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6989 - val_loss: 0.6862 - lr: 1.0927e-06\n",
      "Epoch 201/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6986 - val_loss: 0.6862 - lr: 1.0927e-06\n",
      "Epoch 202/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6987 - val_loss: 0.6868 - lr: 1.0927e-06\n",
      "Epoch 203/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6982 - val_loss: 0.6862 - lr: 1.0927e-06\n",
      "Epoch 204/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6988 - val_loss: 0.6863 - lr: 1.0927e-06\n",
      "Epoch 205/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6976 - val_loss: 0.6865 - lr: 1.0927e-06\n",
      "Epoch 206/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6957 - val_loss: 0.6827 - lr: 5.4634e-07\n",
      "Epoch 207/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6935 - val_loss: 0.6828 - lr: 5.4634e-07\n",
      "Epoch 208/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6951 - val_loss: 0.6828 - lr: 5.4634e-07\n",
      "Epoch 209/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6946 - val_loss: 0.6827 - lr: 5.4634e-07\n",
      "Epoch 210/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6956 - val_loss: 0.6827 - lr: 5.4634e-07\n",
      "Epoch 211/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6952 - val_loss: 0.6826 - lr: 5.4634e-07\n",
      "Epoch 212/300\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6929 - val_loss: 0.6809 - lr: 2.7317e-07\n",
      "Epoch 213/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6926 - val_loss: 0.6809 - lr: 2.7317e-07\n",
      "Epoch 214/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6920 - val_loss: 0.6809 - lr: 2.7317e-07\n",
      "Epoch 215/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6929 - val_loss: 0.6809 - lr: 2.7317e-07\n",
      "Epoch 216/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6931 - val_loss: 0.6809 - lr: 2.7317e-07\n",
      "Epoch 217/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6929 - val_loss: 0.6808 - lr: 2.7317e-07\n",
      "Epoch 218/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6918 - val_loss: 0.6800 - lr: 1.3659e-07\n",
      "Epoch 219/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6920 - val_loss: 0.6800 - lr: 1.3659e-07\n",
      "Epoch 220/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6925 - val_loss: 0.6800 - lr: 1.3659e-07\n",
      "Epoch 221/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6923 - val_loss: 0.6800 - lr: 1.3659e-07\n",
      "Epoch 222/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6917 - val_loss: 0.6800 - lr: 1.3659e-07\n",
      "Epoch 223/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6938 - val_loss: 0.6799 - lr: 1.3659e-07\n",
      "Epoch 224/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.6924 - val_loss: 0.6795 - lr: 6.8293e-08\n",
      "Epoch 225/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6930 - val_loss: 0.6795 - lr: 6.8293e-08\n",
      "Epoch 226/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6920 - val_loss: 0.6795 - lr: 6.8293e-08\n",
      "Epoch 227/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6913 - val_loss: 0.6795 - lr: 6.8293e-08\n",
      "Epoch 228/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.6912 - val_loss: 0.6795 - lr: 6.8293e-08\n",
      "Epoch 229/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6919 - val_loss: 0.6795 - lr: 6.8293e-08\n",
      "Epoch 230/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.6907 - val_loss: 0.6793 - lr: 3.4146e-08\n",
      "Epoch 231/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6922 - val_loss: 0.6793 - lr: 3.4146e-08\n",
      "Epoch 232/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6910 - val_loss: 0.6793 - lr: 3.4146e-08\n",
      "Epoch 233/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6905 - val_loss: 0.6793 - lr: 3.4146e-08\n",
      "Epoch 234/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6909 - val_loss: 0.6793 - lr: 3.4146e-08\n",
      "Epoch 235/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6911 - val_loss: 0.6793 - lr: 3.4146e-08\n",
      "Epoch 236/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6913 - val_loss: 0.6791 - lr: 1.7073e-08\n",
      "Epoch 237/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6912 - val_loss: 0.6791 - lr: 1.7073e-08\n",
      "Epoch 238/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6919 - val_loss: 0.6792 - lr: 1.7073e-08\n",
      "Epoch 239/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6919 - val_loss: 0.6791 - lr: 1.7073e-08\n",
      "Epoch 240/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6923 - val_loss: 0.6791 - lr: 1.7073e-08\n",
      "Epoch 241/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6919 - val_loss: 0.6791 - lr: 1.7073e-08\n",
      "Epoch 242/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6912 - val_loss: 0.6791 - lr: 8.5366e-09\n",
      "Epoch 243/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6916 - val_loss: 0.6791 - lr: 8.5366e-09\n",
      "Epoch 244/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6918 - val_loss: 0.6791 - lr: 8.5366e-09\n",
      "Epoch 245/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6911 - val_loss: 0.6791 - lr: 8.5366e-09\n",
      "Epoch 246/300\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.691 - 1s 18ms/step - loss: 0.6912 - val_loss: 0.6791 - lr: 8.5366e-09\n",
      "Epoch 247/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6915 - val_loss: 0.6791 - lr: 4.2683e-09\n",
      "Epoch 248/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6920 - val_loss: 0.6791 - lr: 4.2683e-09\n",
      "Epoch 249/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6909 - val_loss: 0.6791 - lr: 4.2683e-09\n",
      "Epoch 250/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6915 - val_loss: 0.6791 - lr: 4.2683e-09\n",
      "Epoch 251/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.6912 - val_loss: 0.6791 - lr: 4.2683e-09\n",
      "Epoch 252/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6913 - val_loss: 0.6790 - lr: 2.1342e-09\n",
      "Epoch 253/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6918 - val_loss: 0.6790 - lr: 2.1342e-09\n",
      "Epoch 254/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6913 - val_loss: 0.6790 - lr: 2.1342e-09\n",
      "Epoch 255/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6910 - val_loss: 0.6790 - lr: 2.1342e-09\n",
      "Epoch 256/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6914 - val_loss: 0.6790 - lr: 2.1342e-09\n",
      "Epoch 257/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6909 - val_loss: 0.6790 - lr: 2.1342e-09\n",
      "Epoch 258/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6908 - val_loss: 0.6790 - lr: 1.0671e-09\n",
      "Epoch 259/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6915 - val_loss: 0.6790 - lr: 1.0671e-09\n",
      "Epoch 260/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6910 - val_loss: 0.6790 - lr: 1.0671e-09\n",
      "Epoch 261/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6917 - val_loss: 0.6790 - lr: 1.0671e-09\n",
      "Epoch 262/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6911 - val_loss: 0.6790 - lr: 1.0671e-09\n",
      "Epoch 263/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6912 - val_loss: 0.6790 - lr: 5.3354e-10\n",
      "Epoch 264/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6909 - val_loss: 0.6790 - lr: 5.3354e-10\n",
      "Epoch 265/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6899 - val_loss: 0.6790 - lr: 5.3354e-10\n",
      "Epoch 266/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6922 - val_loss: 0.6790 - lr: 5.3354e-10\n",
      "Epoch 267/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6912 - val_loss: 0.6790 - lr: 5.3354e-10\n",
      "Epoch 268/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.6908 - val_loss: 0.6790 - lr: 2.6677e-10\n",
      "Epoch 269/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6910 - val_loss: 0.6790 - lr: 2.6677e-10\n",
      "Epoch 270/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.6913 - val_loss: 0.6790 - lr: 2.6677e-10\n",
      "Epoch 271/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6905 - val_loss: 0.6790 - lr: 2.6677e-10\n",
      "Epoch 272/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6919 - val_loss: 0.6790 - lr: 2.6677e-10\n",
      "Epoch 273/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6903 - val_loss: 0.6790 - lr: 1.3338e-10\n",
      "Epoch 274/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6914 - val_loss: 0.6790 - lr: 1.3338e-10\n",
      "Epoch 275/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6908 - val_loss: 0.6790 - lr: 1.3338e-10\n",
      "Epoch 276/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6911 - val_loss: 0.6790 - lr: 1.3338e-10\n",
      "Epoch 277/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6917 - val_loss: 0.6790 - lr: 1.3338e-10\n",
      "Epoch 278/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6908 - val_loss: 0.6790 - lr: 6.6692e-11\n",
      "Epoch 279/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6921 - val_loss: 0.6790 - lr: 6.6692e-11\n",
      "Epoch 280/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6912 - val_loss: 0.6790 - lr: 6.6692e-11\n",
      "Epoch 281/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6918 - val_loss: 0.6790 - lr: 6.6692e-11\n",
      "Epoch 282/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6918 - val_loss: 0.6790 - lr: 6.6692e-11\n",
      "Epoch 283/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6921 - val_loss: 0.6790 - lr: 3.3346e-11\n",
      "Epoch 284/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6907 - val_loss: 0.6790 - lr: 3.3346e-11\n",
      "Epoch 285/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6916 - val_loss: 0.6790 - lr: 3.3346e-11\n",
      "Epoch 286/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6900 - val_loss: 0.6790 - lr: 3.3346e-11\n",
      "Epoch 287/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6913 - val_loss: 0.6790 - lr: 3.3346e-11\n",
      "Epoch 288/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6914 - val_loss: 0.6790 - lr: 1.6673e-11\n",
      "Epoch 289/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6913 - val_loss: 0.6790 - lr: 1.6673e-11\n",
      "Epoch 290/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6915 - val_loss: 0.6790 - lr: 1.6673e-11\n",
      "Epoch 291/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6911 - val_loss: 0.6790 - lr: 1.6673e-11\n",
      "Epoch 292/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6914 - val_loss: 0.6790 - lr: 1.6673e-11\n",
      "Epoch 293/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6914 - val_loss: 0.6790 - lr: 8.3365e-12\n",
      "Epoch 294/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6916 - val_loss: 0.6790 - lr: 8.3365e-12\n",
      "Epoch 295/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6911 - val_loss: 0.6790 - lr: 8.3365e-12\n",
      "Epoch 296/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6909 - val_loss: 0.6790 - lr: 8.3365e-12\n",
      "Epoch 297/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.6905 - val_loss: 0.6790 - lr: 8.3365e-12\n",
      "Epoch 298/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.6906 - val_loss: 0.6790 - lr: 4.1683e-12\n",
      "Epoch 299/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6911 - val_loss: 0.6790 - lr: 4.1683e-12\n",
      "Epoch 300/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6914 - val_loss: 0.6790 - lr: 4.1683e-12\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "#from keras import backend\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import Dropout\n",
    "#from keras.regularizers import l1\n",
    "#from keras.regularizers import l2\n",
    "#from keras.regularizers import l1_l2\n",
    "\n",
    "\n",
    "x_train_vector = ut.df_a_vector(x_train)\n",
    "y_train_vector = ut.df_a_vector(y_train)\n",
    "x_test_vector = ut.df_a_vector(x_test)\n",
    "y_test_vector = ut.df_a_vector(y_test)\n",
    "\n",
    "#input_dim = x_train.shape[1]\n",
    "#\n",
    "#alfa = 1e-3\n",
    "#\n",
    "#model = Sequential()\n",
    "#model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(Dense(128, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "##model.add(Dropout(0.25))\n",
    "##model.add(Dense(256, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='tanh'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(128, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(64, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='tanh'))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(Dense(16, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "#model.add(Dense(8, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "#model.add(Dense(1, activation='tanh'))\n",
    "#\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#\n",
    "#backend.set_value(model.optimizer.learning_rate, 5e-4)\n",
    "\n",
    "best_hparams = ut.hyperparams_from_json('../Archivos/Neuronales')\n",
    "model = md.get_neural_network_model(best_hparams, x_train.shape[1])\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"Neuronales_Mejor_Modelo.hdf5\", \n",
    "                                       monitor='val_loss', \n",
    "                                       verbose=0,\n",
    "                                       save_best_only=True, \n",
    "                                       mode='min'),\n",
    "    \n",
    "    #tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "    #                                 min_delta=0.01,\n",
    "    #                                 mode='min',\n",
    "    #                                 patience=10),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                      mode='min',\n",
    "                                      factor=0.5,\n",
    "                                      patience=5,\n",
    "                                      cooldown=0, \n",
    "                                      min_lr=1e-24)\n",
    "]\n",
    "\n",
    "fit_dict = {\n",
    "    #'x' : x_train_vector,\n",
    "    #'y' : y_train_vector,\n",
    "    #'validation_data' : (x_test_vector, y_test_vector),\n",
    "    'epochs' : epochs,\n",
    "    'batch_size' : batch_size,\n",
    "    'verbose' : 1,\n",
    "    'callbacks' : my_callbacks\n",
    "}\n",
    "\n",
    "\n",
    "history = model.fit(x_train_vector,\n",
    "                    y_train_vector,\n",
    "                    validation_data=(x_test_vector, y_test_vector),\n",
    "                    **fit_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('Neuronales_7740.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "BoO8syeWoQhd",
    "outputId": "e4737981-19af-4c5f-c69e-f2208387dcff"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Accuracy vs Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model = keras.models.load_model('Neuronales_Mejor_Modelo.hdf5')\n",
    "#new_predictions = new_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6053156146179401"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = [a > 0.5 for a in model.predict(x_test_vector)]\n",
    "\n",
    "\n",
    "score = accuracy_score(y_pred, y_test_vector)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 0s 3ms/step - loss: 0.6790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6790302991867065"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_vector, y_test_vector)#, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Pruebas TP2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
