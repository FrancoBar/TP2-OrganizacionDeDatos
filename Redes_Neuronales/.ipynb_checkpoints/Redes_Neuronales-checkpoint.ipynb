{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQfQ2ppfQasK"
   },
   "source": [
    "# Redes Neuronales\n",
    "## Preparacion de datos para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "8OQLMHvqkvdV",
    "outputId": "4d6f4c0b-232b-468e-8734-083a58109f70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales_Contract_No</th>\n",
       "      <th>Opportunity_Name</th>\n",
       "      <th>Planned_Opportunity_Duration</th>\n",
       "      <th>Account_Name</th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Planned_Time_Until_Deliver</th>\n",
       "      <th>Bureaucratic_Code</th>\n",
       "      <th>Last_Modified_By</th>\n",
       "      <th>Opportunity_Owner</th>\n",
       "      <th>Product_Family</th>\n",
       "      <th>Opportunity_Type</th>\n",
       "      <th>Total_Amount(USD)</th>\n",
       "      <th>Account_Owner</th>\n",
       "      <th>Account_Type</th>\n",
       "      <th>Opportunity_Created_Date</th>\n",
       "      <th>Territory</th>\n",
       "      <th>Product_Amount_Deviation_of_Product_Family_rate</th>\n",
       "      <th>Planned_Deliver_Duration</th>\n",
       "      <th>Account_Created_Date</th>\n",
       "      <th>Billing_Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>13.14</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.15</td>\n",
       "      <td>15.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-4.78</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.97</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>5.51</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6.19</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>-4.22</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.66</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>5.98</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-4.19</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sales_Contract_No  Opportunity_Name  Planned_Opportunity_Duration  \\\n",
       "0               0.42             -0.08                         13.14   \n",
       "1              -0.39             -0.08                          5.51   \n",
       "2              -0.66             -0.08                          5.98   \n",
       "\n",
       "   Account_Name  Product_Name  Planned_Time_Until_Deliver  Bureaucratic_Code  \\\n",
       "0         -0.02          0.15                       15.21               0.14   \n",
       "1         -0.02          0.15                        6.19               0.14   \n",
       "2         -0.02          0.15                        6.75               0.14   \n",
       "\n",
       "   Last_Modified_By  Opportunity_Owner  Product_Family  Opportunity_Type  \\\n",
       "0              0.37               0.38            0.34              0.50   \n",
       "1              0.37               0.38            0.34              0.50   \n",
       "2              0.37               0.38            0.34             -1.06   \n",
       "\n",
       "   Total_Amount(USD)  Account_Owner  Account_Type  Opportunity_Created_Date  \\\n",
       "0               0.89           0.33          0.39                     -4.78   \n",
       "1               1.63           0.33         -1.09                     -4.22   \n",
       "2               0.47           0.33          0.39                     -4.19   \n",
       "\n",
       "   Territory  Product_Amount_Deviation_of_Product_Family_rate  \\\n",
       "0       0.36                                             0.97   \n",
       "1       0.36                                             2.44   \n",
       "2       0.36                                            -0.70   \n",
       "\n",
       "   Planned_Deliver_Duration  Account_Created_Date  Billing_Country  \n",
       "0                     -0.41                 -1.35             0.53  \n",
       "1                      0.28                 -0.96             0.53  \n",
       "2                      0.26                 -1.29             0.53  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import Utilidades as ut\n",
    "import Modelos as md\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_train = pd.read_pickle(\"../Archivos/Neuronales_entrenamiento.pkl\")\n",
    "df_test = pd.read_pickle(\"../Archivos/Neuronales_validacion.pkl\")\n",
    "\n",
    "x_train, y_train = ut.split_labels(df_train)\n",
    "x_test, y_test = ut.split_labels(df_test)\n",
    "\n",
    "#Convertimos las fechas a numeros (cantidad de dias transcurridos) y luego las normalizamos\n",
    "x_train, x_test = ut.conversion_fechas(x_train, x_test)\n",
    "x_train, x_test = ut.codificar_categoricas(x_train, y_train, x_test, modo='catboost')\n",
    "x_train, x_test = ut.normalizacion_numericas(x_train, x_test, modo='normalizacion')\n",
    "x_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oSHQttP_cuK"
   },
   "source": [
    "## Creacion del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rN3ODhM_Zxm",
    "outputId": "81bc755e-a46a-4f34-eced-99d6297b67e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando hiperparametros desde el archivo: '../Archivos/Neuronales_best_hyperparam.json'\n",
      "Epoch 1/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 17.6687 - val_loss: 4.9637 - lr: 0.0042\n",
      "Epoch 2/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 2.7545 - val_loss: 1.6309 - lr: 0.0042\n",
      "Epoch 3/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 1.1880 - val_loss: 1.0569 - lr: 0.0042\n",
      "Epoch 4/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.8919 - val_loss: 0.8888 - lr: 0.0042\n",
      "Epoch 5/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.7435 - val_loss: 0.8191 - lr: 0.0042\n",
      "Epoch 6/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.8037 - val_loss: 0.9321 - lr: 0.0042\n",
      "Epoch 7/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.7487 - val_loss: 0.7201 - lr: 0.0042\n",
      "Epoch 8/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.6752 - val_loss: 0.7063 - lr: 0.0042\n",
      "Epoch 9/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.6501 - val_loss: 0.7496 - lr: 0.0042\n",
      "Epoch 10/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.6851 - val_loss: 0.7316 - lr: 0.0042\n",
      "Epoch 11/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.6800 - val_loss: 0.6982 - lr: 0.0042\n",
      "Epoch 12/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6556 - val_loss: 0.7404 - lr: 0.0042\n",
      "Epoch 13/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.6452 - val_loss: 0.6998 - lr: 0.0042\n",
      "Epoch 14/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.6769 - val_loss: 0.7080 - lr: 0.0042\n",
      "Epoch 15/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.6315 - val_loss: 0.6945 - lr: 0.0042\n",
      "Epoch 16/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6354 - val_loss: 0.6728 - lr: 0.0042\n",
      "Epoch 17/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6623 - val_loss: 0.7478 - lr: 0.0042\n",
      "Epoch 18/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.6475 - val_loss: 0.6604 - lr: 0.0042\n",
      "Epoch 19/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6167 - val_loss: 0.6954 - lr: 0.0042\n",
      "Epoch 20/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6250 - val_loss: 0.7159 - lr: 0.0042\n",
      "Epoch 21/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.6286 - val_loss: 0.6864 - lr: 0.0042\n",
      "Epoch 22/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.6148 - val_loss: 0.6996 - lr: 0.0042\n",
      "Epoch 23/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.6235 - val_loss: 0.6803 - lr: 0.0042\n",
      "Epoch 24/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.5081 - val_loss: 0.5415 - lr: 0.0021\n",
      "Epoch 25/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.4712 - val_loss: 0.5482 - lr: 0.0021\n",
      "Epoch 26/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.4747 - val_loss: 0.5445 - lr: 0.0021\n",
      "Epoch 27/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.4831 - val_loss: 0.5436 - lr: 0.0021\n",
      "Epoch 28/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.4746 - val_loss: 0.5451 - lr: 0.0021\n",
      "Epoch 29/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.4815 - val_loss: 0.5356 - lr: 0.0021\n",
      "Epoch 30/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.4768 - val_loss: 0.5292 - lr: 0.0021\n",
      "Epoch 31/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.4680 - val_loss: 0.5483 - lr: 0.0021\n",
      "Epoch 32/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.4655 - val_loss: 0.5262 - lr: 0.0021\n",
      "Epoch 33/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.4689 - val_loss: 0.5503 - lr: 0.0021\n",
      "Epoch 34/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.4638 - val_loss: 0.5461 - lr: 0.0021\n",
      "Epoch 35/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.4691 - val_loss: 0.5349 - lr: 0.0021\n",
      "Epoch 36/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.4601 - val_loss: 0.5351 - lr: 0.0021\n",
      "Epoch 37/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.4621 - val_loss: 0.5234 - lr: 0.0021\n",
      "Epoch 38/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.4633 - val_loss: 0.5377 - lr: 0.0021\n",
      "Epoch 39/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.4594 - val_loss: 0.5241 - lr: 0.0021\n",
      "Epoch 40/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.4551 - val_loss: 0.5267 - lr: 0.0021\n",
      "Epoch 41/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.4567 - val_loss: 0.5333 - lr: 0.0021\n",
      "Epoch 42/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.4563 - val_loss: 0.5226 - lr: 0.0021\n",
      "Epoch 43/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.4579 - val_loss: 0.5207 - lr: 0.0021\n",
      "Epoch 44/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.4470 - val_loss: 0.5129 - lr: 0.0021\n",
      "Epoch 45/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.4557 - val_loss: 0.5277 - lr: 0.0021\n",
      "Epoch 46/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.4563 - val_loss: 0.5156 - lr: 0.0021\n",
      "Epoch 47/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.4563 - val_loss: 0.5292 - lr: 0.0021\n",
      "Epoch 48/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.4532 - val_loss: 0.5450 - lr: 0.0021\n",
      "Epoch 49/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.4463 - val_loss: 0.5237 - lr: 0.0021\n",
      "Epoch 50/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3938 - val_loss: 0.4606 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.3815 - val_loss: 0.4549 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.3858 - val_loss: 0.4540 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3785 - val_loss: 0.4575 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.3751 - val_loss: 0.4649 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.3806 - val_loss: 0.4569 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.3772 - val_loss: 0.4594 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3715 - val_loss: 0.4636 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3455 - val_loss: 0.4293 - lr: 5.2098e-04\n",
      "Epoch 59/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3402 - val_loss: 0.4164 - lr: 5.2098e-04\n",
      "Epoch 60/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3372 - val_loss: 0.4239 - lr: 5.2098e-04\n",
      "Epoch 61/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.3353 - val_loss: 0.4238 - lr: 5.2098e-04\n",
      "Epoch 62/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3382 - val_loss: 0.4230 - lr: 5.2098e-04\n",
      "Epoch 63/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.3384 - val_loss: 0.4227 - lr: 5.2098e-04\n",
      "Epoch 64/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.3317 - val_loss: 0.4211 - lr: 5.2098e-04\n",
      "Epoch 65/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3191 - val_loss: 0.4033 - lr: 2.6049e-04\n",
      "Epoch 66/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3159 - val_loss: 0.4032 - lr: 2.6049e-04\n",
      "Epoch 67/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3162 - val_loss: 0.4083 - lr: 2.6049e-04\n",
      "Epoch 68/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.3207 - val_loss: 0.4045 - lr: 2.6049e-04\n",
      "Epoch 69/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3195 - val_loss: 0.4032 - lr: 2.6049e-04\n",
      "Epoch 70/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3153 - val_loss: 0.4046 - lr: 2.6049e-04\n",
      "Epoch 71/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3193 - val_loss: 0.4037 - lr: 2.6049e-04\n",
      "Epoch 72/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3173 - val_loss: 0.4052 - lr: 2.6049e-04\n",
      "Epoch 73/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3178 - val_loss: 0.4035 - lr: 2.6049e-04\n",
      "Epoch 74/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3274 - val_loss: 0.4023 - lr: 2.6049e-04\n",
      "Epoch 75/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.3242 - val_loss: 0.4028 - lr: 2.6049e-04\n",
      "Epoch 76/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3148 - val_loss: 0.4067 - lr: 2.6049e-04\n",
      "Epoch 77/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3178 - val_loss: 0.4045 - lr: 2.6049e-04\n",
      "Epoch 78/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3242 - val_loss: 0.3998 - lr: 2.6049e-04\n",
      "Epoch 79/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3172 - val_loss: 0.3993 - lr: 2.6049e-04\n",
      "Epoch 80/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3162 - val_loss: 0.4033 - lr: 2.6049e-04\n",
      "Epoch 81/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3167 - val_loss: 0.4090 - lr: 2.6049e-04\n",
      "Epoch 82/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3253 - val_loss: 0.4020 - lr: 2.6049e-04\n",
      "Epoch 83/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3174 - val_loss: 0.4024 - lr: 2.6049e-04\n",
      "Epoch 84/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3191 - val_loss: 0.4049 - lr: 2.6049e-04\n",
      "Epoch 85/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3056 - val_loss: 0.3956 - lr: 1.3024e-04\n",
      "Epoch 86/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3051 - val_loss: 0.3977 - lr: 1.3024e-04\n",
      "Epoch 87/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3052 - val_loss: 0.3986 - lr: 1.3024e-04\n",
      "Epoch 88/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.3139 - val_loss: 0.3931 - lr: 1.3024e-04\n",
      "Epoch 89/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3058 - val_loss: 0.3962 - lr: 1.3024e-04\n",
      "Epoch 90/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3089 - val_loss: 0.3964 - lr: 1.3024e-04\n",
      "Epoch 91/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.3024 - val_loss: 0.3975 - lr: 1.3024e-04\n",
      "Epoch 92/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3037 - val_loss: 0.3963 - lr: 1.3024e-04\n",
      "Epoch 93/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.3065 - val_loss: 0.3943 - lr: 1.3024e-04\n",
      "Epoch 94/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.3020 - val_loss: 0.3913 - lr: 6.5122e-05\n",
      "Epoch 95/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.3059 - val_loss: 0.3907 - lr: 6.5122e-05\n",
      "Epoch 96/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.3047 - val_loss: 0.3907 - lr: 6.5122e-05\n",
      "Epoch 97/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3076 - val_loss: 0.3890 - lr: 6.5122e-05\n",
      "Epoch 98/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3045 - val_loss: 0.3882 - lr: 6.5122e-05\n",
      "Epoch 99/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3044 - val_loss: 0.3882 - lr: 6.5122e-05\n",
      "Epoch 100/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.3033 - val_loss: 0.3891 - lr: 6.5122e-05\n",
      "Epoch 101/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3064 - val_loss: 0.3878 - lr: 6.5122e-05\n",
      "Epoch 102/300\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.3032 - val_loss: 0.3872 - lr: 6.5122e-05\n",
      "Epoch 103/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3044 - val_loss: 0.3878 - lr: 6.5122e-05\n",
      "Epoch 104/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.3043 - val_loss: 0.3885 - lr: 6.5122e-05\n",
      "Epoch 105/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.3058 - val_loss: 0.3890 - lr: 6.5122e-05\n",
      "Epoch 106/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.2977 - val_loss: 0.3890 - lr: 6.5122e-05\n",
      "Epoch 107/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3009 - val_loss: 0.3883 - lr: 6.5122e-05\n",
      "Epoch 108/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.2987 - val_loss: 0.3865 - lr: 3.2561e-05\n",
      "Epoch 109/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2964 - val_loss: 0.3871 - lr: 3.2561e-05\n",
      "Epoch 110/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2974 - val_loss: 0.3867 - lr: 3.2561e-05\n",
      "Epoch 111/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2968 - val_loss: 0.3877 - lr: 3.2561e-05\n",
      "Epoch 112/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2981 - val_loss: 0.3873 - lr: 3.2561e-05\n",
      "Epoch 113/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2988 - val_loss: 0.3877 - lr: 3.2561e-05\n",
      "Epoch 114/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.2986 - val_loss: 0.3870 - lr: 1.6281e-05\n",
      "Epoch 115/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2955 - val_loss: 0.3869 - lr: 1.6281e-05\n",
      "Epoch 116/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2980 - val_loss: 0.3872 - lr: 1.6281e-05\n",
      "Epoch 117/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2980 - val_loss: 0.3872 - lr: 1.6281e-05\n",
      "Epoch 118/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3016 - val_loss: 0.3870 - lr: 1.6281e-05\n",
      "Epoch 119/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3002 - val_loss: 0.3865 - lr: 8.1403e-06\n",
      "Epoch 120/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2967 - val_loss: 0.3868 - lr: 8.1403e-06\n",
      "Epoch 121/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3006 - val_loss: 0.3867 - lr: 8.1403e-06\n",
      "Epoch 122/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2949 - val_loss: 0.3867 - lr: 8.1403e-06\n",
      "Epoch 123/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2983 - val_loss: 0.3866 - lr: 8.1403e-06\n",
      "Epoch 124/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3035 - val_loss: 0.3861 - lr: 4.0701e-06\n",
      "Epoch 125/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3004 - val_loss: 0.3862 - lr: 4.0701e-06\n",
      "Epoch 126/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.2944 - val_loss: 0.3859 - lr: 4.0701e-06\n",
      "Epoch 127/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2934 - val_loss: 0.3861 - lr: 4.0701e-06\n",
      "Epoch 128/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3011 - val_loss: 0.3860 - lr: 4.0701e-06\n",
      "Epoch 129/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3017 - val_loss: 0.3860 - lr: 4.0701e-06\n",
      "Epoch 130/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2949 - val_loss: 0.3860 - lr: 4.0701e-06\n",
      "Epoch 131/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.2920 - val_loss: 0.3861 - lr: 4.0701e-06\n",
      "Epoch 132/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.2916 - val_loss: 0.3860 - lr: 2.0351e-06\n",
      "Epoch 133/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2953 - val_loss: 0.3860 - lr: 2.0351e-06\n",
      "Epoch 134/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3028 - val_loss: 0.3860 - lr: 2.0351e-06\n",
      "Epoch 135/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3019 - val_loss: 0.3860 - lr: 2.0351e-06\n",
      "Epoch 136/300\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.2929 - val_loss: 0.3860 - lr: 2.0351e-06\n",
      "Epoch 137/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2980 - val_loss: 0.3859 - lr: 1.0175e-06\n",
      "Epoch 138/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2992 - val_loss: 0.3859 - lr: 1.0175e-06\n",
      "Epoch 139/300\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.2954 - val_loss: 0.3859 - lr: 1.0175e-06\n",
      "Epoch 140/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3019 - val_loss: 0.3859 - lr: 1.0175e-06\n",
      "Epoch 141/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.2964 - val_loss: 0.3859 - lr: 1.0175e-06\n",
      "Epoch 142/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2947 - val_loss: 0.3858 - lr: 5.0877e-07\n",
      "Epoch 143/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.2995 - val_loss: 0.3858 - lr: 5.0877e-07\n",
      "Epoch 144/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3042 - val_loss: 0.3858 - lr: 5.0877e-07\n",
      "Epoch 145/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2947 - val_loss: 0.3858 - lr: 5.0877e-07\n",
      "Epoch 146/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2983 - val_loss: 0.3859 - lr: 5.0877e-07\n",
      "Epoch 147/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2962 - val_loss: 0.3858 - lr: 2.5438e-07\n",
      "Epoch 148/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.2927 - val_loss: 0.3858 - lr: 2.5438e-07\n",
      "Epoch 149/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2953 - val_loss: 0.3858 - lr: 2.5438e-07\n",
      "Epoch 150/300\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.2973 - val_loss: 0.3858 - lr: 2.5438e-07\n",
      "Epoch 151/300\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.2980 - val_loss: 0.3858 - lr: 2.5438e-07\n",
      "Epoch 152/300\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3019 - val_loss: 0.3858 - lr: 1.2719e-07\n",
      "Epoch 153/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3011 - val_loss: 0.3858 - lr: 1.2719e-07\n",
      "Epoch 154/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2978 - val_loss: 0.3858 - lr: 1.2719e-07\n",
      "Epoch 155/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2968 - val_loss: 0.3858 - lr: 1.2719e-07\n",
      "Epoch 156/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2972 - val_loss: 0.3858 - lr: 1.2719e-07\n",
      "Epoch 157/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2925 - val_loss: 0.3858 - lr: 6.3596e-08\n",
      "Epoch 158/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2971 - val_loss: 0.3858 - lr: 6.3596e-08\n",
      "Epoch 159/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.3030 - val_loss: 0.3858 - lr: 6.3596e-08\n",
      "Epoch 160/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.2999 - val_loss: 0.3858 - lr: 6.3596e-08\n",
      "Epoch 161/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2926 - val_loss: 0.3858 - lr: 6.3596e-08\n",
      "Epoch 162/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2970 - val_loss: 0.3858 - lr: 3.1798e-08\n",
      "Epoch 163/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2947 - val_loss: 0.3858 - lr: 3.1798e-08\n",
      "Epoch 164/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2919 - val_loss: 0.3858 - lr: 3.1798e-08\n",
      "Epoch 165/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2985 - val_loss: 0.3858 - lr: 3.1798e-08\n",
      "Epoch 166/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.2971 - val_loss: 0.3858 - lr: 3.1798e-08\n",
      "Epoch 167/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.2956 - val_loss: 0.3858 - lr: 1.5899e-08\n",
      "Epoch 168/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2975 - val_loss: 0.3858 - lr: 1.5899e-08\n",
      "Epoch 169/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.3021 - val_loss: 0.3858 - lr: 1.5899e-08\n",
      "Epoch 170/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2992 - val_loss: 0.3858 - lr: 1.5899e-08\n",
      "Epoch 171/300\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.2955 - val_loss: 0.3858 - lr: 1.5899e-08\n",
      "Epoch 172/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.3002 - val_loss: 0.3858 - lr: 7.9495e-09\n",
      "Epoch 173/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2960 - val_loss: 0.3858 - lr: 7.9495e-09\n",
      "Epoch 174/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.2946 - val_loss: 0.3858 - lr: 7.9495e-09\n",
      "Epoch 175/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3009 - val_loss: 0.3858 - lr: 7.9495e-09\n",
      "Epoch 176/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.3027 - val_loss: 0.3858 - lr: 7.9495e-09\n",
      "Epoch 177/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2951 - val_loss: 0.3858 - lr: 3.9748e-09\n",
      "Epoch 178/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2997 - val_loss: 0.3858 - lr: 3.9748e-09\n",
      "Epoch 179/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2961 - val_loss: 0.3858 - lr: 3.9748e-09\n",
      "Epoch 180/300\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.2987 - val_loss: 0.3858 - lr: 3.9748e-09\n",
      "Epoch 181/300\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.2957 - val_loss: 0.3858 - lr: 3.9748e-09\n",
      "Epoch 182/300\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.2962 - val_loss: 0.3858 - lr: 1.9874e-09\n",
      "Epoch 183/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2992 - val_loss: 0.3858 - lr: 1.9874e-09\n",
      "Epoch 184/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2947 - val_loss: 0.3858 - lr: 1.9874e-09\n",
      "Epoch 185/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2990 - val_loss: 0.3858 - lr: 1.9874e-09\n",
      "Epoch 186/300\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.3019 - val_loss: 0.3858 - lr: 1.9874e-09\n",
      "Epoch 187/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.3030 - val_loss: 0.3858 - lr: 9.9369e-10\n",
      "Epoch 188/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.2999 - val_loss: 0.3858 - lr: 9.9369e-10\n",
      "Epoch 189/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2958 - val_loss: 0.3858 - lr: 9.9369e-10\n",
      "Epoch 190/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.3016 - val_loss: 0.3858 - lr: 9.9369e-10\n",
      "Epoch 191/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2948 - val_loss: 0.3858 - lr: 9.9369e-10\n",
      "Epoch 192/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2962 - val_loss: 0.3858 - lr: 4.9684e-10\n",
      "Epoch 193/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3000 - val_loss: 0.3858 - lr: 4.9684e-10\n",
      "Epoch 194/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2983 - val_loss: 0.3858 - lr: 4.9684e-10\n",
      "Epoch 195/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.2948 - val_loss: 0.3858 - lr: 4.9684e-10\n",
      "Epoch 196/300\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.2951 - val_loss: 0.3858 - lr: 4.9684e-10\n",
      "Epoch 197/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.3072 - val_loss: 0.3858 - lr: 2.4842e-10\n",
      "Epoch 198/300\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.2942 - val_loss: 0.3858 - lr: 2.4842e-10\n",
      "Epoch 199/300\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.2968 - val_loss: 0.3858 - lr: 2.4842e-10\n",
      "Epoch 200/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3000 - val_loss: 0.3858 - lr: 2.4842e-10\n",
      "Epoch 201/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2980 - val_loss: 0.3858 - lr: 2.4842e-10\n",
      "Epoch 202/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.3002 - val_loss: 0.3858 - lr: 1.2421e-10\n",
      "Epoch 203/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2919 - val_loss: 0.3858 - lr: 1.2421e-10\n",
      "Epoch 204/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2974 - val_loss: 0.3858 - lr: 1.2421e-10\n",
      "Epoch 205/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2985 - val_loss: 0.3858 - lr: 1.2421e-10\n",
      "Epoch 206/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.2988 - val_loss: 0.3858 - lr: 1.2421e-10\n",
      "Epoch 207/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2915 - val_loss: 0.3858 - lr: 6.2106e-11\n",
      "Epoch 208/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.3009 - val_loss: 0.3858 - lr: 6.2106e-11\n",
      "Epoch 209/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2988 - val_loss: 0.3858 - lr: 6.2106e-11\n",
      "Epoch 210/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.2972 - val_loss: 0.3858 - lr: 6.2106e-11\n",
      "Epoch 211/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.3031 - val_loss: 0.3858 - lr: 6.2106e-11\n",
      "Epoch 212/300\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.2968 - val_loss: 0.3858 - lr: 3.1053e-11\n",
      "Epoch 213/300\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3024 - val_loss: 0.3858 - lr: 3.1053e-11\n",
      "Epoch 214/300\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.2969 - val_loss: 0.3858 - lr: 3.1053e-11\n",
      "Epoch 215/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2926 - val_loss: 0.3858 - lr: 3.1053e-11\n",
      "Epoch 216/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2918 - val_loss: 0.3858 - lr: 3.1053e-11\n",
      "Epoch 217/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3001 - val_loss: 0.3858 - lr: 1.5526e-11\n",
      "Epoch 218/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2982 - val_loss: 0.3858 - lr: 1.5526e-11\n",
      "Epoch 219/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2946 - val_loss: 0.3858 - lr: 1.5526e-11\n",
      "Epoch 220/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.2994 - val_loss: 0.3858 - lr: 1.5526e-11\n",
      "Epoch 221/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.2978 - val_loss: 0.3858 - lr: 1.5526e-11\n",
      "Epoch 222/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.2947 - val_loss: 0.3858 - lr: 7.7632e-12\n",
      "Epoch 223/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3037 - val_loss: 0.3858 - lr: 7.7632e-12\n",
      "Epoch 224/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.2977 - val_loss: 0.3858 - lr: 7.7632e-12\n",
      "Epoch 225/300\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2978 - val_loss: 0.3858 - lr: 7.7632e-12\n",
      "Epoch 226/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2964 - val_loss: 0.3858 - lr: 7.7632e-12\n",
      "Epoch 227/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.3006 - val_loss: 0.3858 - lr: 3.8816e-12\n",
      "Epoch 228/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.3005 - val_loss: 0.3858 - lr: 3.8816e-12\n",
      "Epoch 229/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2957 - val_loss: 0.3858 - lr: 3.8816e-12\n",
      "Epoch 230/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.2987 - val_loss: 0.3858 - lr: 3.8816e-12\n",
      "Epoch 231/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2946 - val_loss: 0.3858 - lr: 3.8816e-12\n",
      "Epoch 232/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2953 - val_loss: 0.3858 - lr: 1.9408e-12\n",
      "Epoch 233/300\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.2948 - val_loss: 0.3858 - lr: 1.9408e-12\n",
      "Epoch 234/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2965 - val_loss: 0.3858 - lr: 1.9408e-12\n",
      "Epoch 235/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.2966 - val_loss: 0.3858 - lr: 1.9408e-12\n",
      "Epoch 236/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.3007 - val_loss: 0.3858 - lr: 1.9408e-12\n",
      "Epoch 237/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2985 - val_loss: 0.3858 - lr: 9.7040e-13\n",
      "Epoch 238/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2974 - val_loss: 0.3858 - lr: 9.7040e-13\n",
      "Epoch 239/300\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2990 - val_loss: 0.3858 - lr: 9.7040e-13\n",
      "Epoch 240/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2973 - val_loss: 0.3858 - lr: 9.7040e-13\n",
      "Epoch 241/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2953 - val_loss: 0.3858 - lr: 9.7040e-13\n",
      "Epoch 242/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3033 - val_loss: 0.3858 - lr: 4.8520e-13\n",
      "Epoch 243/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2926 - val_loss: 0.3858 - lr: 4.8520e-13\n",
      "Epoch 244/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2914 - val_loss: 0.3858 - lr: 4.8520e-13\n",
      "Epoch 245/300\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.2993 - val_loss: 0.3858 - lr: 4.8520e-13\n",
      "Epoch 246/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3066 - val_loss: 0.3858 - lr: 4.8520e-13\n",
      "Epoch 247/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2923 - val_loss: 0.3858 - lr: 2.4260e-13\n",
      "Epoch 248/300\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2966 - val_loss: 0.3858 - lr: 2.4260e-13\n",
      "Epoch 249/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.3012 - val_loss: 0.3858 - lr: 2.4260e-13\n",
      "Epoch 250/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.2981 - val_loss: 0.3858 - lr: 2.4260e-13\n",
      "Epoch 251/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.3020 - val_loss: 0.3858 - lr: 2.4260e-13\n",
      "Epoch 252/300\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2950 - val_loss: 0.3858 - lr: 1.2130e-13\n",
      "Epoch 253/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2933 - val_loss: 0.3858 - lr: 1.2130e-13\n",
      "Epoch 254/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2947 - val_loss: 0.3858 - lr: 1.2130e-13\n",
      "Epoch 255/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.2947 - val_loss: 0.3858 - lr: 1.2130e-13\n",
      "Epoch 256/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.3009 - val_loss: 0.3858 - lr: 1.2130e-13\n",
      "Epoch 257/300\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.3016 - val_loss: 0.3858 - lr: 6.0650e-14\n",
      "Epoch 258/300\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.2970 - val_loss: 0.3858 - lr: 6.0650e-14\n",
      "Epoch 259/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2998 - val_loss: 0.3858 - lr: 6.0650e-14\n",
      "Epoch 260/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.2987 - val_loss: 0.3858 - lr: 6.0650e-14\n",
      "Epoch 261/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.2983 - val_loss: 0.3858 - lr: 6.0650e-14\n",
      "Epoch 262/300\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.2971 - val_loss: 0.3858 - lr: 3.0325e-14\n",
      "Epoch 263/300\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.3004 - val_loss: 0.3858 - lr: 3.0325e-14\n",
      "Epoch 264/300\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.2966 - val_loss: 0.3858 - lr: 3.0325e-14\n",
      "Epoch 265/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2995 - val_loss: 0.3858 - lr: 3.0325e-14\n",
      "Epoch 266/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2966 - val_loss: 0.3858 - lr: 3.0325e-14\n",
      "Epoch 267/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3005 - val_loss: 0.3858 - lr: 1.5162e-14\n",
      "Epoch 268/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2981 - val_loss: 0.3858 - lr: 1.5162e-14\n",
      "Epoch 269/300\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.3005 - val_loss: 0.3858 - lr: 1.5162e-14\n",
      "Epoch 270/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.2982 - val_loss: 0.3858 - lr: 1.5162e-14\n",
      "Epoch 271/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2928 - val_loss: 0.3858 - lr: 1.5162e-14\n",
      "Epoch 272/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.3004 - val_loss: 0.3858 - lr: 7.5812e-15\n",
      "Epoch 273/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2999 - val_loss: 0.3858 - lr: 7.5812e-15\n",
      "Epoch 274/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2978 - val_loss: 0.3858 - lr: 7.5812e-15\n",
      "Epoch 275/300\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.2933 - val_loss: 0.3858 - lr: 7.5812e-15\n",
      "Epoch 276/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.2959 - val_loss: 0.3858 - lr: 7.5812e-15\n",
      "Epoch 277/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2914 - val_loss: 0.3858 - lr: 3.7906e-15\n",
      "Epoch 278/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2918 - val_loss: 0.3858 - lr: 3.7906e-15\n",
      "Epoch 279/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2993 - val_loss: 0.3858 - lr: 3.7906e-15\n",
      "Epoch 280/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3001 - val_loss: 0.3858 - lr: 3.7906e-15\n",
      "Epoch 281/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.2939 - val_loss: 0.3858 - lr: 3.7906e-15\n",
      "Epoch 282/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2979 - val_loss: 0.3858 - lr: 1.8953e-15\n",
      "Epoch 283/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3006 - val_loss: 0.3858 - lr: 1.8953e-15\n",
      "Epoch 284/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.2958 - val_loss: 0.3858 - lr: 1.8953e-15\n",
      "Epoch 285/300\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2971 - val_loss: 0.3858 - lr: 1.8953e-15\n",
      "Epoch 286/300\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2966 - val_loss: 0.3858 - lr: 1.8953e-15\n",
      "Epoch 287/300\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.2975 - val_loss: 0.3858 - lr: 9.4766e-16\n",
      "Epoch 288/300\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.2960 - val_loss: 0.3858 - lr: 9.4766e-16\n",
      "Epoch 289/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2952 - val_loss: 0.3858 - lr: 9.4766e-16\n",
      "Epoch 290/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.2980 - val_loss: 0.3858 - lr: 9.4766e-16\n",
      "Epoch 291/300\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.2971 - val_loss: 0.3858 - lr: 9.4766e-16\n",
      "Epoch 292/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2982 - val_loss: 0.3858 - lr: 4.7383e-16\n",
      "Epoch 293/300\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2921 - val_loss: 0.3858 - lr: 4.7383e-16\n",
      "Epoch 294/300\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.2963 - val_loss: 0.3858 - lr: 4.7383e-16\n",
      "Epoch 295/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2963 - val_loss: 0.3858 - lr: 4.7383e-16\n",
      "Epoch 296/300\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2988 - val_loss: 0.3858 - lr: 4.7383e-16\n",
      "Epoch 297/300\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.2957 - val_loss: 0.3858 - lr: 2.3691e-16\n",
      "Epoch 298/300\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.3015 - val_loss: 0.3858 - lr: 2.3691e-16\n",
      "Epoch 299/300\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.2989 - val_loss: 0.3858 - lr: 2.3691e-16\n",
      "Epoch 300/300\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2979 - val_loss: 0.3858 - lr: 2.3691e-16\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "#from keras import backend\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers import Dropout\n",
    "#from keras.regularizers import l1\n",
    "#from keras.regularizers import l2\n",
    "#from keras.regularizers import l1_l2\n",
    "\n",
    "\n",
    "x_train_vector = ut.df_a_vector(x_train)\n",
    "y_train_vector = ut.df_a_vector(y_train)\n",
    "x_test_vector = ut.df_a_vector(x_test)\n",
    "y_test_vector = ut.df_a_vector(y_test)\n",
    "\n",
    "#input_dim = x_train.shape[1]\n",
    "#\n",
    "#alfa = 1e-3\n",
    "#\n",
    "#model = Sequential()\n",
    "#model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(Dense(128, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "##model.add(Dropout(0.25))\n",
    "##model.add(Dense(256, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='tanh'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(128, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(64, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='tanh'))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(Dense(16, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "#model.add(Dense(8, kernel_regularizer=l1(alfa), bias_regularizer=l1(alfa), activation='relu'))\n",
    "#model.add(Dense(1, activation='tanh'))\n",
    "#\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#\n",
    "#backend.set_value(model.optimizer.learning_rate, 5e-4)\n",
    "\n",
    "best_hparams = ut.hyperparams_from_json('../Archivos/Neuronales')\n",
    "model = md.get_neural_network_model(best_hparams, x_train.shape[1])\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"Neuronales_Mejor_Modelo.hdf5\", \n",
    "                                       monitor='val_loss', \n",
    "                                       verbose=0,\n",
    "                                       save_best_only=True, \n",
    "                                       mode='min'),\n",
    "    \n",
    "    #tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "    #                                 min_delta=0.01,\n",
    "    #                                 mode='min',\n",
    "    #                                 patience=10),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                      mode='min',\n",
    "                                      factor=0.5,\n",
    "                                      patience=5,\n",
    "                                      cooldown=0, \n",
    "                                      min_lr=1e-24)\n",
    "]\n",
    "\n",
    "fit_dict = {\n",
    "    #'x' : x_train_vector,\n",
    "    #'y' : y_train_vector,\n",
    "    #'validation_data' : (x_test_vector, y_test_vector),\n",
    "    'epochs' : epochs,\n",
    "    'batch_size' : batch_size,\n",
    "    'verbose' : 1,\n",
    "    'callbacks' : my_callbacks\n",
    "}\n",
    "\n",
    "\n",
    "history = model.fit(x_train_vector,\n",
    "                    y_train_vector,\n",
    "                    validation_data=(x_test_vector, y_test_vector),\n",
    "                    **fit_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "BoO8syeWoQhd",
    "outputId": "e4737981-19af-4c5f-c69e-f2208387dcff"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Accuracy vs Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_p, y_train_p = ut.split_labels(df_train)\n",
    "x_test_p, y_test_p = ut.split_labels(df_test)\n",
    "\n",
    "#Convertimos las fechas a numeros (cantidad de dias transcurridos) y luego las normalizamos\n",
    "x_train_p, x_test_p = ut.conversion_fechas(x_train_p, x_test_p)\n",
    "x_train_p, x_test_p = ut.codificar_categoricas(x_train_p, y_train_p, x_test_p)\n",
    "x_train_p, x_test_p = ut.normalizacion_numericas(x_train_p, x_test_p, modo='normalizacion')\n",
    "\n",
    "\n",
    "x_train_vector_p = ut.df_a_vector(x_train_p)\n",
    "y_train_vector_p = ut.df_a_vector(y_train_p)\n",
    "x_test_vector_p = ut.df_a_vector(x_test_p)\n",
    "y_test_vector_p = ut.df_a_vector(y_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model('Neuronales_Mejor_Modelo.hdf5')\n",
    "#new_predictions = new_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9483942414174973"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = [a > 0.5 for a in new_model.predict(x_test_vector)]\n",
    "\n",
    "\n",
    "score = accuracy_score(y_pred, y_test_vector)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 0s 3ms/step - loss: 0.3858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3858181834220886"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_vector, y_test_vector)#, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Pruebas TP2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
