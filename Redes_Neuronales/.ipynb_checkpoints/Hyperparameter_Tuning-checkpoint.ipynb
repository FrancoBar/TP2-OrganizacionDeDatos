{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voQ_N8U1su5c"
   },
   "source": [
    "# Tuneo de hiperparametros\n",
    "\n",
    "## Definicion de funciones auxiliares generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zlnCAnO3su5m"
   },
   "outputs": [],
   "source": [
    "#FUENTE: - https://stackoverflow.com/questions/43533610/how-to-use-hyperopt-for-hyperparameter-optimization-of-keras-deep-learning-netwo\n",
    "#        - https://github.com/keras-team/keras/issues/1591\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Utilidades as ut\n",
    "import Modelos as md\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt import space_eval\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pa5XaBdHsu5n"
   },
   "outputs": [],
   "source": [
    "# El Fold es de tamaño fijo, dividiendo por trimestre.\n",
    "\n",
    "def get_quarter(month_series):\n",
    "    quarter_series = ((month_series - 1)//3 + 1)\n",
    "    return quarter_series\n",
    "\n",
    "## ATENCION: Aca se esta asumiendo que se hace un split train-validation a partir del 2018.\n",
    "## Se podria modificar esta funcion como para que chequee que un trimestre tenga al menos X cantidad de filas.\n",
    "## Ademas esta funcion no tiene en cuenta la cantidad de datos con la que se entrena, prioriza dividir por trimestre.\n",
    "\n",
    "def fold_split_quarter(df_train):\n",
    "    folds = list()\n",
    "    years = [2016, 2017]\n",
    "    last_year = years[-1]\n",
    "    # Se puede hacer un segundo for para los trimestres.\n",
    "    for split_year in years:\n",
    "        actual_year = df_train.Opportunity_Created_Date.dt.year\n",
    "        actual_quarter = get_quarter(df_train.Opportunity_Created_Date.dt.month)\n",
    "        \n",
    "        to_Q1 = df_train[(actual_year <= split_year) & (actual_quarter <= 1)]\n",
    "        to_Q2 = df_train[(actual_year <= split_year) & (actual_quarter <= 2)]\n",
    "        to_Q3 = df_train[(actual_year <= split_year) & (actual_quarter <= 3)]\n",
    "        to_Q4 = df_train[(actual_year <= split_year) & (actual_quarter <= 4)]\n",
    "        \n",
    "        Q1 = df_train[(actual_year == split_year) & (actual_quarter == 1)]\n",
    "        Q2 = df_train[(actual_year == split_year) & (actual_quarter == 2)]\n",
    "        Q3 = df_train[(actual_year == split_year) & (actual_quarter == 3)]\n",
    "        Q4 = df_train[(actual_year == split_year) & (actual_quarter == 4)]\n",
    "        Q5 = df_train[(actual_year == (split_year + 1)) & (actual_quarter == 1)]\n",
    "        \n",
    "        folds.append((to_Q1.copy(), Q2.copy()))\n",
    "        folds.append((to_Q2.copy(), Q3.copy()))\n",
    "        folds.append((to_Q3.copy(), Q4.copy()))\n",
    "        if (split_year != last_year):\n",
    "            folds.append((to_Q4.copy(), Q5.copy()))\n",
    "            \n",
    "    return folds\n",
    "\n",
    "def prepare_folds(folds):\n",
    "    \n",
    "    new_folds = list()\n",
    "    \n",
    "    for (df_train, df_test) in folds:\n",
    "        \n",
    "        #Separamos labels del set de entrenamiento y test\n",
    "        df_train_x, df_train_y = ut.split_labels(df_train)\n",
    "        df_test_x, df_test_y = ut.split_labels(df_test)\n",
    "        \n",
    "        #Encoding, conversion de fechas y normalizacion numerica para set de test y train\n",
    "        df_train_x, df_test_x = ut.conversion_fechas(df_train_x, df_test_x)\n",
    "        df_train_x, df_test_x = ut.codificar_categoricas(df_train_x, df_train_y, df_test_x)\n",
    "        df_train_x, df_test_x = ut.normalizacion_numericas(df_train_x, df_test_x, modo='normalizacion')\n",
    "        \n",
    "        #Conversion de los dataframes a vectores.\n",
    "        x_train = ut.df_a_vector(df_train_x)\n",
    "        y_train = ut.df_a_vector(df_train_y)\n",
    "        x_test = ut.df_a_vector(df_test_x)\n",
    "        y_test = ut.df_a_vector(df_test_y)\n",
    "        \n",
    "        new_folds.append((x_train, y_train, x_test, y_test))\n",
    "        \n",
    "    return new_folds\n",
    "\n",
    "\n",
    "def test_model(params, fit_model, folds, last_k_average=5):\n",
    "    \n",
    "    val_loss = list()\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print(\"Comienzo de iteracion con nuevos parametros\")\n",
    "    \n",
    "    for fold in folds:\n",
    "        history = fit_model(fold, params)\n",
    "        val_loss_mean = np.mean(history[-last_k_average:])\n",
    "        print(f\"Resultado parcial: val_loss = {val_loss_mean}\")\n",
    "        val_loss.append(val_loss_mean)\n",
    "        \n",
    "    val_loss_mean = np.mean(val_loss)\n",
    "    print(f\"\\n\\tResultado final: val_loss = {val_loss_mean}\\n\")\n",
    "    return {'loss': val_loss_mean, 'status': STATUS_OK}#, 'model': model}\n",
    "\n",
    "\n",
    "def load_trials(N, model_name):\n",
    "    #Idea obtenida de: https://github.com/hyperopt/hyperopt/issues/267\n",
    "    trials = None\n",
    "    total_iters = N\n",
    "    try:\n",
    "        fd = open(model_name + \"_hyperparams.hopt\", \"rb\")\n",
    "        trials = pickle.load(fd)\n",
    "        fd.close()\n",
    "        print(\"Se encontró un entrenamiento previo. Cargando...\")\n",
    "        total_iters = len(trials.trials) + N\n",
    "        print(\"Comienza el tuneo desde {} hasta {} trials\".format(len(trials.trials), total_iters))\n",
    "    except:  \n",
    "        trials = Trials()\n",
    "    \n",
    "    return trials, total_iters\n",
    "\n",
    "def save_trials(trials, model_name):\n",
    "    # save the trials object\n",
    "    print(f\"Guardando el entrenamiento en '{model_name}_hyperparams.hopt'\")\n",
    "    with open(model_name + \"_hyperparams.hopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos\n",
    "## Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O4VSwvmHsu5o"
   },
   "outputs": [],
   "source": [
    "neural_network_params = {\n",
    "            \n",
    "            'first_layer' : {\n",
    "                'neurons' : hp.uniform('first_layer_neurons', 64, 128),\n",
    "                'activation' : hp.choice('first_layer_activation', ['relu', 'tanh', 'swish']),\n",
    "                'dropout' : hp.uniform('dropout_0', 0.25, 0.5)\n",
    "            },\n",
    "                        \n",
    "            'hidden_layers' : [\n",
    "                {\n",
    "                    'config' : hp.choice('is_on_1', [\n",
    "                        {'is_on' : False }, \n",
    "                        {\n",
    "                            'is_on' : True, \n",
    "                            'neurons' : hp.uniform('neurons_1', 64, 256),\n",
    "                            'activation' : hp.choice('activation_1', ['relu', 'tanh', 'swish']),\n",
    "                            'dropout' : hp.uniform('dropout_1', 0.25, 0.5)\n",
    "                        }\n",
    "                    ]),\n",
    "                },\n",
    "                {\n",
    "                    'config' : hp.choice('is_on_2', [\n",
    "                        {'is_on' : False }, \n",
    "                        {\n",
    "                            'is_on' : True, \n",
    "                            'neurons' : hp.uniform('neurons_2', 64, 256),\n",
    "                            'activation' : hp.choice('activation_2', ['relu', 'tanh', 'swish']),\n",
    "                            'dropout' : hp.uniform('dropout_2', 0.25, 0.5)\n",
    "                        }\n",
    "                    ]),\n",
    "                },\n",
    "                {\n",
    "                    'config' : hp.choice('is_on_3', [\n",
    "                        {'is_on' : False }, \n",
    "                        {\n",
    "                            'is_on' : True, \n",
    "                            'neurons' : hp.uniform('neurons_3', 64, 256),\n",
    "                            'activation' : hp.choice('activation_3', ['relu', 'tanh', 'swish']),\n",
    "                            'dropout' : hp.uniform('dropout_3', 0.25, 0.5)\n",
    "                        }\n",
    "                    ]),\n",
    "                },\n",
    "                {\n",
    "                    'config' : hp.choice('is_on_4', [\n",
    "                        {'is_on' : False }, \n",
    "                        {\n",
    "                            'is_on' : True, \n",
    "                            'neurons' : hp.uniform('neurons_4', 8, 64),\n",
    "                            'activation' : hp.choice('activation_4', ['relu', 'tanh', 'swish']),\n",
    "                            'dropout' : hp.uniform('dropout_4', 0.25, 0.5)\n",
    "                        }\n",
    "                    ]),\n",
    "                }\n",
    "            ],\n",
    "                         \n",
    "            'last_layer' : {\n",
    "                'activation' : hp.choice('last_layer_activation', ['relu', 'tanh', 'swish'])\n",
    "            },\n",
    "\n",
    "            'optimizer': hp.choice('optimizer',['adadelta','adam','rmsprop']),\n",
    "            'learning_rate' : hp.uniform('learning_rate', 1e-4, 1e-2),\n",
    "            'alpha': hp.uniform('alpha', 1e-3, 10)\n",
    "        }\n",
    "\n",
    "\n",
    "neural_callbacks = [\n",
    "    \n",
    "    keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                  min_delta=0.01,\n",
    "                                  mode='min',\n",
    "                                  patience=10),\n",
    "    \n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                      mode='min',\n",
    "                                      factor=0.5,\n",
    "                                      patience=5,\n",
    "                                      cooldown=0, \n",
    "                                      min_lr=1e-8)\n",
    "]\n",
    "\n",
    "def neural_network_fit(fold, params):\n",
    "    (x_train, y_train, x_test, y_test) = fold\n",
    "    input_dim = len(x_train[0])\n",
    "    model = md.get_neural_network_model(params, input_dim)\n",
    "    \n",
    "    history = model.fit(x_train, \n",
    "                        y_train, \n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=neural_callbacks,\n",
    "                        verbose=0,\n",
    "                        epochs=100,\n",
    "                        batch_size=128)\n",
    "    \n",
    "    return history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estos parametros estan sacados de un tuto de internet\n",
    "\n",
    "xgboost_params = {\n",
    "    'max_depth' : hp.choice('max_depth', range(5, 30, 1)),\n",
    "    'learning_rate' : hp.quniform('learning_rate', 0.01, 0.5, 0.01),\n",
    "    'n_estimators' : hp.choice('n_estimators', range(20, 205, 5)),\n",
    "    'gamma' : hp.quniform('gamma', 0, 0.50, 0.01),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample' : hp.quniform('subsample', 0.1, 1, 0.01),\n",
    "    'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1.0, 0.01)\n",
    "}\n",
    "\n",
    "#Estos parametros estan sacados del repo de github\n",
    "\n",
    "#xgboost_params = {\n",
    "#        \"iterations\" : (5, 6),\n",
    "#        'learning_rate': Real(low=0.01, high=1, prior='log-uniform'),\n",
    "#        \"random_seed\" : (1,40000),\n",
    "#        \"l2_leaf_reg\" : Real(low=1e-9, high=1000, prior='log-uniform'),\n",
    "#        'subsample': Real(low=0.01, high=1, prior='uniform'),\n",
    "#        \"random_strength\" : Real(low=1e-9, high=1000, prior='log-uniform'),\n",
    "#        'depth': (1, 5),\n",
    "#        \"early_stopping_rounds\" : (1, 20),\n",
    "#        \"border_count\" : (1,65535)\n",
    "#}\n",
    "\n",
    "\n",
    "def xgboost_fit(fold, params):\n",
    "    \n",
    "    (x_train, y_train, x_test, y_test) = fold\n",
    "    #No se bien como usar las matrices estas...\n",
    "    #train_matrix = xgb.DMatrix(x_train,y_train)\n",
    "    #test_matrix = xgb.DMatrix(x_test,y_test)\n",
    "    model = md.get_xgboost_model(params)\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              eval_set=[(x_train, y_train), (x_test, y_test)],\n",
    "              eval_metric='logloss',\n",
    "              verbose=False)\n",
    "    \n",
    "    evals_result = model.evals_result()\n",
    "    \n",
    "    \n",
    "    return evals_result['validation_1']['logloss']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo principal\n",
    "### Desde este punto comienza el tuneo de hiperparametros del modelo elegido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ys00rd6Rsu5p"
   },
   "outputs": [],
   "source": [
    "#Cargamos el dataframe de training\n",
    "\n",
    "model_name = 'Neuronales'\n",
    "\n",
    "df_train = pd.read_pickle(model_name + \"_train.pkl\")\n",
    "\n",
    "#Armamos los folds\n",
    "folds = fold_split_quarter(df_train)\n",
    "#Aca estamos codificando, transformando fechas y normalizando, quizas no es necesario para XGBoost\n",
    "folds = prepare_folds(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XoVl895nsu5r",
    "outputId": "bfa9fa27-0894-40de-a9da-849bd0312cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontró un entrenamiento previo. Cargando...\n",
      "Comienza el tuneo desde 82 hasta 83 trials\n",
      "--------------------------------------------------     \n",
      "Comienzo de iteracion con nuevos parametros            \n",
      "Resultado parcial: val_loss = 11137.3712890625         \n",
      "Resultado parcial: val_loss = 10820.5650390625         \n",
      " 99%|█████████▉| 82/83 [00:30<?, ?trial/s, best loss=?]\n",
      "ERROR: Modelo Neuronales no reconocido\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b956f9eb493f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0msave_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspace_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_hparams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#Guardamos, si corresponde, los mejores hiperparametros obtenidos hasta el momento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mspace_eval\u001b[0;34m(space, hp_assignment)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hyperopt_param\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhp_assignment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp_assignment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "model_name = 'Neuronales' #Triquiñuela para que me permita optimizar XGBoost con el dataframe de redes neuronales.\n",
    "\n",
    "#Configuracion de funciones para cada modelo\n",
    "\n",
    "models = {\n",
    "    'Neuronales' : {\n",
    "        'model_fit_function' : neural_network_fit,\n",
    "        'model_hparams' : neural_network_params\n",
    "    },\n",
    "    'XGBoost' : {\n",
    "        'model_fit_function' : xgboost_fit,\n",
    "        'model_hparams' : xgboost_params\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "#Parametros generales\n",
    "\n",
    "#epochs = 20               #Numero de iteraciones de entrenamiento para cada modelo\n",
    "last_k_avg = 5             #Ultimos k valores de val_loss de cada entrenamiento a promediar\n",
    "N = 1                     #Numero de iteraciones del algoritmo de tuneo.\n",
    "best = None                #Donde se almacena informacion del mejor resultado del tuneo\n",
    "save_best_hparams = True   #Sobrescribe el archivo de 'best_hyperparams_<model_name>.json' con el mejor de Trial\n",
    "error_found = False\n",
    "\n",
    "trials, total_iters = load_trials(N, model_name)\n",
    "\n",
    "try:\n",
    "\n",
    "    callback = lambda params: test_model(params, \n",
    "                                         models[model_name]['model_fit_function'], \n",
    "                                         folds, \n",
    "                                         last_k_average=last_k_avg)\n",
    "    best = fmin(callback, \n",
    "                models[model_name]['model_hparams'], \n",
    "                algo=tpe.suggest, \n",
    "                max_evals=total_iters, \n",
    "                trials=trials)\n",
    "except:\n",
    "    print(f\"ERROR: Modelo {model_name} no reconocido\")\n",
    "    #No se como finalizar la ejecucion de la celda\n",
    "    error_found = True\n",
    "\n",
    "#Guardamos la informacion del tuneo para poder continuar en otro momento\n",
    "if (not error_found):\n",
    "    save_trials(trials, model_name)\n",
    "\n",
    "best_params = space_eval(models[model_name]['model_hparams'], best)\n",
    "\n",
    "#Guardamos, si corresponde, los mejores hiperparametros obtenidos hasta el momento\n",
    "if (save_best_hparams and not error_found):\n",
    "    ut.hyperparams_to_json(best_params, model_name)\n",
    "\n",
    "print(\"#\"*50)\n",
    "print(\"\\nEl mejor modelo hasta el momento contiene los siguientes parametros:\\n\")\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comentarios utiles\n",
    "- En caso de haber perdido los parametros del mejor modelo, se los puede recuperar a partir de la variable 'best'.\n",
    "- Si se perdio el resultado de la variable 'best' sera necesario entonces ejecutar la funcion fmin de hyperopt, se lo puede hacer con N=1 para que termine rapido.\n",
    "- Por default hyperopt devuelve un diccionario con los mejores hiperparametros encontrados, donde la key es el label del hiperparametro y el value es, para los hiperparametros con 'hp.choice', el indice a la posicion en el vector de hp.choice definido. Para que devuelva directamente el valor del hiperparametro sera necesario utilizar la funcion 'space_eval', que se encuentra en un ejemplo en la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6ZP3blHFrYJ"
   },
   "outputs": [],
   "source": [
    "#Ejemplo de como se obtienen los mejores parametros y de como se obtiene un objecto model a partir de ellos.\n",
    "#best_parameters = space_eval(neural_network_params, best)\n",
    "#best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo de como obtener el modelo ya creado con los mejores hiperparametros\n",
    "#Dejo comentado esto para que no rompa en caso de utilizar un modelo distinto al de redes neuronales.\n",
    "\n",
    "#best_model = get_neural_network_model(best_parameters, (df_train.shape[1] - 1))\n",
    "\n",
    "#A partir de aca ya se puede usar el modelo, pero tener cuidado que el mismo no se encuentra entrenado."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Hyperparameter_Tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
