{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voQ_N8U1su5c"
   },
   "source": [
    "# Tuneo de hiperparametros\n",
    "\n",
    "## Definicion de funciones auxiliares generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IO1_4lAkwu47",
    "outputId": "a3147aca-cca9-4ee6-fa93-28ac5c6a28ae"
   },
   "outputs": [],
   "source": [
    "#Si se corre en Colab\n",
    "#!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zlnCAnO3su5m"
   },
   "outputs": [],
   "source": [
    "#FUENTE: - https://stackoverflow.com/questions/43533610/how-to-use-hyperopt-for-hyperparameter-optimization-of-keras-deep-learning-netwo\n",
    "#        - https://github.com/keras-team/keras/issues/1591\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import isclose\n",
    "import Utilidades as ut\n",
    "import Modelos as md\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt import space_eval\n",
    "from sklearn.metrics import log_loss\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pa5XaBdHsu5n"
   },
   "outputs": [],
   "source": [
    "# El Fold es de tamaño fijo, dividiendo por trimestre.\n",
    "\n",
    "def get_quarter(month_series):\n",
    "    quarter_series = ((month_series - 1)//3 + 1)\n",
    "    return quarter_series\n",
    "\n",
    "## ATENCION: Aca se esta asumiendo que se hace un split train-validation a partir del 2018.\n",
    "## Se podria modificar esta funcion como para que chequee que un trimestre tenga al menos X cantidad de filas.\n",
    "## Ademas esta funcion no tiene en cuenta la cantidad de datos con la que se entrena, prioriza dividir por trimestre.\n",
    "\n",
    "def fold_split_quarter(df_train):\n",
    "    folds = list()\n",
    "    years = [2016, 2017]\n",
    "    last_year = years[-1]\n",
    "    \n",
    "    actual_year = df_train.Opportunity_Created_Date.dt.year\n",
    "    actual_quarter = get_quarter(df_train.Opportunity_Created_Date.dt.month)\n",
    "    \n",
    "    ##Esto de abajo se tiene que hacer si no se desea utilizar 'Opportunity_Created_Date'.\n",
    "    \n",
    "    df_train = df_train.drop(columns = ['Opportunity_Created_Date'])\n",
    "    \n",
    "    ########################\n",
    "    # Se puede hacer un segundo for para los trimestres.\n",
    "    for split_year in years:\n",
    "        \n",
    "        to_Q1 = df_train[(actual_year <= split_year) & (actual_quarter <= 1)]\n",
    "        to_Q2 = df_train[(actual_year <= split_year) & (actual_quarter <= 2)]\n",
    "        to_Q3 = df_train[(actual_year <= split_year) & (actual_quarter <= 3)]\n",
    "        to_Q4 = df_train[(actual_year <= split_year) & (actual_quarter <= 4)]\n",
    "        \n",
    "        Q1 = df_train[(actual_year == split_year) & (actual_quarter == 1)]\n",
    "        Q2 = df_train[(actual_year == split_year) & (actual_quarter == 2)]\n",
    "        Q3 = df_train[(actual_year == split_year) & (actual_quarter == 3)]\n",
    "        Q4 = df_train[(actual_year == split_year) & (actual_quarter == 4)]\n",
    "        Q5 = df_train[(actual_year == (split_year + 1)) & (actual_quarter == 1)]\n",
    "        \n",
    "        folds.append((to_Q1.copy(), Q2.copy()))\n",
    "        folds.append((to_Q2.copy(), Q3.copy()))\n",
    "        folds.append((to_Q3.copy(), Q4.copy()))\n",
    "        if (split_year != last_year):\n",
    "            folds.append((to_Q4.copy(), Q5.copy()))\n",
    "            \n",
    "    return folds\n",
    "\n",
    "def prepare_folds(folds):\n",
    "    \n",
    "    new_folds = list()\n",
    "    \n",
    "    for (df_train, df_test) in folds:\n",
    "        \n",
    "        #Separamos labels del set de entrenamiento y test\n",
    "        df_train_x, df_train_y = ut.split_labels(df_train)\n",
    "        df_test_x, df_test_y = ut.split_labels(df_test)\n",
    "        \n",
    "        #Encoding, conversion de fechas y normalizacion numerica para set de test y train\n",
    "        df_train_x, df_test_x = ut.conversion_fechas(df_train_x, df_test_x)\n",
    "        df_train_x, df_test_x = ut.codificar_categoricas(df_train_x, df_train_y, df_test_x, modo='catboost')\n",
    "        df_train_x, df_test_x = ut.normalizacion_numericas(df_train_x, df_test_x, modo='normalizacion')\n",
    "        \n",
    "        #Conversion de los dataframes a vectores.\n",
    "        x_train = ut.df_a_vector(df_train_x)\n",
    "        y_train = ut.df_a_vector(df_train_y)\n",
    "        x_test = ut.df_a_vector(df_test_x)\n",
    "        y_test = ut.df_a_vector(df_test_y)\n",
    "        \n",
    "        new_folds.append((x_train, y_train, x_test, y_test))\n",
    "        \n",
    "    return new_folds\n",
    "\n",
    "\n",
    "def spaces_equal(space_1, space_2):\n",
    "    equal = True\n",
    "    rel_tol = 0.01 # 1%\n",
    "    \n",
    "    if len(space_1) != len(space_2):\n",
    "        return False\n",
    "    \n",
    "    #Varios\n",
    "    equal = (equal and (isclose(space_1['learning_rate'], space_2['learning_rate'], rel_tol=rel_tol)))\n",
    "    equal = (equal and (isclose(space_1['alpha'], space_2['alpha'], rel_tol=rel_tol)))\n",
    "    equal = (equal and (space_1['optimizer'] == space_2['optimizer']))\n",
    "    \n",
    "    #First layer\n",
    "    equal = (equal and (isclose(space_1['first_layer']['neurons'], space_2['first_layer']['neurons'], rel_tol=rel_tol)))\n",
    "    equal = (equal and (isclose(space_1['first_layer']['dropout'], space_2['first_layer']['dropout'], rel_tol=rel_tol)))\n",
    "    equal = (equal and (space_1['first_layer']['activation'] == space_2['first_layer']['activation']))\n",
    "    \n",
    "    #Hidden layers\n",
    "    \n",
    "    if len(space_1['hidden_layers']) != len(space_2['hidden_layers']):\n",
    "        return False\n",
    "    \n",
    "    if not equal: return False\n",
    "    \n",
    "    hidden_layers_2 = space_2['hidden_layers']\n",
    "    for idx, layer_1 in enumerate(space_1['hidden_layers']):\n",
    "        if layer_1['config']['is_on'] != hidden_layers_2[idx]['config']['is_on']: return False\n",
    "        if layer_1['config']['is_on']:\n",
    "            equal = (equal and (isclose(layer_1['config']['neurons'], hidden_layers_2[idx]['config']['neurons'], rel_tol=rel_tol)))\n",
    "            equal = (equal and (isclose(layer_1['config']['dropout'], hidden_layers_2[idx]['config']['dropout'], rel_tol=rel_tol)))\n",
    "            equal = (equal and (layer_1['config']['activation'] == hidden_layers_2[idx]['config']['activation']))\n",
    "    \n",
    "    return equal\n",
    "\n",
    "best_loss = [np.inf]\n",
    "\n",
    "def test_model(params, fit_model, folds, space, factor_poda=3, last_k_average=5):\n",
    "\n",
    "    val_loss = list()\n",
    "    #weights = np.array([0.1, 0.1, 0.1, 0.15, 0.15, 0.2, 0.2])\n",
    "    weights = np.array([0.05, 0.05, 0.05, 0.10, 0.2, 0.25, 0.3])\n",
    "    weights = weights * len(weights)\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print(\"Comienzo de iteracion con nuevos parametros\")\n",
    "    print(params)\n",
    "    #Optimizacion extraida de: https://github.com/hyperopt/hyperopt/issues/370\n",
    "    if len(trials.trials)>1:\n",
    "        for x in trials.trials[:-1]:\n",
    "            space_point_index = dict([(key,value[0]) for key,value in x['misc']['vals'].items() if len(value)>0])\n",
    "            if spaces_equal(params, space_eval(space,space_point_index)):\n",
    "                print(\"Se omite la serie de parametros por repeticion\\n\")\n",
    "                loss = x['result']['loss']\n",
    "                return {'loss': loss, 'status': STATUS_OK}\n",
    "    ###\n",
    "    for i, fold in enumerate(folds):\n",
    "        history = fit_model(fold, params)\n",
    "        last_val_loss = np.mean(history[-last_k_average:])\n",
    "        val_loss.append(last_val_loss*weights[i])\n",
    "        val_loss_mean = np.mean(val_loss)\n",
    "        print(f\"Resultado parcial: val_loss = {last_val_loss}\")\n",
    "        if val_loss_mean >= factor_poda*best_loss[0]:\n",
    "            print(\"Se omite serie de parametos por bajo rendimiento\\n\")\n",
    "            print(f\"\\tResultado final (media ponderada): val_loss = {val_loss_mean}\\n\")\n",
    "            return {'loss': val_loss_mean, 'status': STATUS_OK}\n",
    "        \n",
    "    print(f\"\\n\\tResultado final (media ponderada): val_loss = {val_loss_mean}\\n\")\n",
    "\n",
    "    best_loss[0] = min(best_loss[0], val_loss_mean)\n",
    "\n",
    "    return {'loss': val_loss_mean, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def load_trials(N, model_name):\n",
    "    #Idea obtenida de: https://github.com/hyperopt/hyperopt/issues/267\n",
    "    trials = None\n",
    "    total_iters = N\n",
    "    try:\n",
    "        fd = open('Archivos/' + model_name + \"_hyperparams.hopt\", \"rb\")\n",
    "        trials = pickle.load(fd)\n",
    "        fd.close()\n",
    "        print(\"Se encontró un entrenamiento previo. Cargando...\")\n",
    "        total_iters = len(trials.trials) + N\n",
    "        print(\"Comienza el tuneo desde {} hasta {} trials\".format(len(trials.trials), total_iters))\n",
    "    except:  \n",
    "        trials = Trials()\n",
    "    \n",
    "    return trials, total_iters\n",
    "\n",
    "def save_trials(trials, model_name):\n",
    "    # save the trials object\n",
    "    print(f\"Guardando el entrenamiento en Archivos/'{model_name}_hyperparams.hopt'\")\n",
    "    with open('Archivos/' + model_name + \"_hyperparams.hopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRk7CP5KwrUc"
   },
   "source": [
    "# Modelos\n",
    "## Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O4VSwvmHsu5o"
   },
   "outputs": [],
   "source": [
    "#Espacio de busqueda para parametros continuos -> Random o Bayes\n",
    "\n",
    "neural_network_params = {\n",
    "            \n",
    "            'first_layer' : {\n",
    "                'neurons' : hp.quniform('first_layer_neurons', 40, 100, 5),\n",
    "                'activation' : hp.choice('first_layer_activation', ['relu', 'tanh', 'swish']),\n",
    "                'dropout' : 0.2\n",
    "            },\n",
    "                        \n",
    "            'hidden_layers' : [\n",
    "                {\n",
    "                    'config' : hp.choice('is_on_1', [\n",
    "                        {'is_on' : False }, \n",
    "                        {\n",
    "                            'is_on' : True, \n",
    "                            'neurons' : hp.quniform('neurons_1', 80, 160, 10),\n",
    "                            'activation' : hp.choice('activation_1', ['relu', 'tanh', 'swish']),\n",
    "                            'dropout' : 0.4\n",
    "                        }\n",
    "                    ]),\n",
    "                }#,\n",
    "                #{\n",
    "                #    'config' : hp.choice('is_on_2', [\n",
    "                #        {'is_on' : False }, \n",
    "                #        {\n",
    "                #            'is_on' : True, \n",
    "                #            'neurons' : hp.quniform('neurons_2', 80, 160, 10),\n",
    "                #            'activation' : hp.choice('activation_2', ['relu', 'tanh', 'swish']),\n",
    "                #            'dropout' : 0.4\n",
    "                #        }\n",
    "                #    ]),\n",
    "                #}\n",
    "            ],\n",
    "                         \n",
    "            'last_layer' : {\n",
    "                'activation' : 'sigmoid'\n",
    "            },\n",
    "\n",
    "            'optimizer': hp.choice('optimizer',['adadelta','adam','rmsprop']),\n",
    "            'learning_rate' : 1e-3,\n",
    "            'alpha': hp.quniform('alpha', 0.0005, 0.01, 0.0005)\n",
    "        }\n",
    "\n",
    "\n",
    "####################PRUEBA###################333\n",
    "\n",
    "\n",
    "# neural_network_params = {\n",
    "            \n",
    "#             'first_layer' : {\n",
    "#                 'neurons' : hp.quniform('first_layer_neurons', 100, 101, 1),\n",
    "#                 'activation' : 'relu',\n",
    "#                 'dropout' : 0.5\n",
    "#             },\n",
    "                        \n",
    "#             'hidden_layers' : [\n",
    "#                 {\n",
    "#                     'config' : hp.choice('is_on_1', [\n",
    "#                         {'is_on' : False }, \n",
    "#                         {\n",
    "#                             'is_on' : True, \n",
    "#                             'neurons' : hp.quniform('neurons_1', 200, 205, 5),\n",
    "#                             'activation' : 'relu',\n",
    "#                             'dropout' : 0.5\n",
    "#                         }\n",
    "#                     ]),\n",
    "#                 }\n",
    "#             ],\n",
    "                         \n",
    "#             'last_layer' : {\n",
    "#                 #my_relu es una funcion de activacion definida en Modulo.py que tiene como cota superior 1\n",
    "#                 'activation' : 'sigmoid'\n",
    "#             },\n",
    "\n",
    "#             'optimizer': 'adam',\n",
    "#             'learning_rate' : 5e-4,\n",
    "#             'alpha': 1e-3\n",
    "#         }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "\n",
    "\n",
    "#Prueba con una red con el top 5 features\n",
    "#neural_network_params = {\n",
    "#            \n",
    "#            'first_layer' : {\n",
    "#                'neurons' : hp.uniform('first_layer_neurons', 16, 128),\n",
    "#                'activation' : hp.choice('first_layer_activation', ['relu', 'tanh', 'swish']),\n",
    "#                'dropout' : hp.uniform('dropout_0', 0.25, 0.5)\n",
    "#            },\n",
    "#                        \n",
    "#            'hidden_layers' : [\n",
    "#                {\n",
    "#                    'config' : hp.choice('is_on_1', [\n",
    "#                        {'is_on' : False }, \n",
    "#                        {\n",
    "#                            'is_on' : True, \n",
    "#                            'neurons' : hp.uniform('neurons_1', 8, 256),\n",
    "#                            'activation' : hp.choice('activation_1', ['relu', 'tanh', 'swish']),\n",
    "#                            'dropout' : hp.uniform('dropout_1', 0.25, 0.5)\n",
    "#                        }\n",
    "#                    ]),\n",
    "#                },\n",
    "#                {\n",
    "#                    'config' : hp.choice('is_on_4', [\n",
    "#                        {'is_on' : False }, \n",
    "#                        {\n",
    "#                            'is_on' : True, \n",
    "#                            'neurons' : hp.uniform('neurons_4', 8, 256),\n",
    "#                            'activation' : hp.choice('activation_4', ['relu', 'tanh', 'swish']),\n",
    "#                            'dropout' : hp.uniform('dropout_4', 0.25, 0.5)\n",
    "#                        }\n",
    "#                    ]),\n",
    "#                }\n",
    "#            ],\n",
    "#                         \n",
    "#            'last_layer' : {\n",
    "#                #my_relu es una funcion de activacion definida en Modulo.py que tiene como cota superior 1\n",
    "#                'activation' : hp.choice('last_layer_activation', ['my_relu', 'sigmoid'])\n",
    "#            },\n",
    "#\n",
    "#            'optimizer': hp.choice('optimizer',['adadelta','adam','rmsprop']),\n",
    "#            'learning_rate' : hp.uniform('learning_rate', 1e-4, 1e-2),\n",
    "#            'alpha': hp.uniform('alpha', 1e-3, 10)\n",
    "#        }\n",
    "\n",
    "#Espacio de busqueda para hiperparametros discretos -> GridSearch\n",
    "\n",
    "#neural_network_grid_search_params = {\n",
    "#            \n",
    "#            'first_layer' : {\n",
    "#                'neurons' : hp.quniform('first_layer_neurons', 40, 130, 10),\n",
    "#                'activation' : hp.choice('first_layer_activation', ['relu', 'tanh', 'swish']),\n",
    "#                'dropout' : hp.quniform('dropout_0', 0.3, 0.4, 0.1)\n",
    "#            },\n",
    "#                        \n",
    "#            'hidden_layers' : [\n",
    "#                {\n",
    "#                    'config' : hp.choice('is_on_1', [\n",
    "#                        {'is_on' : False }, \n",
    "#                        {\n",
    "#                            'is_on' : True, \n",
    "#                            'neurons' : hp.uniform('neurons_1', 40, 160, 20),\n",
    "#                            'activation' : hp.choice('activation_1', ['relu', 'tanh', 'swish']),\n",
    "#                            'dropout' : hp.uniform('dropout_1', 0.3, 0.4, 0.1)\n",
    "#                        }\n",
    "#                    ]),\n",
    "#                },\n",
    "#                {\n",
    "#                    'config' : hp.choice('is_on_4', [\n",
    "#                        {'is_on' : False }, \n",
    "#                        {\n",
    "#                            'is_on' : True, \n",
    "#                            'neurons' : hp.uniform('neurons_4', 40, 160, 20),\n",
    "#                            'activation' : hp.choice('activation_4', ['relu', 'tanh', 'swish']),\n",
    "#                            'dropout' : hp.uniform('dropout_4', 0.3, 0.4, 0.1)\n",
    "#                        }\n",
    "#                    ]),\n",
    "#                }\n",
    "#            ],\n",
    "#                         \n",
    "#            'last_layer' : {\n",
    "#                #my_relu es una funcion de activacion definida en Modulo.py que tiene como cota superior 1\n",
    "#                'activation' : hp.choice('last_layer_activation', ['sigmoid'])\n",
    "#            },\n",
    "#\n",
    "#            'optimizer': hp.choice('optimizer',['adadelta','adam','rmsprop']),\n",
    "#            'learning_rate' : hp.quniform('learning_rate', 0.0015),\n",
    "#            'alpha': hp.quniform('alpha', 0.01, 0.1, 0.01)\n",
    "#        }\n",
    "\n",
    "\n",
    "neural_callbacks = [\n",
    "                    \n",
    "    # keras.callbacks.ModelCheckpoint(\"hpam_aux.hdf5\", \n",
    "    #                                  monitor='val_loss', \n",
    "    #                                  verbose=0,\n",
    "    #                                  save_best_only=True, \n",
    "    #                                  mode='min'),\n",
    "    \n",
    "    keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                  min_delta=0.0001,\n",
    "                                  mode='min',\n",
    "                                  patience=10),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                      mode='min',\n",
    "                                      factor=0.5,\n",
    "                                      min_delta=0.0001,\n",
    "                                      patience=2,\n",
    "                                      cooldown=0, \n",
    "                                      min_lr=1e-24)\n",
    "]\n",
    "\n",
    "def neural_network_fit(fold, params):\n",
    "    (x_train, y_train, x_test, y_test) = fold\n",
    "    input_dim = len(x_train[0])\n",
    "    model = md.get_neural_network_model(params, input_dim)\n",
    "    \n",
    "    history = model.fit(x_train, \n",
    "                        y_train, \n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=neural_callbacks,\n",
    "                        verbose=0,\n",
    "                        epochs=150,\n",
    "                        batch_size=256)\n",
    "\n",
    "    #new_model = keras.models.load_model('hpam_aux.hdf5')\n",
    "    y_pred_proba = model.predict(x_test).flatten()\n",
    "    score = log_loss(y_test.flatten(), y_pred_proba, eps=1e-7)\n",
    "\n",
    "    return [score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsKl87E0wrUf"
   },
   "source": [
    "## XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nN8MbITmwrUg"
   },
   "outputs": [],
   "source": [
    "#Estos parametros estan sacados de un tuto de internet\n",
    "\n",
    "xgboost_params = {\n",
    "    'max_depth' : hp.choice('max_depth', range(5, 30, 1)),\n",
    "    'learning_rate' : hp.quniform('learning_rate', 0.01, 0.5, 0.01),\n",
    "    'n_estimators' : hp.choice('n_estimators', range(20, 205, 5)),\n",
    "    'gamma' : hp.quniform('gamma', 0, 0.50, 0.01),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample' : hp.quniform('subsample', 0.1, 1, 0.01),\n",
    "    'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1.0, 0.01)\n",
    "}\n",
    "\n",
    "#Estos parametros estan sacados del repo de github\n",
    "\n",
    "#xgboost_params = {\n",
    "#        \"iterations\" : (5, 6),\n",
    "#        'learning_rate': Real(low=0.01, high=1, prior='log-uniform'),\n",
    "#        \"random_seed\" : (1,40000),\n",
    "#        \"l2_leaf_reg\" : Real(low=1e-9, high=1000, prior='log-uniform'),\n",
    "#        'subsample': Real(low=0.01, high=1, prior='uniform'),\n",
    "#        \"random_strength\" : Real(low=1e-9, high=1000, prior='log-uniform'),\n",
    "#        'depth': (1, 5),\n",
    "#        \"early_stopping_rounds\" : (1, 20),\n",
    "#        \"border_count\" : (1,65535)\n",
    "#}\n",
    "\n",
    "\n",
    "def xgboost_fit(fold, params):\n",
    "    \n",
    "    (x_train, y_train, x_test, y_test) = fold\n",
    "    #No se bien como usar las matrices estas...\n",
    "    #train_matrix = xgb.DMatrix(x_train,y_train)\n",
    "    #test_matrix = xgb.DMatrix(x_test,y_test)\n",
    "    model = md.get_xgboost_model(params)\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              eval_set=[(x_train, y_train), (x_test, y_test)],\n",
    "              eval_metric='logloss',\n",
    "              verbose=False)\n",
    "    \n",
    "    evals_result = model.evals_result()\n",
    "    \n",
    "    \n",
    "    return evals_result['validation_1']['logloss']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgqYFJVIwrUh"
   },
   "source": [
    "# Codigo principal\n",
    "### Desde este punto comienza el tuneo de hiperparametros del modelo elegido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ys00rd6Rsu5p",
    "outputId": "92d68cdd-8a92-499c-fd39-ad77a99a4408"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/home/julian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/home/julian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/home/julian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/home/julian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/home/julian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/home/julian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "#Cargamos el dataframe de training\n",
    "\n",
    "model_name = 'Neuronales'\n",
    "\n",
    "df_train = pd.read_pickle('Archivos/' + model_name + \"_entrenamiento.pkl\")\n",
    "\n",
    "#Armamos los folds\n",
    "folds = fold_split_quarter(df_train)\n",
    "#Aca estamos codificando, transformando fechas y normalizando, quizas no es necesario para XGBoost\n",
    "folds = prepare_folds(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeihbB9T3aSP",
    "outputId": "89f9b547-9acf-4fa2-8609-6395140fa5ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataframe tiene 85 columnas\n"
     ]
    }
   ],
   "source": [
    "cols = int(folds[0][0].size / len(folds[0][0]))\n",
    "print(f\"El dataframe tiene {cols} columnas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XoVl895nsu5r",
    "outputId": "e4608c6b-0887-45e5-cc70-7d66eb90964c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------   \n",
      "Comienzo de iteracion con nuevos parametros          \n",
      "{'alpha': 0.002, 'first_layer': {'activation': 'relu', 'dropout': 0.2, 'neurons': 55.0}, 'hidden_layers': ({'config': {'is_on': False}},), 'last_layer': {'activation': 'sigmoid'}, 'learning_rate': 0.001, 'optimizer': 'adadelta'}\n",
      "Resultado parcial: val_loss = 0.9493907559177533     \n",
      "Resultado parcial: val_loss = 0.7904856094539585     \n",
      "Resultado parcial: val_loss = 0.7955843875448775     \n",
      "Resultado parcial: val_loss = 0.8345521121989636     \n",
      "Resultado parcial: val_loss = 0.7745430511863697     \n",
      "Resultado parcial: val_loss = 0.613106583083372      \n",
      "Resultado parcial: val_loss = 0.6282036206115772     \n",
      "                                                     \n",
      "\tResultado final (media ponderada): val_loss = 0.706874591057316\n",
      "\n",
      "--------------------------------------------------                             \n",
      "Comienzo de iteracion con nuevos parametros                                    \n",
      "{'alpha': 0.007, 'first_layer': {'activation': 'swish', 'dropout': 0.2, 'neurons': 95.0}, 'hidden_layers': ({'config': {'is_on': False}},), 'last_layer': {'activation': 'sigmoid'}, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "Resultado parcial: val_loss = 0.4726283378431229                               \n",
      "Resultado parcial: val_loss = 0.9659942401909425                               \n",
      "Resultado parcial: val_loss = 0.7367829601041118                               \n",
      "Resultado parcial: val_loss = 0.4569719027309919                               \n",
      "Resultado parcial: val_loss = 0.5233199106670151                               \n",
      "Resultado parcial: val_loss = 0.8153545271971578                               \n",
      "Resultado parcial: val_loss = 0.6523159959436253                               \n",
      "                                                                               \n",
      "\tResultado final (media ponderada): val_loss = 0.6586648798957881\n",
      "\n",
      "--------------------------------------------------                             \n",
      "Comienzo de iteracion con nuevos parametros                                    \n",
      "{'alpha': 0.0025, 'first_layer': {'activation': 'relu', 'dropout': 0.2, 'neurons': 90.0}, 'hidden_layers': ({'config': {'activation': 'tanh', 'dropout': 0.4, 'is_on': True, 'neurons': 120.0}},), 'last_layer': {'activation': 'sigmoid'}, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "Resultado parcial: val_loss = 0.3302567919048092                               \n",
      "Resultado parcial: val_loss = 0.8774909171361525                               \n",
      "Resultado parcial: val_loss = 0.6148414314847822                               \n",
      "Resultado parcial: val_loss = 0.4708397270351728                               \n",
      "Resultado parcial: val_loss = 0.5541234077260503                               \n",
      "Resultado parcial: val_loss = 0.99676690758314                                 \n",
      "Resultado parcial: val_loss = 0.8307256305984015                               \n",
      "                                                                               \n",
      "\tResultado final (media ponderada): val_loss = 0.74744752735032\n",
      "\n",
      "--------------------------------------------------                             \n",
      "Comienzo de iteracion con nuevos parametros                                    \n",
      "{'alpha': 0.0085, 'first_layer': {'activation': 'relu', 'dropout': 0.2, 'neurons': 90.0}, 'hidden_layers': ({'config': {'is_on': False}},), 'last_layer': {'activation': 'sigmoid'}, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "Resultado parcial: val_loss = 0.45825328347074545                              \n",
      "Resultado parcial: val_loss = 0.9565223351765566                               \n",
      "Resultado parcial: val_loss = 0.7338328727716101                               \n",
      "Resultado parcial: val_loss = 0.4608326417395698                               \n",
      "Resultado parcial: val_loss = 0.5564451953797269                               \n",
      "Resultado parcial: val_loss = 0.9753189905515933                               \n",
      "Resultado parcial: val_loss = 0.708773998711336                                \n",
      "                                                                               \n",
      "\tResultado final (media ponderada): val_loss = 0.7212646750721471\n",
      "\n",
      "--------------------------------------------------                             \n",
      "Comienzo de iteracion con nuevos parametros                                    \n",
      "{'alpha': 0.0015, 'first_layer': {'activation': 'relu', 'dropout': 0.2, 'neurons': 100.0}, 'hidden_layers': ({'config': {'activation': 'swish', 'dropout': 0.4, 'is_on': True, 'neurons': 80.0}},), 'last_layer': {'activation': 'sigmoid'}, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "Resultado parcial: val_loss = 0.33761323391361275                              \n",
      "Resultado parcial: val_loss = 0.8745127633732522                               \n",
      "Resultado parcial: val_loss = 0.6302486738306413                               \n",
      "Resultado parcial: val_loss = 0.4736651796466677                               \n",
      "Resultado parcial: val_loss = 0.6353249249985594                               \n",
      "Resultado parcial: val_loss = 1.0130790761913069                               \n",
      "Resultado parcial: val_loss = 0.972400532316913                                \n",
      "                                                                               \n",
      "\tResultado final (media ponderada): val_loss = 0.8115401652631545\n",
      "\n",
      "100%|██████████| 5/5 [04:55<00:00, 59.02s/trial, best loss: 0.6586648798957881]\n",
      "Guardando el entrenamiento en Archivos/'Neuronales_hyperparams.hopt'\n",
      "Guardando hiperparametros en el archivo: 'Archivos/Neuronales_best_hyperparam.json'\n",
      "##################################################\n",
      "\n",
      "El mejor modelo hasta el momento contiene los siguientes parametros:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.007,\n",
       " 'first_layer': {'activation': 'swish', 'dropout': 0.2, 'neurons': 95.0},\n",
       " 'hidden_layers': ({'config': {'is_on': False}},),\n",
       " 'last_layer': {'activation': 'sigmoid'},\n",
       " 'learning_rate': 0.001,\n",
       " 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Configuracion de funciones para cada modelo\n",
    "\n",
    "models = {\n",
    "    'Neuronales' : {\n",
    "        'model_fit_function' : neural_network_fit,\n",
    "        'model_hparams' : neural_network_params\n",
    "    },\n",
    "    'XGBoost' : {\n",
    "        'model_fit_function' : xgboost_fit,\n",
    "        'model_hparams' : xgboost_params\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "#Parametros generales\n",
    "\n",
    "#epochs = 20               #Numero de iteraciones de entrenamiento para cada modelo\n",
    "last_k_avg = 3             #Ultimos k valores de val_loss de cada entrenamiento a promediar\n",
    "N = 5                      #Numero de iteraciones del algoritmo de tuneo.\n",
    "best = None                #Donde se almacena informacion del mejor resultado del tuneo\n",
    "save_best_hparams = True   #Sobrescribe el archivo de 'best_hyperparams_<model_name>.json' con el mejor de Trial\n",
    "error_found = False\n",
    "\n",
    "trials, total_iters = load_trials(N, model_name)\n",
    "\n",
    "best_loss[0] = np.inf\n",
    "for a in trials.results:\n",
    "  if a['loss'] < best_loss[0]:\n",
    "    best_loss[0] = a['loss']\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    callback = lambda params: test_model(params, \n",
    "                                         models[model_name]['model_fit_function'], \n",
    "                                         folds,\n",
    "                                         models[model_name]['model_hparams'],\n",
    "                                         last_k_average=last_k_avg)\n",
    "    best = fmin(callback, \n",
    "                models[model_name]['model_hparams'], \n",
    "                algo=tpe.suggest, \n",
    "                max_evals=total_iters, \n",
    "                trials=trials)\n",
    "except Exception as e:\n",
    "    #print(f\"ERROR: Modelo {model_name} no reconocido\")\n",
    "    print(\"ERROR: {}\".format(str(e)))\n",
    "    #No se como finalizar la ejecucion de la celda\n",
    "    error_found = True\n",
    "\n",
    "#Guardamos la informacion del tuneo para poder continuar en otro momento\n",
    "if (not error_found):\n",
    "    save_trials(trials, model_name)\n",
    "\n",
    "best_params = space_eval(models[model_name]['model_hparams'], best)\n",
    "\n",
    "#Guardamos, si corresponde, los mejores hiperparametros obtenidos hasta el momento\n",
    "if (save_best_hparams and not error_found):\n",
    "    ut.hyperparams_to_json(best_params, 'Archivos/' + model_name)\n",
    "\n",
    "print(\"#\"*50)\n",
    "print(\"\\nEl mejor modelo hasta el momento contiene los siguientes parametros:\\n\")\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyngLisXwrUk"
   },
   "source": [
    "## Comentarios utiles\n",
    "- En caso de haber perdido los parametros del mejor modelo, se los puede recuperar a partir de la variable 'best'.\n",
    "- Si se perdio el resultado de la variable 'best' sera necesario entonces ejecutar la funcion fmin de hyperopt, se lo puede hacer con N=1 para que termine rapido.\n",
    "- Por default hyperopt devuelve un diccionario con los mejores hiperparametros encontrados, donde la key es el label del hiperparametro y el value es, para los hiperparametros con 'hp.choice', el indice a la posicion en el vector de hp.choice definido. Para que devuelva directamente el valor del hiperparametro sera necesario utilizar la funcion 'space_eval', que se encuentra en un ejemplo en la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "R6ZP3blHFrYJ"
   },
   "outputs": [],
   "source": [
    "#Ejemplo de como se obtienen los mejores parametros y de como se obtiene un objecto model a partir de ellos.\n",
    "#best_parameters = space_eval(neural_network_params, best)\n",
    "#best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gbNkHn8rwrUn"
   },
   "outputs": [],
   "source": [
    "#Ejemplo de como obtener el modelo ya creado con los mejores hiperparametros\n",
    "#Dejo comentado esto para que no rompa en caso de utilizar un modelo distinto al de redes neuronales.\n",
    "\n",
    "#best_model = get_neural_network_model(best_parameters, (df_train.shape[1] - 1))\n",
    "\n",
    "#A partir de aca ya se puede usar el modelo, pero tener cuidado que el mismo no se encuentra entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMAD6cSPwrUn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Hyperparameter_Tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
